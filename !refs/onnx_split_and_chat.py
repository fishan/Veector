# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
!pip install onnx onnxruntime transformers psutil huggingface_hub

import onnx
import os
import numpy as np
import shutil
import zipfile
import psutil
import onnxruntime as ort
from google.colab import drive
from transformers import AutoTokenizer
from onnx.external_data_helper import load_external_data_for_model
from huggingface_hub import hf_hub_download

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
print("üü¢ [LOG] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_repo = "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX"
model_filename = "onnx/model_quantized.onnx"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º quantized –≤–µ—Ä—Å–∏—é
model_path = hf_hub_download(repo_id=model_repo, filename=model_filename)
print(f"üü¢ [LOG] ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏
model_dir = "model_files"
os.makedirs(model_dir, exist_ok=True)
split_model_path = os.path.join(model_dir, "model_split.onnx")

# –†–∞–∑–±–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
chunk_size = 1024 * 1024 * 50  # 50 MB
model = onnx.load(model_path)
onnx.save_model(model, split_model_path, save_as_external_data=True, all_tensors_to_one_file=False, size_threshold=chunk_size)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
files = [f for f in os.listdir(model_dir) if f.startswith("model_split") or "quantized" in f]
print(f"üü¢ [LOG] üìÇ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –≤ {model_dir}:")
for f in files:
    size_mb = os.path.getsize(os.path.join(model_dir, f)) / (1024 * 1024)
    print(f"üü¢ [LOG] üìÑ {f}: {size_mb:.2f} MB")

# –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
zip_name = "DeepSeek-R1-Distill-Qwen-1.5B-splited-onnx.zip"
with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for f in files:
        zipf.write(os.path.join(model_dir, f), arcname=f)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ –ø—É—Ç–∏ –ø–∞–ø–∫–∏
print(f"üü¢ [LOG] üì¶ –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {zip_name}, —Ä–∞–∑–º–µ—Ä: {os.path.getsize(zip_name) / (1024 * 1024):.2f} MB")

# –í—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ Google Drive
drive.mount('/content/drive', force_remount=True)
destination_path = f"/content/drive/My Drive/{zip_name}"
shutil.copy(zip_name, destination_path)
print(f"üü¢ [LOG] ‚úÖ –ê—Ä—Ö–∏–≤ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ Google Drive: {destination_path}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–ø–∫–∏
os.environ["ONNX_LOAD_EXTERNAL_LOGGING"] = "1"
onnx_model = onnx.load(split_model_path, load_external_data=False)
load_external_data_for_model(onnx_model, model_dir)
onnx.save(onnx_model, split_model_path)
session = ort.InferenceSession(split_model_path)
print("üü¢ [LOG] ‚úÖ ONNX Runtime –∑–∞–≥—Ä—É–∂–µ–Ω.")

# –õ–æ–≥–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
memory_info = psutil.virtual_memory()
print(f"üü¢ [LOG] üìä –ü–∞–º—è—Ç—å: {memory_info.used / (1024 * 1024):.2f} MB / {memory_info.total / (1024 * 1024):.2f} MB")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
num_hidden_layers = 28
num_key_value_heads = 2
head_dim = 128
max_new_tokens = 512

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
PROMPT_TEMPLATE = "<ÔΩúUserÔΩú>{message}<ÔΩúAssistantÔΩú>"

def preprocess_text(text):
    formatted_text = PROMPT_TEMPLATE.format(message=text)
    inputs = tokenizer(formatted_text, return_tensors="np")
    input_feed = {
        "input_ids": inputs["input_ids"].astype(np.int64),
        "attention_mask": inputs["attention_mask"].astype(np.int64),
        "position_ids": np.arange(inputs["input_ids"].shape[1], dtype=np.int64)[None, :],
    }
    for i in range(num_hidden_layers):
        input_feed[f"past_key_values.{i}.key"] = np.zeros((1, num_key_value_heads, 0, head_dim), dtype=np.float32)
        input_feed[f"past_key_values.{i}.value"] = np.zeros((1, num_key_value_heads, 0, head_dim), dtype=np.float32)
    return input_feed, inputs["input_ids"], inputs["attention_mask"]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º
def generate_text(input_feed, input_ids, attention_mask, max_new_tokens):
    generated_ids = input_ids[0].tolist()
    past_key_values = {k: v for k, v in input_feed.items() if "past_key_values" in k}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º "<think>\n"
    think_tokens = tokenizer.encode("<think>\n", add_special_tokens=False)
    generated_ids.extend(think_tokens)
    print(f"üü¢ [STREAM] {tokenizer.decode(think_tokens, skip_special_tokens=False)}", end='', flush=True)

    # –ü–µ—Ä–≤—ã–π —à–∞–≥
    print(f"üü¢ [LOG] –ü–µ—Ä–≤—ã–π —à–∞–≥: input_ids shape={input_ids.shape}")
    outputs = session.run(None, input_feed)
    logits = outputs[0][:, -1, :]
    temperature = 0.6
    logits = logits / temperature
    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)
    next_token = np.random.choice(len(probs[0]), p=probs[0])
    generated_ids.append(next_token)
    print(f"üü¢ [STREAM] {tokenizer.decode([next_token], skip_special_tokens=False)}", end='', flush=True)

    # –û–±–Ω–æ–≤–ª—è–µ–º past_key_values
    for i in range(num_hidden_layers):
        past_key_values[f"past_key_values.{i}.key"] = outputs[2 * i + 1]
        past_key_values[f"past_key_values.{i}.value"] = outputs[2 * i + 2]
        if i == 0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
            print(f"üü¢ [LOG] past_key_values.{i}.key shape={past_key_values[f'past_key_values.{i}.key'].shape}")

    # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    for step in range(max_new_tokens - 1):
        seq_length = past_key_values["past_key_values.0.key"].shape[2]
        input_feed = {
            "input_ids": np.array([[next_token]], dtype=np.int64),
            "attention_mask": np.ones((1, seq_length + 1), dtype=np.int64),
            "position_ids": np.arange(seq_length, seq_length + 1, dtype=np.int64)[None, :],
        }
        input_feed.update(past_key_values)

        print(f"üü¢ [LOG] –®–∞–≥ {step + 1}: input_ids shape={input_feed['input_ids'].shape}, attention_mask shape={input_feed['attention_mask'].shape}")
        outputs = session.run(None, input_feed)
        logits = outputs[0][:, -1, :]
        logits = logits / temperature
        probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)

        if step < 10 and 151648 in tokenizer.get_vocab():
            probs[0, 151648] = 0  # –ò—Å–∫–ª—é—á–∞–µ–º "<think>"
            probs = probs / np.sum(probs, axis=-1, keepdims=True)

        next_token = np.random.choice(len(probs[0]), p=probs[0])
        generated_ids.append(next_token)
        print(f"üü¢ [STREAM] {tokenizer.decode([next_token], skip_special_tokens=False)}", end='', flush=True)

        for i in range(num_hidden_layers):
            past_key_values[f"past_key_values.{i}.key"] = outputs[2 * i + 1]
            past_key_values[f"past_key_values.{i}.value"] = outputs[2 * i + 2]

        if next_token == tokenizer.eos_token_id:
            break

    print()  # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

# –ß–∞—Ç
def chat():
    print("\nü§ñ ONNX-–ß–∞—Ç –∞–∫—Ç–∏–≤–µ–Ω! –ù–∞–ø–∏—à–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å ('–≤—ã—Ö–æ–¥' –¥–ª—è –≤—ã—Ö–æ–¥–∞).")
    while True:
        user_input = input("–¢—ã: ")
        if user_input.lower() == "–≤—ã—Ö–æ–¥":
            print("ü§ñ –ß–∞—Ç –∑–∞–≤–µ—Ä—à–µ–Ω.")
            break

        print("üü¢ [LOG] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞...")
        input_feed, input_ids, attention_mask = preprocess_text(user_input)

        try:
            response_text = generate_text(input_feed, input_ids, attention_mask, max_new_tokens)
            print(f"üü¢ [LOG] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–∞–º—è—Ç—å: {psutil.virtual_memory().used / (1024 * 1024):.2f} MB")
            print(f"ü§ñ ONNX: {response_text}")
        except Exception as e:
            print(f"üü¢ [LOG] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")

# –ó–∞–ø—É—Å–∫
chat()