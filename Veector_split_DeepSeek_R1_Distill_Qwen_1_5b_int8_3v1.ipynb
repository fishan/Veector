{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiNtrdu6mvJZH0szw8++Xj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishan/Veector/blob/base/Veector_split_DeepSeek_R1_Distill_Qwen_1_5b_int8_3v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 0: Install Dependencies ===\n",
        "# Installs necessary Python packages using pip.\n",
        "# Run this cell first if you are in a new environment.\n",
        "\n",
        "!pip install numpy psutil torch transformers accelerate bitsandbytes ipfshttpclient qiskit qiskit-aer requests huggingface_hub -q\n",
        "\n",
        "print(\"Dependencies installed/checked.\")"
      ],
      "metadata": {
        "id": "p0yyhMeA34E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data/"
      ],
      "metadata": {
        "id": "BJ4OPwRe37j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Configuration & General Imports ===\n",
        "# Defines main configuration variables, performs necessary imports,\n",
        "# handles authentication, and mounts Google Drive.\n",
        "\n",
        "# --- Standard & External Library Imports ---\n",
        "import numpy as np\n",
        "import pickle\n",
        "import hashlib\n",
        "import time\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"Warning: PyTorch not found.\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, PreTrainedTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Warning: Transformers library not found.\")\n",
        "\n",
        "from google.colab import drive, files, userdata # Colab specific\n",
        "from huggingface_hub import login             # Hugging Face login\n",
        "\n",
        "print(\"Standard/External imports loaded.\")\n",
        "\n",
        "# --- Veector Project Imports ---\n",
        "# Ensure Veector files (core.py, tensors.py, etc.) are accessible in the Colab environment\n",
        "# (e.g., uploaded to /content/ or accessible via sys.path)\n",
        "PROJECT_IMPORTS_OK = False\n",
        "try:\n",
        "    from core import Veector, CORE_VERSION\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING, get_tensor_hash,\n",
        "        TAG_TYPE_PROCESSOR, TAG_TYPE_KNOWLEDGE, TAG_TYPE_CONVERTER, TAG_TYPE_STATE,\n",
        "        TAG_MODEL_QWEN2, TAG_MODEL_LLAMA3, TAG_MODEL_DEEPSEEK,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16,\n",
        "        TAG_PREC_INT8, TAG_PREC_INT4,\n",
        "        TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_EMBEDDING, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_ATTN_QKV,\n",
        "        TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LAYERNORM,\n",
        "        TAG_COMP_LM_HEAD,\n",
        "        TAG_FUNC_LINEAR, TAG_FUNC_ATTENTION, TAG_FUNC_FFN,\n",
        "        TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_CAST_DTYPE, TAG_FUNC_RESHAPE,\n",
        "        TAG_SEMANTIC_HIDDEN_STATE, TAG_SEMANTIC_LOGITS, TAG_SEMANTIC_TOKEN_IDS,\n",
        "        TAG_SEMANTIC_KV_CACHE,\n",
        "        tag_layer,\n",
        "        GROUP_IDX_QWEN_KNOWLEDGE, GROUP_IDX_QWEN_PROCESSOR,\n",
        "        GROUP_IDX_DEEPSEEK_KNOWLEDGE\n",
        "    )\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "    from operations import OPERATIONS_VERSION # Import version, specific ops imported later if needed\n",
        "    # OP Codes needed globally or frequently\n",
        "    OP_ADD=[0,0,2]\n",
        "    OP_MATRIX_MULTIPLY=[30,0,0]\n",
        "    OP_LINEAR=OP_MATRIX_MULTIPLY\n",
        "    OP_EMBEDDING_LOOKUP=[40,6,0]\n",
        "    OP_LINEAR_HEAD=OP_LINEAR\n",
        "    META_OP_CATEGORY=99\n",
        "    OP_STORE=[99,0,0]\n",
        "    OP_LOAD=[99,0,1]\n",
        "    OP_QWEN2_RMSNORM = [300, 0, 0]\n",
        "    OP_QWEN2_ATTENTION = [300, 1, 0]\n",
        "    OP_QWEN2_MLP = [300, 2, 0]\n",
        "    OP_GET_TUPLE_ELEM_0 = [99, 3, 0]\n",
        "    OP_GET_TUPLE_ELEM_1 = [99, 3, 1]\n",
        "    OP_GET_TUPLE_ELEM_2 = [99, 3, 2]\n",
        "\n",
        "    print(\"Veector project components imported successfully.\")\n",
        "    print(f\"Versions: Core={CORE_VERSION}, Tensors={TENSORS_VERSION}, Ops={OPERATIONS_VERSION}, DB={VEECTORDB_VERSION}\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"---!!! FATAL ERROR (ImportError) !!! ---\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "    print(f\"Could not import required name from Veector files.\")\n",
        "    print(f\"Ensure files are UP-TO-DATE and ACCESSIBLE.\")\n",
        "    print(f\"-----------------------------------------\\\")\n",
        "    # Optionally define dummies if needed for notebook structure\n",
        "except Exception as other_e:\n",
        "    print(f\"---!!! FATAL ERROR (Other Exception during Import) !!! ---\")\n",
        "    print(f\"Specific error: {other_e}\")\n",
        "    traceback.print_exc()\n",
        "    print(f\"Check imported files for syntax errors.\")\n",
        "    print(f\"----------------------------------------------------------\")\n",
        "\n",
        "if not PROJECT_IMPORTS_OK:\n",
        "     raise ImportError(\"Failed to import necessary Veector components.\")\n",
        "if not TORCH_AVAILABLE or not TRANSFORMERS_AVAILABLE:\n",
        "     raise ImportError(\"Failed to import Torch or Transformers.\")\n",
        "\n",
        "# --- Configuration Variables ---\n",
        "# --- Модель и Пути ---\n",
        "MODEL_NAME: str = \"DeepSeek-R1-Distill-Qwen-1.5B\" # Имя для файлов и логов\n",
        "HF_MODEL_SOURCE: str = f\"deepseek-ai/{MODEL_NAME}\" # Полный идентификатор HF\n",
        "DB_ROOT_DIR: str = \"/content/data\" # Корневая директория для данных\n",
        "DB_PATH: Path = Path(DB_ROOT_DIR) / \"db\" # Путь к базе данных Veector\n",
        "\n",
        "# --- Параметры Конвертации и Точности ---\n",
        "# Используйте torch.float16 для совместимости или torch.bfloat16 если поддерживается\n",
        "CONVERSION_DTYPE: torch.dtype = torch.float16\n",
        "# Определяем соответствующий тег точности Veector\n",
        "if CONVERSION_DTYPE == torch.float16:\n",
        "    DEFAULT_PRECISION_TAG: int = TAG_PREC_FLOAT16\n",
        "elif CONVERSION_DTYPE == torch.bfloat16:\n",
        "    DEFAULT_PRECISION_TAG: int = TAG_PREC_BFLOAT16\n",
        "elif CONVERSION_DTYPE == torch.float32:\n",
        "    DEFAULT_PRECISION_TAG: int = TAG_PREC_FLOAT32\n",
        "else:\n",
        "    DEFAULT_PRECISION_TAG: int = TAG_PREC_FLOAT16 # Fallback\n",
        "    print(f\"Warning: Unsupported CONVERSION_DTYPE {CONVERSION_DTYPE}, falling back to float16 tag.\")\n",
        "\n",
        "# Квантовать ли Embedding и LM Head слои в INT8?\n",
        "QUANTIZE_EMBED_LMHEAD: bool = True\n",
        "QUANTIZED_PRECISION_TAG: int = TAG_PREC_INT8\n",
        "\n",
        "# --- Параметры Групп и Модели ---\n",
        "KNOWLEDGE_GROUP_IDX: int = GROUP_IDX_DEEPSEEK_KNOWLEDGE # Используем ID для DeepSeek\n",
        "PROCESSOR_GROUP_IDX: int = GROUP_IDX_QWEN_PROCESSOR # Процессоры Qwen2\n",
        "MODEL_TAG: int = TAG_MODEL_DEEPSEEK # Тег модели\n",
        "\n",
        "# --- Параметры для Тестирования ---\n",
        "PROMPT_FOR_TESTING: str = \"Hello, how are you?\"\n",
        "\n",
        "# --- Создание директорий ---\n",
        "try:\n",
        "    DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Veector DB directory ensured at: {DB_PATH.resolve()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating DB directory {DB_PATH}: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Аутентификация и Google Drive ---\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"HF_TOKEN not found in Colab secrets. Please add it.\")\n",
        "    login(token=hf_token, add_to_git_credential=False)\n",
        "    print(\"Hugging Face login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face login failed: {e}\")\n",
        "    # Decide if this is fatal or not\n",
        "    # raise\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Google Drive mount failed: {e}\")\n",
        "    # Decide if this is fatal or not\n",
        "\n",
        "print(\"\\n--- Cell 1: Configuration & Imports Finished ---\")\n"
      ],
      "metadata": {
        "id": "vCn328My4nGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Knowledge Tensor Conversion ===\n",
        "# Loads the HF model and converts its parameters into Veector knowledge tensors.\n",
        "# Saves knowledge tensors, knowledge map, name ID map, and a dedicated knowledge index.\n",
        "# This cell is self-contained, relying only on variables from Cell 1 (Configuration).\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Imports (Redundant but ensures independence) ---\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    from core import Veector\n",
        "    from tensors import (\n",
        "        TensorCoordinate, create_tensor, validate_tensor,\n",
        "        TAG_TYPE_KNOWLEDGE, TAG_COMP_WEIGHTS, TAG_COMP_BIAS,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_LM_HEAD, TAG_COMP_LAYERNORM, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O,\n",
        "        TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN,\n",
        "        tag_layer, DTYPE_MAPPING, TAG_PREC_INT8\n",
        "    )\n",
        "    from veectordb import VeectorDB # Needed to access INDEX_FILENAME if saving index here\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR in Cell 2: Missing imports: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Configuration (Load from Cell 1 variables) ---\n",
        "# These should be defined in the global scope by running Cell 1\n",
        "if 'HF_MODEL_SOURCE' not in globals(): raise NameError(\"HF_MODEL_SOURCE not defined. Run Cell 1.\")\n",
        "if 'DB_PATH' not in globals(): raise NameError(\"DB_PATH not defined. Run Cell 1.\")\n",
        "if 'CONVERSION_DTYPE' not in globals(): raise NameError(\"CONVERSION_DTYPE not defined. Run Cell 1.\")\n",
        "if 'DEFAULT_PRECISION_TAG' not in globals(): raise NameError(\"DEFAULT_PRECISION_TAG not defined. Run Cell 1.\")\n",
        "if 'QUANTIZE_EMBED_LMHEAD' not in globals(): raise NameError(\"QUANTIZE_EMBED_LMHEAD not defined. Run Cell 1.\")\n",
        "if 'QUANTIZED_PRECISION_TAG' not in globals(): raise NameError(\"QUANTIZED_PRECISION_TAG not defined. Run Cell 1.\")\n",
        "if 'KNOWLEDGE_GROUP_IDX' not in globals(): raise NameError(\"KNOWLEDGE_GROUP_IDX not defined. Run Cell 1.\")\n",
        "if 'MODEL_TAG' not in globals(): raise NameError(\"MODEL_TAG not defined. Run Cell 1.\")\n",
        "if 'MODEL_NAME' not in globals(): raise NameError(\"MODEL_NAME not defined. Run Cell 1.\")\n",
        "\n",
        "print(f\"--- Running Cell 2: Knowledge Conversion for {MODEL_NAME} ---\")\n",
        "print(f\"    Target DB: {DB_PATH.resolve()}\")\n",
        "print(f\"    Conversion Dtype: {CONVERSION_DTYPE}\")\n",
        "print(f\"    Quantize Embed/LMHead: {QUANTIZE_EMBED_LMHEAD}\")\n",
        "start_cell2_time = time.time()\n",
        "\n",
        "# --- Initialization ---\n",
        "hf_model = None\n",
        "vec_knowledge: Optional[Veector] = None\n",
        "ORIGINAL_NAME_TO_ID_MAP: Dict[str, int] = {}\n",
        "ID_TO_ORIGINAL_NAME_MAP: Dict[int, str] = {}\n",
        "NEXT_NAME_ID: int = 0\n",
        "knowledge_map: Dict[str, str] = {} # HF Name -> Veector Tensor ID\n",
        "param_count: int = 0\n",
        "conversion_errors: int = 0\n",
        "\n",
        "# --- Helper function for Name IDs ---\n",
        "def get_or_create_name_id(name: Optional[str]) -> int:\n",
        "    \"\"\"Assigns and returns a unique ID for a parameter name.\"\"\"\n",
        "    global NEXT_NAME_ID, ORIGINAL_NAME_TO_ID_MAP, ID_TO_ORIGINAL_NAME_MAP\n",
        "    if not name: return -1\n",
        "    if name in ORIGINAL_NAME_TO_ID_MAP: return ORIGINAL_NAME_TO_ID_MAP[name]\n",
        "    current_id = NEXT_NAME_ID\n",
        "    ORIGINAL_NAME_TO_ID_MAP[name] = current_id\n",
        "    ID_TO_ORIGINAL_NAME_MAP[current_id] = name\n",
        "    NEXT_NAME_ID += 1\n",
        "    return current_id\n",
        "\n",
        "try:\n",
        "    # --- 1. Load Hugging Face Model ---\n",
        "    print(f\"\\nLoading HF Model: {HF_MODEL_SOURCE}...\")\n",
        "    # Load in the target conversion dtype directly if possible\n",
        "    # Note: Loading directly in float16 might cause issues if operations require float32\n",
        "    # It might be safer to load in float32 and convert parameter by parameter.\n",
        "    # Let's load in float32 for robustness during conversion.\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "        HF_MODEL_SOURCE,\n",
        "        torch_dtype=torch.float32, # Load in float32 for processing\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    hf_model.eval() # Set to evaluation mode\n",
        "    model_config = hf_model.config # Get config from loaded model\n",
        "    print(f\"HF Model '{MODEL_NAME}' loaded successfully.\")\n",
        "    gc.collect()\n",
        "\n",
        "    # --- 2. Initialize Veector Instance for this Cell ---\n",
        "    print(f\"\\nInitializing Veector instance for knowledge conversion...\")\n",
        "    # Initialize with the main DB path, it will create an empty index if needed\n",
        "    # We will save the knowledge index separately later.\n",
        "    vec_knowledge = Veector(db_dir=DB_PATH, ipfs_enabled=False)\n",
        "    print(f\"Veector initialized. DB Index entries: {len(vec_knowledge.db.index)}\")\n",
        "    # Clear any existing index entries if we want a clean conversion\n",
        "    # vec_knowledge.db.index = {}\n",
        "    # vec_knowledge.db._index_dirty = True # Mark dirty if cleared\n",
        "    # print(\"Cleared existing index for clean knowledge conversion.\")\n",
        "\n",
        "\n",
        "    # --- 3. Conversion Loop ---\n",
        "    print(f\"\\nStarting parameter conversion...\")\n",
        "    total_params = sum(1 for _ in hf_model.named_parameters())\n",
        "    print(f\"Found {total_params} parameters to process.\")\n",
        "\n",
        "    for idx, (name, param) in enumerate(hf_model.named_parameters()):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\nProcessing Param {idx+1}/{total_params}: {name}\")\n",
        "        print(f\"  Original Shape: {param.shape} | Dtype: {param.dtype}\")\n",
        "\n",
        "        param_data_fp32: Optional[np.ndarray] = None\n",
        "        knowledge_data_to_pass: Optional[np.ndarray] = None\n",
        "        tags: List[int] = []\n",
        "        metadata_extra_to_pass: Optional[Dict] = None\n",
        "        dtype_to_pass: Any = None\n",
        "        final_tags: List[int] = []\n",
        "        knowledge_coord: Optional[TensorCoordinate] = None\n",
        "        name_id: int = -1\n",
        "        create_result: Optional[List] = None\n",
        "        knowledge_id: Optional[str] = None\n",
        "        requires_transpose: bool = False\n",
        "\n",
        "        try:\n",
        "            # Step 3a: Get data, name ID, base tags, coordinates\n",
        "            param_data_fp32 = param.data.cpu().numpy() # Already float32\n",
        "            name_id = get_or_create_name_id(name)\n",
        "            tags = [TAG_TYPE_KNOWLEDGE, MODEL_TAG] # Base tags\n",
        "            layer_idx = -1\n",
        "            group_idx = KNOWLEDGE_GROUP_IDX\n",
        "            coord_x = 0\n",
        "            current_nest = 1 # Default nest for knowledge\n",
        "            is_weight = name.endswith(\".weight\")\n",
        "            is_bias = name.endswith(\".bias\")\n",
        "\n",
        "            if is_weight: tags.append(TAG_COMP_WEIGHTS)\n",
        "            elif is_bias: tags.append(TAG_COMP_BIAS)\n",
        "\n",
        "            # Determine component type, layer index, and X coordinate\n",
        "            if \"model.embed_tokens.weight\" in name:\n",
        "                 tags.append(TAG_COMP_EMBEDDING); coord_x = 0\n",
        "            elif \"lm_head.weight\" in name:\n",
        "                 tags.append(TAG_COMP_LM_HEAD); coord_x = 1; requires_transpose = True\n",
        "            elif \"model.norm.weight\" in name:\n",
        "                 layer_idx = model_config.num_hidden_layers; tags.append(TAG_COMP_LAYERNORM); coord_x = 0\n",
        "            elif \".layers.\" in name:\n",
        "                try:\n",
        "                    layer_part = name.split('.layers.')[1]\n",
        "                    layer_idx = int(layer_part.split('.')[0])\n",
        "                    if layer_idx >= 0: tags.append(tag_layer(layer_idx))\n",
        "                    else: raise ValueError(f\"Invalid L idx: {layer_idx}\")\n",
        "\n",
        "                    component_tag_layer = None\n",
        "                    if \"self_attn\" in name:\n",
        "                        if \"q_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 10; requires_transpose = True\n",
        "                        elif \"q_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 11\n",
        "                        elif \"k_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 20; requires_transpose = True\n",
        "                        elif \"k_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 21\n",
        "                        elif \"v_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 30; requires_transpose = True\n",
        "                        elif \"v_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 31\n",
        "                        elif \"o_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_O; coord_x = 40; requires_transpose = True\n",
        "                    elif \"mlp\" in name:\n",
        "                        if \"gate_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_GATE; coord_x = 50; requires_transpose = True\n",
        "                        elif \"up_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_UP; coord_x = 60; requires_transpose = True\n",
        "                        elif \"down_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_DOWN; coord_x = 70; requires_transpose = True\n",
        "                    elif \"input_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 1\n",
        "                    elif \"post_attention_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 2\n",
        "\n",
        "                    if component_tag_layer: tags.append(component_tag_layer)\n",
        "                    elif not is_weight and not is_bias: print(f\"  WARN: Unrecognized comp in L{layer_idx}: {name}\"); coord_x = 99\n",
        "                except Exception as parse_e:\n",
        "                    print(f\"  Error parsing layer for {name}: {parse_e}\"); conversion_errors += 1; continue\n",
        "            else:\n",
        "                print(f\"  WARN: Param unmatched: {name}\"); layer_idx = -1; coord_x = 999\n",
        "\n",
        "            knowledge_coord = TensorCoordinate(layer=layer_idx, group=group_idx, nest=current_nest, x=coord_x)\n",
        "\n",
        "            # Step 3b: Quantization / Type Casting / Transposition\n",
        "            quantization_scale = None\n",
        "            current_precision_tag = DEFAULT_PRECISION_TAG\n",
        "            data_before_save = None\n",
        "            target_np_dtype = np.float16 # Default target\n",
        "            if CONVERSION_DTYPE == torch.float16: target_np_dtype = np.float16\n",
        "            elif CONVERSION_DTYPE == torch.bfloat16: target_np_dtype = np.float32 # Numpy has no bfloat16, store as float32\n",
        "            elif CONVERSION_DTYPE == torch.float32: target_np_dtype = np.float32\n",
        "\n",
        "            # Special handling for Embedding and LM Head (Quantization)\n",
        "            if QUANTIZE_EMBED_LMHEAD and (name == \"model.embed_tokens.weight\" or name == \"lm_head.weight\"):\n",
        "                try:\n",
        "                    print(f\"  Quantizing {name} to INT8...\")\n",
        "                    abs_max = np.max(np.abs(param_data_fp32)); scale = 1.0\n",
        "                    if abs_max >= 1e-9: scale = abs_max / 127.0\n",
        "                    scale = max(scale, 1e-9) # Prevent division by zero\n",
        "                    quantized_data = np.round(param_data_fp32 / scale).astype(np.int8)\n",
        "                    data_before_save = quantized_data\n",
        "                    dtype_to_pass = np.int8\n",
        "                    quantization_scale = float(scale)\n",
        "                    current_precision_tag = QUANTIZED_PRECISION_TAG # Use INT8 tag\n",
        "                    metadata_extra_to_pass = {\"quantization_scale\": quantization_scale}\n",
        "                    print(f\"    Quantized Shape: {data_before_save.shape}, Scale: {quantization_scale:.4f}\")\n",
        "                    # Transpose LM Head AFTER quantization\n",
        "                    if name == \"lm_head.weight\": # requires_transpose is True here\n",
        "                        print(\"    Transposing quantized LM Head weights...\")\n",
        "                        data_before_save = data_before_save.T\n",
        "                        print(f\"    Transposed Shape: {data_before_save.shape}\")\n",
        "                except Exception as quant_e:\n",
        "                     print(f\"  ERROR quantizing {name}: {quant_e}\"); conversion_errors += 1; continue\n",
        "            else: # Standard parameters (cast and maybe transpose)\n",
        "                try:\n",
        "                    print(f\"  Casting {name} to {target_np_dtype}...\")\n",
        "                    data_before_save = param_data_fp32.astype(target_np_dtype)\n",
        "                    dtype_to_pass = data_before_save.dtype\n",
        "                    current_precision_tag = DEFAULT_PRECISION_TAG # Use default precision tag\n",
        "                    metadata_extra_to_pass = None\n",
        "                    # Transpose if required\n",
        "                    if requires_transpose:\n",
        "                        print(f\"    Transposing {name} weights...\")\n",
        "                        data_before_save = data_before_save.T\n",
        "                        print(f\"    Transposed Shape: {data_before_save.shape}\")\n",
        "                except Exception as cast_e:\n",
        "                     print(f\"  ERROR casting/transposing {name}: {cast_e}\"); conversion_errors += 1; continue\n",
        "\n",
        "            knowledge_data_to_pass = data_before_save\n",
        "            final_shape_to_save = knowledge_data_to_pass.shape if knowledge_data_to_pass is not None else None\n",
        "\n",
        "            # Step 3c: Finalize tags\n",
        "            final_tags = list(tags) # Start with base tags\n",
        "            # Remove default precision if a specific one was applied\n",
        "            if current_precision_tag != DEFAULT_PRECISION_TAG and DEFAULT_PRECISION_TAG in final_tags:\n",
        "                 final_tags.remove(DEFAULT_PRECISION_TAG)\n",
        "            # Add the actual precision tag used\n",
        "            if current_precision_tag:\n",
        "                final_tags.append(current_precision_tag)\n",
        "            final_tags = sorted(list(set(final_tags))) # Ensure uniqueness and order\n",
        "\n",
        "            print(f\"  Final Tags: {final_tags}\"); print(f\"  Coordinate: {knowledge_coord}\")\n",
        "            print(f\"  Data to save: dtype={dtype_to_pass}, shape={final_shape_to_save}\")\n",
        "            if metadata_extra_to_pass: print(f\"  Extra Metadata: {metadata_extra_to_pass}\")\n",
        "\n",
        "            # Step 3d: Create Veector Tensor structure (list)\n",
        "            create_result = vec_knowledge.create_tensor(\n",
        "                 coord=knowledge_coord,\n",
        "                 tensor_type=\"knowledge\",\n",
        "                 knowledge_data=knowledge_data_to_pass,\n",
        "                 tags=final_tags,\n",
        "                 dtype=dtype_to_pass,\n",
        "                 shape=final_shape_to_save,\n",
        "                 name_id=name_id,\n",
        "                 metadata_extra=metadata_extra_to_pass, # Pass quantization scale if any\n",
        "                 status=\"active\"\n",
        "             )\n",
        "            if not validate_tensor(create_result):\n",
        "                 raise ValueError(f\"Invalid tensor structure created for {name}\")\n",
        "\n",
        "            # Step 3e: Save Tensor\n",
        "            knowledge_id = vec_knowledge.save_tensor(create_result) # Pass the list structure\n",
        "\n",
        "            if knowledge_id:\n",
        "                knowledge_map[name] = knowledge_id # Store HF name -> Veector ID mapping\n",
        "                param_count += 1\n",
        "                print(f\"    Saved knowledge tensor with ID: {knowledge_id}\")\n",
        "            else:\n",
        "                conversion_errors += 1\n",
        "                print(f\"  ERROR saving tensor for {name}\")\n",
        "\n",
        "        except Exception as create_save_e:\n",
        "            print(f\"  ERROR during create/save for {name}: {create_save_e}\")\n",
        "            traceback.print_exc()\n",
        "            conversion_errors += 1\n",
        "        finally:\n",
        "            # Clean up intermediate numpy array for this parameter\n",
        "            if param_data_fp32 is not None: del param_data_fp32\n",
        "            # loop_end_time = time.time()\n",
        "            # print(f\"  Param {idx+1} time: {loop_end_time - loop_start_time:.2f}s\") # Optional timing log\n",
        "\n",
        "    # --- End of Conversion Loop ---\n",
        "\n",
        "    print(f\"\\n--- Finished saving {param_count} knowledge tensors to {vec_knowledge.db.db_root_path if vec_knowledge.db else 'N/A'} ---\")\n",
        "    if conversion_errors > 0:\n",
        "        print(f\"!!! WARNING: {conversion_errors} errors occurred during knowledge conversion !!!\")\n",
        "\n",
        "    # --- 4. Save Maps and Knowledge Index ---\n",
        "    # Save Name ID Map\n",
        "    name_map_file = DB_PATH / f\"{MODEL_NAME}_name_id_map.pkl\"\n",
        "    try:\n",
        "        map_data_to_save = {\n",
        "            \"name_to_id\": ORIGINAL_NAME_TO_ID_MAP,\n",
        "            \"id_to_name\": ID_TO_ORIGINAL_NAME_MAP,\n",
        "            \"next_id\": NEXT_NAME_ID\n",
        "        }\n",
        "        print(f\"\\nSaving Name <-> ID map ({len(ORIGINAL_NAME_TO_ID_MAP)} entries) to {name_map_file}...\")\n",
        "        with open(name_map_file, 'wb') as f: pickle.dump(map_data_to_save, f)\n",
        "        print(f\"Name ID map saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saving name ID map: {e}\")\n",
        "\n",
        "    # Save Knowledge Map (HF Name -> Veector ID)\n",
        "    knowledge_map_file = DB_PATH / f\"{MODEL_NAME}_knowledge_map.pkl\"\n",
        "    try:\n",
        "        print(f\"\\nSaving Knowledge map ({len(knowledge_map)} entries) to {knowledge_map_file}...\")\n",
        "        with open(knowledge_map_file, 'wb') as f: pickle.dump(knowledge_map, f)\n",
        "        print(f\"Knowledge map saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saving knowledge map: {e}\")\n",
        "\n",
        "    # Save the index containing ONLY the knowledge tensors created in this cell\n",
        "    knowledge_index_file = DB_PATH / f\"{MODEL_NAME}_knowledge_index.pkl\"\n",
        "    try:\n",
        "        print(f\"\\nSaving Knowledge index ({len(vec_knowledge.db.index)} entries) to {knowledge_index_file}...\")\n",
        "        # Use save_index_as to save the current index (only knowledge) to a specific file\n",
        "        vec_knowledge.db.save_index_as(knowledge_index_file)\n",
        "        print(f\"Knowledge index saved successfully to {knowledge_index_file.name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saving knowledge index: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # --- 5. Cleanup ---\n",
        "    print(\"\\nCleaning up resources for Cell 2...\")\n",
        "    if vec_knowledge and hasattr(vec_knowledge, 'db') and vec_knowledge.db:\n",
        "        vec_knowledge.db.close() # Close DB connection (won't save main index)\n",
        "        print(\"Veector DB connection closed.\")\n",
        "    if hf_model is not None:\n",
        "        del hf_model\n",
        "        print(\"HF model deleted.\")\n",
        "    if 'vec_knowledge' in locals():\n",
        "        del vec_knowledge\n",
        "        print(\"Veector instance deleted.\")\n",
        "\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"CUDA cache cleared.\")\n",
        "    gc.collect()\n",
        "    print(\"Garbage collection run.\")\n",
        "\n",
        "except Exception as cell2_e:\n",
        "    print(f\"\\n---!!! FATAL ERROR in Cell 2 Execution: {cell2_e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "    # Ensure cleanup happens even on error\n",
        "    if 'vec_knowledge' in locals() and vec_knowledge and hasattr(vec_knowledge, 'db') and vec_knowledge.db:\n",
        "        try: vec_knowledge.db.close()\n",
        "        except: pass\n",
        "    if 'hf_model' in locals() and hf_model is not None: del hf_model\n",
        "    if 'vec_knowledge' in locals(): del vec_knowledge\n",
        "    gc.collect()\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    raise # Re-raise the exception to stop notebook execution\n",
        "\n",
        "finally:\n",
        "    end_cell2_time = time.time()\n",
        "    print(f\"\\n--- Cell 2 Finished in {end_cell2_time - start_cell2_time:.2f} seconds ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DFebXl6h3wJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./data/db/g500/ && rm ./data/db/DeepSeek-R1-Distill-Qwen-1.5B_proc_map.pkl"
      ],
      "metadata": {
        "id": "beeQYz_84JWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3 (Updated v1.1 - Handle Tuple from MLP Op) ===\n",
        "# Loads model config, knowledge map, name map, and knowledge index from Cell 2 outputs.\n",
        "# Initializes Veector with the knowledge index.\n",
        "# Defines and saves processor tensors (Embedding, Attn, FFN, Norm, LM Head).\n",
        "# FFN processor sequence updated to handle tuple output from OP_QWEN2_MLP.\n",
        "# Saves the processor map and the final combined index (tensor_index.pkl).\n",
        "# This cell is self-contained, relying only on Cell 1 variables and Cell 2 output files.\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Imports (Redundant but ensures independence) ---\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoConfig # Only need AutoConfig here\n",
        "    from core import Veector\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING, get_tensor_hash,\n",
        "        TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_ATTENTION,\n",
        "        TAG_FUNC_FFN, TAG_FUNC_LINEAR, TAG_COMP_LAYERNORM, TAG_MODEL_DEEPSEEK,\n",
        "        tag_layer, GROUP_IDX_QWEN_PROCESSOR, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LM_HEAD,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "    from operations import OPERATIONS_VERSION # Import version, specific ops imported later if needed\n",
        "    # OP Codes needed globally or frequently\n",
        "    OP_ADD=[0,0,2]\n",
        "    OP_MATRIX_MULTIPLY=[30,0,0]\n",
        "    OP_LINEAR=OP_MATRIX_MULTIPLY\n",
        "    OP_EMBEDDING_LOOKUP=[40,6,0]\n",
        "    OP_LINEAR_HEAD=OP_LINEAR\n",
        "    META_OP_CATEGORY=99\n",
        "    OP_STORE=[99,0,0]\n",
        "    OP_LOAD=[99,0,1]\n",
        "    OP_QWEN2_RMSNORM = [300, 0, 0]\n",
        "    OP_QWEN2_ATTENTION = [300, 1, 0]\n",
        "    OP_QWEN2_MLP = [300, 2, 0]\n",
        "    OP_GET_TUPLE_ELEM_0 = [99, 3, 0]\n",
        "    OP_GET_TUPLE_ELEM_1 = [99, 3, 1]\n",
        "    OP_GET_TUPLE_ELEM_2 = [99, 3, 2]\n",
        "    OP_GET_TUPLE_ELEM_3 = [99, 3, 3] # Define needed OP code for tuple element 3\n",
        "\n",
        "    print(\"External and Veector libraries imported successfully.\")\n",
        "    # print(f\"Using Core: {CORE_VERSION}, Tensors: {TENSORS_VERSION}, Ops: {OPERATIONS_VERSION}, DB: {VEECTORDB_VERSION}\") # Optional print\n",
        "    # Add version checks if desired\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Failed to import components: {e}\")\n",
        "    raise\n",
        "except Exception as e_other:\n",
        "    print(f\"FATAL ERROR during imports: {e_other}\")\n",
        "    raise\n",
        "\n",
        "# --- Configuration (Load from Cell 1 variables) ---\n",
        "if 'MODEL_NAME' not in globals(): raise NameError(\"MODEL_NAME not defined. Run Cell 1.\")\n",
        "if 'HF_MODEL_SOURCE' not in globals(): raise NameError(\"HF_MODEL_SOURCE not defined. Run Cell 1.\")\n",
        "if 'DB_PATH' not in globals(): raise NameError(\"DB_PATH not defined. Run Cell 1.\")\n",
        "if 'PROCESSOR_GROUP_IDX' not in globals(): raise NameError(\"PROCESSOR_GROUP_IDX not defined. Run Cell 1.\")\n",
        "if 'MODEL_TAG' not in globals(): raise NameError(\"MODEL_TAG not defined. Run Cell 1.\")\n",
        "if 'DEFAULT_PRECISION_TAG' not in globals(): raise NameError(\"DEFAULT_PRECISION_TAG not defined. Run Cell 1.\")\n",
        "if 'QUANTIZED_PRECISION_TAG' not in globals(): raise NameError(\"QUANTIZED_PRECISION_TAG not defined. Run Cell 1.\")\n",
        "prec_tag_weights = DEFAULT_PRECISION_TAG\n",
        "prec_tag_quant = QUANTIZED_PRECISION_TAG\n",
        "\n",
        "print(f\"--- Running Cell 3 (v1.1): Processor Creation for {MODEL_NAME} ---\")\n",
        "print(f\"    Target DB: {DB_PATH.resolve()}\")\n",
        "start_cell3_time = time.time()\n",
        "\n",
        "# --- Initialization ---\n",
        "model_config = None\n",
        "knowledge_map: Optional[Dict[str, str]] = None\n",
        "name_id_map_data: Optional[Dict] = None\n",
        "vec_processor: Optional[Veector] = None\n",
        "processor_errors: int = 0\n",
        "processor_map: Dict[str, str] = {}\n",
        "\n",
        "# --- File Paths for Inputs from Cell 2 ---\n",
        "knowledge_map_filepath = DB_PATH / f\"{MODEL_NAME}_knowledge_map.pkl\"\n",
        "name_map_filepath = DB_PATH / f\"{MODEL_NAME}_name_id_map.pkl\"\n",
        "knowledge_index_filepath = DB_PATH / f\"{MODEL_NAME}_knowledge_index.pkl\"\n",
        "main_index_filepath = DB_PATH / VeectorDB.INDEX_FILENAME\n",
        "\n",
        "try:\n",
        "    # --- 1. Load Model Config ---\n",
        "    print(f\"\\nLoading Model Config from: {HF_MODEL_SOURCE}...\")\n",
        "    model_config = AutoConfig.from_pretrained(HF_MODEL_SOURCE, trust_remote_code=True)\n",
        "    num_layers = model_config.num_hidden_layers\n",
        "    num_attention_heads = model_config.num_attention_heads\n",
        "    num_key_value_heads = getattr(model_config, 'num_key_value_heads', num_attention_heads)\n",
        "    hidden_size = model_config.hidden_size\n",
        "    head_dim = hidden_size // num_attention_heads\n",
        "    rms_norm_eps = model_config.rms_norm_eps\n",
        "    hidden_act_function_name = model_config.hidden_act # Needed for FFN processor\n",
        "    rope_theta_value = getattr(model_config, 'rope_theta', 10000.0)\n",
        "    print(f\"Model Config loaded. HiddenAct='{hidden_act_function_name}', RopeTheta={rope_theta_value}\")\n",
        "\n",
        "    # --- 2. Load Maps from Cell 2 ---\n",
        "    print(f\"\\nLoading maps from {DB_PATH}...\")\n",
        "    if not knowledge_map_filepath.is_file(): raise FileNotFoundError(f\"Knowledge map file not found: {knowledge_map_filepath}\")\n",
        "    if not knowledge_index_filepath.is_file(): raise FileNotFoundError(f\"Knowledge index file not found: {knowledge_index_filepath}\")\n",
        "\n",
        "    with open(knowledge_map_filepath, 'rb') as f: knowledge_map = pickle.load(f)\n",
        "    print(f\"Loaded knowledge map ({len(knowledge_map)} entries).\")\n",
        "\n",
        "    if name_map_filepath.is_file():\n",
        "         with open(name_map_filepath, 'rb') as f: name_id_map_data = pickle.load(f)\n",
        "         print(f\"Loaded name ID map.\")\n",
        "    else: print(f\"Warning: Name ID map file not found at {name_map_filepath}\")\n",
        "\n",
        "    # --- 3. Initialize Veector with Knowledge Index ---\n",
        "    print(f\"\\nInitializing Veector instance for processor creation...\")\n",
        "    print(f\"Loading initial index from: '{knowledge_index_filepath.name}'\")\n",
        "    vec_processor = Veector(db_dir=DB_PATH, initial_index_path=knowledge_index_filepath)\n",
        "    print(f\"Veector initialized. DB Index entries loaded: {len(vec_processor.db.index)}\")\n",
        "    if len(vec_processor.db.index) == 0: print(f\"WARNING: Loaded knowledge index is empty!\")\n",
        "    vec_processor.db.index_path = main_index_filepath\n",
        "    print(f\"Default index save path set to: '{vec_processor.db.index_path.name}'\")\n",
        "    vec_processor.db._index_dirty = True # Mark dirty as we will add processors\n",
        "\n",
        "    # --- 4. Helper Functions ---\n",
        "    def find_knowledge_id(hf_param_name: str) -> Optional[str]:\n",
        "        if knowledge_map is None: return None\n",
        "        return knowledge_map.get(hf_param_name)\n",
        "\n",
        "    def create_and_save_processor(name: str, coord: TensorCoordinate, tags: List[int], interface: Dict, ops_sequences: Dict):\n",
        "        global processor_errors # Use global to modify counter\n",
        "        # processor_map and vec_processor are accessible from outer scope\n",
        "        proc_id: Optional[str] = None\n",
        "        try:\n",
        "            print(f\"  Defining Processor: {name} at {coord}\")\n",
        "            tensor_structure = vec_processor.create_tensor(\n",
        "                coord=coord, tensor_type=\"processor\", tags=tags,\n",
        "                interface=interface, ops_sequences=ops_sequences,\n",
        "                status=\"active\", name_id=-1\n",
        "            )\n",
        "            if not validate_tensor(tensor_structure): raise ValueError(f\"Invalid list structure for {name}\")\n",
        "\n",
        "            proc_id = vec_processor.save_tensor(tensor_structure)\n",
        "\n",
        "            if proc_id:\n",
        "                map_key = \"\"\n",
        "                if \"Embedding\" in name: map_key = \"embedding\"\n",
        "                elif \"Final Norm\" in name: map_key = \"final_norm\"\n",
        "                elif \"LM Head\" in name: map_key = \"lm_head\"\n",
        "                elif \"Attention Processor L\" in name:\n",
        "                  try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"attn_{layer_idx}\"\n",
        "                  except: pass\n",
        "                elif \"FFN Processor L\" in name:\n",
        "                  try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"ffn_{layer_idx}\"\n",
        "                  except: pass\n",
        "\n",
        "                if map_key:\n",
        "                    processor_map[map_key] = proc_id # Modify outer scope dict\n",
        "                    print(f\"    SUCCESS: Saved {name} with ID: {proc_id} (Key: {map_key})\")\n",
        "                else: print(f\"    WARN: Saved {name} with ID: {proc_id}, but no map key.\")\n",
        "            else:\n",
        "                processor_errors += 1; print(f\"    ERROR saving processor {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR during definition/saving of processor {name}: {e}\")\n",
        "            traceback.print_exc(); processor_errors += 1\n",
        "        return proc_id\n",
        "\n",
        "    # --- 5. Define and Save Processors ---\n",
        "    print(\"\\n--- Defining and Saving Veector Processor Tensors ---\")\n",
        "\n",
        "    # 5.A Embedding Processor\n",
        "    print(\"\\n--- Defining Embedding Processor ---\")\n",
        "    try:\n",
        "        coord = TensorCoordinate(layer=-1, group=PROCESSOR_GROUP_IDX, nest=0, x=0)\n",
        "        tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, MODEL_TAG]\n",
        "        param_name = \"embedding_matrix\"; hf_name = \"model.embed_tokens.weight\"\n",
        "        kn_tags = [TAG_COMP_EMBEDDING, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_quant]\n",
        "        kid = find_knowledge_id(hf_name)\n",
        "        if not kid: raise ValueError(f\"Embedding knowledge ID not found for '{hf_name}'.\")\n",
        "        interface = {\"inputs\": [{\"name\":\"token_ids\", \"dtype\":\"int64\"}], \"outputs\": [{\"name\":\"hidden_states\", \"dtype\":\"float16\"}], \"knowledge_needed\": [{\"param_name\": param_name, \"tags\": kn_tags, \"knowledge_id\": kid}]}\n",
        "        ops_sequences = {'default': [[OP_EMBEDDING_LOOKUP, {\"embedding_matrix\": param_name}]]}\n",
        "        create_and_save_processor(\"Embedding Processor\", coord, tags, interface, ops_sequences)\n",
        "    except Exception as e: print(f\"Error defining Embedding Processor: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "\n",
        "    # 5.B Transformer Layers\n",
        "    print(f\"\\n--- Defining Transformer Layer Processors (0 to {num_layers-1}) ---\")\n",
        "    for layer_idx in range(num_layers):\n",
        "        layer_tag = tag_layer(layer_idx)\n",
        "        print(f\"  Processing Layer {layer_idx}...\")\n",
        "\n",
        "        # --- Attention Processor ---\n",
        "        try:\n",
        "            coord_attn = TensorCoordinate(layer=layer_idx, group=PROCESSOR_GROUP_IDX, nest=0, x=0)\n",
        "            tags_attn = [TAG_TYPE_PROCESSOR, TAG_FUNC_ATTENTION, layer_tag, MODEL_TAG]\n",
        "            kn_defs_attn = [\n",
        "                {\"p\":f\"L{layer_idx}_input_norm_w\", \"t\":[TAG_COMP_LAYERNORM, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.input_layernorm.weight\"},\n",
        "                {\"p\":f\"L{layer_idx}_q_w\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"},\n",
        "                {\"p\":f\"L{layer_idx}_q_b\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, MODEL_TAG, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.bias\", \"opt\": True},\n",
        "                {\"p\":f\"L{layer_idx}_k_w\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"},\n",
        "                {\"p\":f\"L{layer_idx}_k_b\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, MODEL_TAG, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.bias\", \"opt\": True},\n",
        "                {\"p\":f\"L{layer_idx}_v_w\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"},\n",
        "                {\"p\":f\"L{layer_idx}_v_b\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, MODEL_TAG, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.bias\", \"opt\": True},\n",
        "                {\"p\":f\"L{layer_idx}_o_w\",   \"t\":[TAG_COMP_ATTN_O, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"},\n",
        "            ]\n",
        "            knowledge_needs_attn = []\n",
        "            missing_essential = False\n",
        "            for kdef in kn_defs_attn:\n",
        "                kid = find_knowledge_id(kdef[\"f\"])\n",
        "                is_opt = kdef.get(\"opt\", False)\n",
        "                if kid: knowledge_needs_attn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "                elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for Attn L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "            if not missing_essential:\n",
        "                interface_attn = { \"inputs\": [ {\"name\": \"hidden_state_in\"}, {\"name\": \"residual_input\"}, {\"name\": \"position_ids\"}, {\"name\": \"past_key\", \"optional\": True}, {\"name\": \"past_value\", \"optional\": True}, {\"name\": \"start_pos\", \"dtype\": \"int\", \"optional\": True}, {\"name\": \"total_seq_len\", \"dtype\": \"int\", \"optional\": True} ], \"outputs\": [{\"name\": \"attn_block_output\"}], \"knowledge_needed\": knowledge_needs_attn }\n",
        "                ops_sequences_attn = {'default': [ [OP_STORE, 'residual_attn'], [OP_QWEN2_RMSNORM, {\"norm_weight\": f\"L{layer_idx}_input_norm_w\", \"eps\": rms_norm_eps}], [OP_QWEN2_ATTENTION, {\"q_weights\": f\"L{layer_idx}_q_w\", \"k_weights\": f\"L{layer_idx}_k_w\", \"v_weights\": f\"L{layer_idx}_v_w\", \"o_weights\": f\"L{layer_idx}_o_w\", \"q_bias\": f\"L{layer_idx}_q_b\", \"k_bias\": f\"L{layer_idx}_k_b\", \"v_bias\": f\"L{layer_idx}_v_b\", \"position_ids\": \"position_ids\", \"past_key\": \"past_key\", \"past_value\": \"past_value\", \"start_pos\": \"start_pos\", \"total_seq_len\": \"total_seq_len\", \"num_heads\": num_attention_heads, \"num_kv_heads\": num_key_value_heads, \"head_dim\": head_dim, \"hidden_size\": hidden_size, \"layer_idx\": layer_idx, \"rope_theta\": rope_theta_value}], [OP_STORE, 'attn_tuple_output'], [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_1], [OP_STORE, 'k_cache_out'], [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_2], [OP_STORE, 'v_cache_out'], [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_0], [OP_ADD, {\"input_a\": \"residual_attn\", \"input_b\": \"_\"}] ]}\n",
        "                create_and_save_processor(f\"Attention Processor L{layer_idx}\", coord_attn, tags_attn, interface_attn, ops_sequences_attn)\n",
        "            else: processor_errors += 1\n",
        "        except Exception as e: print(f\"Error defining Attn L{layer_idx}: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "\n",
        "        # --- FFN Processor (Handles tuple output from MLP) ---\n",
        "        try:\n",
        "            coord_ffn = TensorCoordinate(layer=layer_idx, group=PROCESSOR_GROUP_IDX, nest=0, x=1)\n",
        "            tags_ffn = [TAG_TYPE_PROCESSOR, TAG_FUNC_FFN, layer_tag, MODEL_TAG]\n",
        "            kn_defs_ffn = [\n",
        "                {\"p\": f\"L{layer_idx}_post_attn_norm_w\", \"t\": [TAG_COMP_LAYERNORM, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\": f\"model.layers.{layer_idx}.post_attention_layernorm.weight\"},\n",
        "                {\"p\": f\"L{layer_idx}_gate_w\", \"t\": [TAG_COMP_FFN_GATE, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.gate_proj.weight\"},\n",
        "                {\"p\": f\"L{layer_idx}_up_w\",   \"t\": [TAG_COMP_FFN_UP, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights],    \"f\": f\"model.layers.{layer_idx}.mlp.up_proj.weight\"},\n",
        "                {\"p\": f\"L{layer_idx}_down_w\", \"t\": [TAG_COMP_FFN_DOWN, layer_tag, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.down_proj.weight\"},\n",
        "            ]\n",
        "            knowledge_needs_ffn = []\n",
        "            missing_essential = False\n",
        "            for kdef in kn_defs_ffn:\n",
        "                kid = find_knowledge_id(kdef[\"f\"])\n",
        "                is_opt = kdef.get(\"opt\", False)\n",
        "                if kid: knowledge_needs_ffn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "                elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for FFN L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "            if not missing_essential:\n",
        "                interface_ffn = { \"inputs\": [{\"name\":\"attn_block_output\"}, {\"name\":\"residual_input\"}], \"outputs\": [{\"name\":\"layer_output\"}], \"knowledge_needed\": knowledge_needs_ffn }\n",
        "                # --- UPDATED FFN Operation Sequence ---\n",
        "                ops_sequences_ffn = {'default': [\n",
        "                    # 1. Store input for the second residual connection\n",
        "                    [OP_STORE, 'residual_ffn'],\n",
        "                    # 2. Apply Post-Attention RMS Normalization\n",
        "                    [OP_QWEN2_RMSNORM, {\"norm_weight\": f\"L{layer_idx}_post_attn_norm_w\", \"eps\": rms_norm_eps}],\n",
        "                    # 3. Execute the MLP block operation (now returns a tuple)\n",
        "                    [OP_QWEN2_MLP, {\n",
        "                        \"gate_weights\": f\"L{layer_idx}_gate_w\",\n",
        "                        \"up_weights\": f\"L{layer_idx}_up_w\",\n",
        "                        \"down_weights\": f\"L{layer_idx}_down_w\",\n",
        "                        \"hidden_act\": hidden_act_function_name # Use variable from config\n",
        "                    }],\n",
        "                    # 4. Store the results tuple from MLP\n",
        "                    [OP_STORE, 'mlp_results_tuple'],\n",
        "                    # 5. (Optional) Store intermediate MLP results for debugging\n",
        "                    # These assume OP_GET_TUPLE_ELEM_X are defined up to index 3\n",
        "                    [OP_LOAD, 'mlp_results_tuple'], [OP_GET_TUPLE_ELEM_1], [OP_STORE, f'L{layer_idx}_dbg_mlp_gate_out'],\n",
        "                    [OP_LOAD, 'mlp_results_tuple'], [OP_GET_TUPLE_ELEM_2], [OP_STORE, f'L{layer_idx}_dbg_mlp_up_out'],\n",
        "                    [OP_LOAD, 'mlp_results_tuple'], [OP_GET_TUPLE_ELEM_3], [OP_STORE, f'L{layer_idx}_dbg_mlp_activated'],\n",
        "                    # 6. Extract the final MLP output (element 0) to be used in residual add\n",
        "                    [OP_LOAD, 'mlp_results_tuple'],\n",
        "                    [OP_GET_TUPLE_ELEM_0], # current_data is now the final MLP output tensor (before residual)\n",
        "                    # 7. Add the second residual connection\n",
        "                    [OP_ADD, {\"input_a\": \"residual_ffn\", \"input_b\": \"_\"}] # Adds stored input to MLP output\n",
        "                ]}\n",
        "                # --- END UPDATED FFN Operation Sequence ---\n",
        "                create_and_save_processor(f\"FFN Processor L{layer_idx}\", coord_ffn, tags_ffn, interface_ffn, ops_sequences_ffn)\n",
        "            else: processor_errors += 1\n",
        "        except Exception as e: print(f\"Error defining FFN L{layer_idx}: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "    # --- End Layer Loop ---\n",
        "\n",
        "    # 5.C Final Norm Processor\n",
        "    print(\"\\n--- Defining Final Norm Processor ---\")\n",
        "    try:\n",
        "        coord = TensorCoordinate(layer=-1, group=PROCESSOR_GROUP_IDX, nest=0, x=1)\n",
        "        tags = [TAG_TYPE_PROCESSOR, TAG_COMP_LAYERNORM, MODEL_TAG]\n",
        "        kn_tags = [TAG_COMP_LAYERNORM, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_weights]\n",
        "        hf_name = \"model.norm.weight\"; kid = find_knowledge_id(hf_name)\n",
        "        if not kid: raise ValueError(f\"Final Norm knowledge ID not found for '{hf_name}'.\")\n",
        "        knowledge_needs = [{\"param_name\": \"norm_weight\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "        interface = {\"inputs\": [{\"name\":\"final_hidden_state\"}], \"outputs\": [{\"name\":\"final_normed_state\"}], \"knowledge_needed\": knowledge_needs}\n",
        "        ops_sequences = {'default': [[OP_QWEN2_RMSNORM, {\"norm_weight\": \"norm_weight\", \"eps\": rms_norm_eps}]]}\n",
        "        create_and_save_processor(\"Final Norm Processor\", coord, tags, interface, ops_sequences)\n",
        "    except Exception as e: print(f\"Error defining Final Norm Processor: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "\n",
        "    # 5.D LM Head Processor\n",
        "    print(\"\\n--- Defining LM Head Processor ---\")\n",
        "    try:\n",
        "        coord = TensorCoordinate(layer=-1, group=PROCESSOR_GROUP_IDX, nest=0, x=2)\n",
        "        tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_LINEAR, MODEL_TAG]\n",
        "        kn_tags = [TAG_COMP_LM_HEAD, MODEL_TAG, TAG_COMP_WEIGHTS, prec_tag_quant] # Expect quantized\n",
        "        hf_name = \"lm_head.weight\"; kid = find_knowledge_id(hf_name)\n",
        "        if not kid: raise ValueError(f\"LM Head knowledge ID not found for '{hf_name}'.\")\n",
        "        knowledge_needs = [{\"param_name\": \"lm_head_weights\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "        interface = {\"inputs\": [{\"name\":\"final_normed_state\"}], \"outputs\": [{\"name\":\"logits\"}], \"knowledge_needed\": knowledge_needs}\n",
        "        ops_sequences = {'default': [[OP_LINEAR_HEAD, {\"weights\": \"lm_head_weights\"}]]}\n",
        "        create_and_save_processor(\"LM Head Processor\", coord, tags, interface, ops_sequences)\n",
        "    except Exception as e: print(f\"Error defining LM Head Processor: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "\n",
        "    # --- 6. Finalization ---\n",
        "    print(f\"\\n--- Finalizing Cell 3 ({processor_errors} errors during processor creation) ---\")\n",
        "\n",
        "    # Save Processor Map\n",
        "    processor_map_filepath = DB_PATH / f\"{MODEL_NAME}_proc_map.pkl\"\n",
        "    try:\n",
        "        expected_proc_count = 3 + 2 * num_layers # Embed, Norm, Head + 2*Layers\n",
        "        if processor_errors == 0 and len(processor_map) == expected_proc_count:\n",
        "            print(f\"Saving processor map ({len(processor_map)} entries) to {processor_map_filepath}...\")\n",
        "            with open(processor_map_filepath, 'wb') as f: pickle.dump(processor_map, f)\n",
        "            print(f\"Processor map saved successfully.\")\n",
        "        elif processor_errors > 0:\n",
        "            print(f\"Processor map NOT saved due to {processor_errors} errors during creation.\")\n",
        "        else:\n",
        "             print(f\"WARN: Processor map has incorrect entry count ({len(processor_map)} vs {expected_proc_count}). NOT SAVED.\")\n",
        "    except Exception as e: print(f\"Error saving processor map: {e}\")\n",
        "\n",
        "    # --- 7. Cleanup ---\n",
        "    print(\"\\nCleaning up resources for Cell 3...\")\n",
        "    if vec_processor and hasattr(vec_processor, 'db') and vec_processor.db:\n",
        "        print(f\"Closing Veector DB connection (saving main index to '{main_index_filepath.name}')...\")\n",
        "        print(f\"Index size before final save in Cell 3: {len(vec_processor.db.index)}\")\n",
        "        vec_processor.db.close() # Saves the main index (knowledge + processors)\n",
        "        print(\"Veector DB connection closed.\")\n",
        "    if 'vec_processor' in locals():\n",
        "        del vec_processor\n",
        "        print(\"Veector instance deleted.\")\n",
        "\n",
        "    gc.collect()\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"CUDA cache cleared.\")\n",
        "    print(\"Garbage collection run.\")\n",
        "\n",
        "except Exception as cell3_e:\n",
        "    print(f\"\\n---!!! FATAL ERROR in Cell 3 Execution: {cell3_e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "    # Ensure cleanup happens even on error\n",
        "    if 'vec_processor' in locals() and vec_processor and hasattr(vec_processor, 'db') and vec_processor.db:\n",
        "        try: vec_processor.db.close()\n",
        "        except: pass\n",
        "    if 'vec_processor' in locals(): del vec_processor\n",
        "    gc.collect()\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "finally:\n",
        "    end_cell3_time = time.time()\n",
        "    if processor_errors == 0:\n",
        "        print(f\"\\n--- Cell 3 Finished Successfully in {end_cell3_time - start_cell3_time:.2f} seconds ---\")\n",
        "    else:\n",
        "        print(f\"\\n--- Cell 3 Finished with {processor_errors} ERRORS in {end_cell3_time - start_cell3_time:.2f} seconds ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E4rfZFE82aVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Reference HF Run ===\n",
        "# Version: 1.3 (Added hooks for MLP intermediate outputs)\n",
        "# Loads the original Hugging Face model and tokenizer.\n",
        "# Runs a forward pass with a test prompt to capture intermediate outputs using hooks.\n",
        "# Saves these outputs to a .pkl file for later comparison.\n",
        "# Relies only on configuration variables from Cell 1.\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Imports (Redundant but ensures independence) ---\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, PreTrainedTokenizer\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR in Cell 4: Missing imports: {e}\")\n",
        "    raise SystemExit(f\"Missing essential libraries: {e}\")\n",
        "\n",
        "# --- Configuration (Load from Cell 1 variables) ---\n",
        "# These should be defined in the global scope by running Cell 1 first\n",
        "if 'MODEL_NAME' not in globals(): raise NameError(\"MODEL_NAME not defined. Run Cell 1.\")\n",
        "if 'HF_MODEL_SOURCE' not in globals(): raise NameError(\"HF_MODEL_SOURCE not defined. Run Cell 1.\")\n",
        "if 'DB_PATH' not in globals(): raise NameError(\"DB_PATH not defined. Run Cell 1.\")\n",
        "if 'PROMPT_FOR_TESTING' not in globals(): raise NameError(\"PROMPT_FOR_TESTING not defined. Run Cell 1.\")\n",
        "\n",
        "print(f\"--- Running Cell 4: Reference HF Run for {MODEL_NAME} ---\")\n",
        "print(f\"    Using Prompt: '{PROMPT_FOR_TESTING}'\")\n",
        "start_cell4_time = time.time()\n",
        "\n",
        "# --- Output File ---\n",
        "# Define the path where the reference outputs will be saved\n",
        "output_dir_ref = DB_PATH # Use DB_PATH defined in Cell 1\n",
        "output_filename_ref = f\"{MODEL_NAME}_hf_reference_outputs_fp32.pkl\" # File will contain more outputs now\n",
        "output_filepath_ref = output_dir_ref / output_filename_ref\n",
        "\n",
        "# --- Initialization ---\n",
        "tokenizer_ref: Optional[PreTrainedTokenizer] = None\n",
        "model_ref_fp32 = None\n",
        "hf_outputs_ref: Dict[str, np.ndarray] = {} # Dictionary to store captured outputs\n",
        "hook_handles_ref: List[Any] = []\n",
        "input_ids_torch_ref: Optional[torch.Tensor] = None\n",
        "\n",
        "# --- Hook Functions ---\n",
        "# Hook to capture OUTPUT of a module\n",
        "def get_output_hook_ref(name: str):\n",
        "    \"\"\"Creates a hook to capture the module's output.\"\"\"\n",
        "    def hook_fn(module: nn.Module, input_args: Tuple[Any, ...], output: Any):\n",
        "        \"\"\"Captures the output and stores it in hf_outputs_ref.\"\"\"\n",
        "        actual_output: Optional[torch.Tensor] = None\n",
        "        if isinstance(output, torch.Tensor): actual_output = output\n",
        "        elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], torch.Tensor): actual_output = output[0]\n",
        "        elif isinstance(output, dict) and 'last_hidden_state' in output and isinstance(output['last_hidden_state'], torch.Tensor): actual_output = output['last_hidden_state']\n",
        "        elif isinstance(output, tuple) and len(output) > 0 and name.endswith(\"_attn_out\"):\n",
        "             if isinstance(output[0], torch.Tensor): actual_output = output[0]\n",
        "\n",
        "        if actual_output is not None:\n",
        "            hf_outputs_ref[name] = actual_output.detach().cpu().numpy().astype(np.float32)\n",
        "        else:\n",
        "            pass # Ignore non-tensor outputs silently\n",
        "    return hook_fn\n",
        "\n",
        "# Hook to capture INPUT of a module (using pre-hook)\n",
        "def get_input_hook_ref(name: str):\n",
        "    \"\"\"Creates a pre-hook to capture the module's first input argument.\"\"\"\n",
        "    def pre_hook_fn(module: nn.Module, input_args: Tuple[Any, ...]):\n",
        "        \"\"\"Captures the first input tensor and stores it.\"\"\"\n",
        "        if input_args and isinstance(input_args[0], torch.Tensor):\n",
        "            hf_outputs_ref[name] = input_args[0].detach().cpu().numpy().astype(np.float32)\n",
        "        else:\n",
        "            print(f\"  [HOOK_IN_REF] WARN: Could not capture input tensor for {name}. Input type: {type(input_args[0]) if input_args else 'None'}\")\n",
        "    return pre_hook_fn\n",
        "\n",
        "try:\n",
        "    # --- 1. Load Tokenizer ---\n",
        "    print(f\"\\nLoading Tokenizer from: {HF_MODEL_SOURCE}\")\n",
        "    tokenizer_ref = AutoTokenizer.from_pretrained(HF_MODEL_SOURCE, trust_remote_code=True, use_fast=False)\n",
        "    print(f\"Tokenizer class: {tokenizer_ref.__class__.__name__}\")\n",
        "\n",
        "    user_token = \"<|User|>\"; assistant_token = \"<|Assistant|>\"\n",
        "    num_added = tokenizer_ref.add_special_tokens({'additional_special_tokens': [user_token, assistant_token]})\n",
        "    print(f\"Added {num_added} special tokens explicitly.\")\n",
        "\n",
        "    bos_token_id_ref = tokenizer_ref.bos_token_id\n",
        "    eos_token_id_ref = tokenizer_ref.eos_token_id\n",
        "    user_token_id_ref = tokenizer_ref.convert_tokens_to_ids(user_token)\n",
        "    assistant_token_id_ref = tokenizer_ref.convert_tokens_to_ids(assistant_token)\n",
        "\n",
        "    if isinstance(user_token_id_ref, str) or user_token_id_ref == tokenizer_ref.unk_token_id: raise ValueError(\"User token ID not found.\")\n",
        "    if isinstance(assistant_token_id_ref, str) or assistant_token_id_ref == tokenizer_ref.unk_token_id: raise ValueError(\"Assistant token ID not found.\")\n",
        "    if tokenizer_ref.pad_token_id is None: tokenizer_ref.pad_token_id = eos_token_id_ref if eos_token_id_ref is not None else tokenizer_ref.vocab_size\n",
        "    print(f\"Tokens: BOS={bos_token_id_ref}, EOS={eos_token_id_ref}, PAD={tokenizer_ref.pad_token_id}, User={user_token_id_ref}, Assistant={assistant_token_id_ref}\")\n",
        "\n",
        "    # --- 2. Prepare Input (Using ONNX-style prompt - NO BOS) ---\n",
        "    print(\"\\nPreparing Input IDs (ONNX-style, no BOS)...\")\n",
        "    user_text_ids_ref = tokenizer_ref.encode(PROMPT_FOR_TESTING, add_special_tokens=False)\n",
        "    input_ids_list_ref = []\n",
        "    # if bos_token_id_ref is not None: input_ids_list_ref.append(bos_token_id_ref) # BOS removed\n",
        "    input_ids_list_ref.append(user_token_id_ref)\n",
        "    input_ids_list_ref.extend(user_text_ids_ref)\n",
        "    input_ids_list_ref.append(assistant_token_id_ref)\n",
        "\n",
        "    prompt_input_ids_np_ref = np.array([input_ids_list_ref], dtype=np.int64)\n",
        "    input_ids_torch_ref = torch.tensor(prompt_input_ids_np_ref)\n",
        "    print(f\"Input IDs shape: {input_ids_torch_ref.shape}\")\n",
        "    # Print decoded input for verification\n",
        "    print(f\"Decoded Input for Reference Run: '{tokenizer_ref.decode(input_ids_list_ref)}'\")\n",
        "\n",
        "    # --- 3. Load Model ---\n",
        "    print(f\"\\nLoading HF Model {HF_MODEL_SOURCE} with float32...\")\n",
        "    model_ref_fp32 = AutoModelForCausalLM.from_pretrained(HF_MODEL_SOURCE, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    model_ref_fp32.eval()\n",
        "    device_ref = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device_ref}\")\n",
        "    model_ref_fp32.to(device_ref)\n",
        "    input_ids_torch_ref = input_ids_torch_ref.to(device_ref)\n",
        "    print(f\"HF Model loaded to device: {model_ref_fp32.device}\")\n",
        "\n",
        "    # --- 4. Register Hooks (Updated for MLP intermediates) ---\n",
        "    print(\"\\nRegistering hooks...\")\n",
        "    model_config_ref = model_ref_fp32.config\n",
        "    num_layers_ref = model_config_ref.num_hidden_layers\n",
        "    hook_handles_ref.clear() # Clear previous handles\n",
        "\n",
        "    # Hook on Embedding output\n",
        "    hook_handles_ref.append(model_ref_fp32.model.embed_tokens.register_forward_hook(get_output_hook_ref(\"embed_tokens\")))\n",
        "\n",
        "    # Hooks for each layer\n",
        "    for i in range(num_layers_ref):\n",
        "        layer = model_ref_fp32.model.layers[i]\n",
        "        # Attention block related hooks\n",
        "        hook_handles_ref.append(layer.input_layernorm.register_forward_hook(get_output_hook_ref(f\"L{i}_input_norm_out\")))\n",
        "        hook_handles_ref.append(layer.self_attn.register_forward_hook(get_output_hook_ref(f\"L{i}_attn_out\"))) # Attention output before residual\n",
        "        hook_handles_ref.append(layer.post_attention_layernorm.register_forward_pre_hook(get_input_hook_ref(f\"L{i}_attn_block_output\"))) # Input to post_attn_norm = Output after first residual\n",
        "\n",
        "        # MLP block related hooks\n",
        "        hook_handles_ref.append(layer.post_attention_layernorm.register_forward_hook(get_output_hook_ref(f\"L{i}_post_attn_norm_out\"))) # Output of norm before MLP\n",
        "        # --- MLP Intermediate Hooks ---\n",
        "        hook_handles_ref.append(layer.mlp.gate_proj.register_forward_hook(get_output_hook_ref(f\"L{i}_mlp_gate_out\"))) # Output of gate projection\n",
        "        hook_handles_ref.append(layer.mlp.up_proj.register_forward_hook(get_output_hook_ref(f\"L{i}_mlp_up_out\"))) # Output of up projection\n",
        "        hook_handles_ref.append(layer.mlp.down_proj.register_forward_pre_hook(get_input_hook_ref(f\"L{i}_mlp_act_mul_up\"))) # Input to down_proj = result of act(gate)*up\n",
        "        hook_handles_ref.append(layer.mlp.down_proj.register_forward_hook(get_output_hook_ref(f\"L{i}_mlp_down_out\"))) # Output of down_proj (before residual)\n",
        "        # --- End MLP Intermediate Hooks ---\n",
        "        hook_handles_ref.append(layer.mlp.register_forward_hook(get_output_hook_ref(f\"L{i}_mlp_out\"))) # Output of the whole MLP module (usually same as down_out)\n",
        "        hook_handles_ref.append(layer.register_forward_hook(get_output_hook_ref(f\"L{i}_layer_output\"))) # Output of the entire layer (after second residual add)\n",
        "\n",
        "    # Hook on Final Norm output\n",
        "    hook_handles_ref.append(model_ref_fp32.model.norm.register_forward_hook(get_output_hook_ref(\"final_norm\")))\n",
        "    # Hook on LM Head output\n",
        "    hook_handles_ref.append(model_ref_fp32.lm_head.register_forward_hook(get_output_hook_ref(\"lm_head\")))\n",
        "\n",
        "    print(f\"Registered {len(hook_handles_ref)} hooks.\")\n",
        "\n",
        "    # --- 5. Run Forward Pass ---\n",
        "    print(\"\\nRunning HF model forward pass (float32)...\")\n",
        "    with torch.no_grad():\n",
        "        hf_model_output = model_ref_fp32(input_ids=input_ids_torch_ref, use_cache=False)\n",
        "    print(\"HF forward pass complete.\")\n",
        "\n",
        "    # --- 6. Save Outputs ---\n",
        "    if hf_outputs_ref:\n",
        "        print(f\"\\nSaving Captured Float32 Outputs (including MLP intermediates) to {output_filepath_ref}...\")\n",
        "        output_filepath_ref.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(output_filepath_ref, 'wb') as f:\n",
        "            pickle.dump(hf_outputs_ref, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Successfully saved {len(hf_outputs_ref)} reference outputs.\")\n",
        "        # Print some keys to confirm new outputs are present\n",
        "        saved_keys = list(hf_outputs_ref.keys())\n",
        "        print(f\"  Example saved keys: {saved_keys[:5]}...{saved_keys[-5:]}\")\n",
        "        # Check for one of the new MLP keys\n",
        "        if \"L0_mlp_gate_out\" in saved_keys:\n",
        "             print(\"  Confirmed MLP intermediate keys (e.g., 'L0_mlp_gate_out') are present.\")\n",
        "        else:\n",
        "             print(\"  WARNING: MLP intermediate keys NOT found in saved outputs!\")\n",
        "    else:\n",
        "        print(\"\\n--- No outputs captured from HF model, skipping save. ---\")\n",
        "\n",
        "except Exception as cell4_e:\n",
        "    print(f\"\\n---!!! FATAL ERROR in Cell 4 Execution: {cell4_e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    # --- 7. Cleanup ---\n",
        "    print(\"\\nCleaning up resources for Cell 4...\")\n",
        "    if hook_handles_ref:\n",
        "        print(f\"Removing {len(hook_handles_ref)} hooks...\")\n",
        "        for handle in hook_handles_ref: handle.remove()\n",
        "        print(\"Removed hooks.\")\n",
        "        hook_handles_ref.clear()\n",
        "    if model_ref_fp32 is not None:\n",
        "        del model_ref_fp32\n",
        "        print(\"HF model deleted.\")\n",
        "    if 'tokenizer_ref' in locals(): del tokenizer_ref\n",
        "    if 'input_ids_torch_ref' in locals(): del input_ids_torch_ref\n",
        "    if 'hf_outputs_ref' in locals(): hf_outputs_ref.clear()\n",
        "\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"CUDA cache cleared.\")\n",
        "    gc.collect()\n",
        "    print(\"Garbage collection run.\")\n",
        "\n",
        "    end_cell4_time = time.time()\n",
        "    print(f\"\\n--- Cell 4 Finished in {end_cell4_time - start_cell4_time:.2f} seconds ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "asjSa9br2iMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rb5TvtZ2GRF"
      },
      "outputs": [],
      "source": [
        "# === Cell 5 (FFN L0 Test Cell - Intermediate Comparison) ===\n",
        "# Version: 1.2 (Fixed NameError in final comparison call)\n",
        "# Loads prerequisites including reference file with MLP intermediate outputs.\n",
        "# Initializes Veector.\n",
        "# Loads the REFERENCE Attention block output from the HF run.\n",
        "# Runs ONLY the Layer 0 FFN processor (which now stores intermediates in context).\n",
        "# Extracts and Compares intermediate MLP outputs (gate, up, activated, down)\n",
        "# as well as the final layer output against the reference values.\n",
        "# Relies on Cell 1 variables and output files from Cell 3 (v1.2+) and Cell 4 (v1.3+).\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Imports ---\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoConfig, PreTrainedTokenizer\n",
        "    from core import Veector\n",
        "    from tensors import TensorCoordinate, GROUP_IDX_QWEN_KNOWLEDGE\n",
        "    from operations import softmax\n",
        "    from veectordb import VeectorDB\n",
        "    print(\"External and Veector libraries imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR in FFN Test Cell: Missing imports: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Configuration ---\n",
        "if 'MODEL_NAME' not in globals(): raise NameError(\"MODEL_NAME not defined. Run Cell 1.\")\n",
        "if 'HF_MODEL_SOURCE' not in globals(): raise NameError(\"HF_MODEL_SOURCE not defined. Run Cell 1.\")\n",
        "if 'DB_PATH' not in globals(): raise NameError(\"DB_PATH not defined. Run Cell 1.\")\n",
        "if 'KNOWLEDGE_GROUP_IDX' not in globals(): KNOWLEDGE_GROUP_IDX = GROUP_IDX_QWEN_KNOWLEDGE # Fallback\n",
        "\n",
        "# --- Test Parameters ---\n",
        "NEST_LEVEL: int = 1 # Precision level for the FFN processor\n",
        "ATOL: float = 1e-2 # Use increased tolerance\n",
        "RTOL: float = 1e-2 # Use increased tolerance\n",
        "\n",
        "print(f\"--- Running Cell 5.1 (FFN L0 Intermediate Test v1.2) for {MODEL_NAME} ---\")\n",
        "print(f\"    DB Path: {DB_PATH.resolve()}\")\n",
        "print(f\"    Nest Level: {NEST_LEVEL}\")\n",
        "print(f\"    Comparison Tolerances: atol={ATOL}, rtol={RTOL}\")\n",
        "start_cell_ffn_test_time = time.time()\n",
        "\n",
        "# --- File Paths ---\n",
        "proc_map_filepath = DB_PATH / f\"{MODEL_NAME}_proc_map.pkl\"\n",
        "# Expecting reference file generated by Cell 4 v1.3+\n",
        "ref_output_filepath = DB_PATH / f\"{MODEL_NAME}_hf_reference_outputs_fp32.pkl\"\n",
        "main_index_filepath = DB_PATH / VeectorDB.INDEX_FILENAME\n",
        "\n",
        "# --- Initialization ---\n",
        "processor_map_test_ffn: Optional[Dict[str, str]] = None\n",
        "hf_outputs_test_ffn: Optional[Dict[str, np.ndarray]] = None\n",
        "vec_test_ffn: Optional[Veector] = None\n",
        "error_occurred_test_ffn = False\n",
        "difference_found_test_ffn = False # Reset flag\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def log_memory_usage_ffn(stage: str):\n",
        "    try: process = psutil.Process(os.getpid()); mem_info = process.memory_info(); vmem = psutil.virtual_memory(); print(f\"  [MEM_LOG_FFN] {stage}: RSS={mem_info.rss / (1024**2):.2f} MB, RAM Used={vmem.percent:.1f}%\")\n",
        "    except Exception as e: print(f\"  [MEM_LOG_FFN] Error getting memory usage: {e}\")\n",
        "def log_tensor_stats_ffn(name: str, tensor: Optional[np.ndarray], log_values: bool = False):\n",
        "    if tensor is None: print(f\"  [STATS_FFN] {name}: None\"); return\n",
        "    has_nan = np.any(np.isnan(tensor)); shape_str = str(tensor.shape); dtype_str = str(tensor.dtype)\n",
        "    print(f\"  [STATS_FFN] {name}: shape={shape_str}, dtype={dtype_str}, NaN={has_nan}\")\n",
        "    if (has_nan or log_values) and tensor.size > 0 :\n",
        "        try: sample_slice = tensor.flatten()[:5].tolist(); print(f\"               Sample: {sample_slice}\")\n",
        "        except Exception as e: print(f\"               Error getting sample: {e}\")\n",
        "\n",
        "# --- Renamed Comparison Function ---\n",
        "def compare_values(key_ref: str, key_vec: str, vec_out: Optional[np.ndarray], ref_outputs: Dict) -> bool:\n",
        "    \"\"\"Compares Veector output with HF reference and logs the result. Returns True if difference found.\"\"\"\n",
        "    global difference_found_test_ffn # Allow modification\n",
        "    if difference_found_test_ffn: return True # Skip if already different\n",
        "\n",
        "    print(f\"  Comparing Veector output ('{key_vec}') with Reference ('{key_ref}')...\")\n",
        "    hf_out = ref_outputs.get(key_ref)\n",
        "\n",
        "    if hf_out is None or vec_out is None:\n",
        "        print(f\"    ERROR: Output missing for comparison (Ref '{key_ref}': {'OK' if hf_out is not None else 'MISSING'}, Vec '{key_vec}': {'OK' if vec_out is not None else 'MISSING'})\")\n",
        "        difference_found_test_ffn = True; return True\n",
        "\n",
        "    # Direct comparison as we are only processing the prompt (step 0)\n",
        "    hf_out_sliced = hf_out\n",
        "    vec_out_sliced = vec_out\n",
        "\n",
        "    print(f\"    HF Shape (fp32): {hf_out_sliced.shape}, dtype: {hf_out_sliced.dtype}\"); print(f\"    Veector Shape (target): {vec_out_sliced.shape}, dtype: {vec_out_sliced.dtype}\")\n",
        "    if hf_out_sliced.shape != vec_out_sliced.shape: print(f\"    ERROR: Shape mismatch!\"); difference_found_test_ffn = True; return True\n",
        "    try:\n",
        "        hf_out_f32 = hf_out_sliced.astype(np.float32); vec_out_f32 = vec_out_sliced.astype(np.float32)\n",
        "        are_close = np.allclose(hf_out_f32, vec_out_f32, atol=ATOL, rtol=RTOL)\n",
        "        print(f\"    Result: {'CLOSE' if are_close else '!!! DIFFERENT !!!'}\")\n",
        "        if not are_close:\n",
        "            diff = np.abs(hf_out_f32 - vec_out_f32); max_diff = np.max(diff); mean_diff = np.mean(diff); max_diff_idx = np.unravel_index(np.argmax(diff), diff.shape)\n",
        "            print(f\"      Max Abs Difference:  {max_diff:.6f} at index {max_diff_idx}\"); print(f\"      Mean Abs Difference: {mean_diff:.6f}\")\n",
        "            print(f\"      HF Sample ('{key_ref}') @ max diff:      {hf_out_f32[max_diff_idx]:.6f}\"); print(f\"      Veector Sample ('{key_vec}') @ max diff: {vec_out_f32[max_diff_idx]:.6f}\")\n",
        "            difference_found_test_ffn = True; return True\n",
        "    except Exception as cmp_e: print(f\"    ERROR during comparison for {key_vec}/{key_ref}: {cmp_e}\"); difference_found_test_ffn = True; return True\n",
        "    return False\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "try:\n",
        "    log_memory_usage_ffn(\"Start of FFN Test Cell\")\n",
        "    # --- 1. Load Prerequisites ---\n",
        "    print(f\"\\nLoading prerequisites...\")\n",
        "    # Load Processor Map\n",
        "    if not proc_map_filepath.is_file(): raise FileNotFoundError(f\"Processor map file not found: {proc_map_filepath}\")\n",
        "    print(f\"  Loading Processor map from: {proc_map_filepath}\")\n",
        "    with open(proc_map_filepath, 'rb') as f: processor_map_test_ffn = pickle.load(f)\n",
        "    print(f\"  Processor map loaded ({len(processor_map_test_ffn)} entries).\")\n",
        "\n",
        "    # Load Reference Outputs\n",
        "    if not ref_output_filepath.is_file(): raise FileNotFoundError(f\"Reference output file not found: {ref_output_filepath}\")\n",
        "    print(f\"  Loading Reference HF outputs from: {ref_output_filepath}\")\n",
        "    with open(ref_output_filepath, 'rb') as f: hf_outputs_test_ffn = pickle.load(f)\n",
        "    if not isinstance(hf_outputs_test_ffn, dict): raise TypeError(\"Reference data is not a dict.\")\n",
        "    print(f\"  Reference outputs loaded ({len(hf_outputs_test_ffn)} entries).\")\n",
        "    # Check if the required keys exist\n",
        "    required_ref_keys = [\n",
        "        \"L0_attn_block_output\", \"L0_layer_output\",\n",
        "        \"L0_mlp_gate_out\", \"L0_mlp_up_out\",\n",
        "        \"L0_mlp_act_mul_up\", \"L0_mlp_down_out\"\n",
        "    ]\n",
        "    for key in required_ref_keys:\n",
        "        if key not in hf_outputs_test_ffn:\n",
        "            raise KeyError(f\"Reference file missing required key '{key}'. Re-run Cell 4 (v1.3+).\")\n",
        "\n",
        "    # --- 2. Initialize Veector ---\n",
        "    print(f\"\\nInitializing Veector instance...\")\n",
        "    vec_test_ffn = Veector(db_dir=DB_PATH)\n",
        "    print(f\"Veector initialized. DB Index entries: {len(vec_test_ffn.db.index)}\")\n",
        "    if len(vec_test_ffn.db.index) == 0: raise RuntimeError(\"Loaded main index is empty!\")\n",
        "\n",
        "    # --- 3. Check FFN Processor ID ---\n",
        "    print(\"\\nChecking processor ID...\")\n",
        "    if \"ffn_0\" not in processor_map_test_ffn: raise ValueError(\"Processor ID 'ffn_0' not found in map.\")\n",
        "    ffn_0_processor_id_test = processor_map_test_ffn[\"ffn_0\"]\n",
        "    print(\"Processor ID 'ffn_0' found.\")\n",
        "\n",
        "    # --- 4. Get Input Data (Reference Attention Output) ---\n",
        "    print(\"\\nLoading reference 'L0_attn_block_output' as input for FFN...\")\n",
        "    ffn_input_data_np = hf_outputs_test_ffn.get(\"L0_attn_block_output\")\n",
        "    if ffn_input_data_np is None: raise ValueError(\"Failed to get 'L0_attn_block_output' from reference data.\")\n",
        "    # Ensure input is in the target precision for the Veector processor\n",
        "    ffn_input_data_np = ffn_input_data_np.astype(np.float16 if NEST_LEVEL == 1 else np.float32)\n",
        "    log_tensor_stats_ffn(\"Input to FFN (Ref Attn Output)\", ffn_input_data_np)\n",
        "\n",
        "    # --- 5. Execute Veector FFN Processor L0 ---\n",
        "    print(f\"\\n--- Running Veector FFN Processor L0 ---\")\n",
        "    ffn_output_test = None\n",
        "    ffn_step_context = None # To store context with intermediates\n",
        "    ffn_context_test = {\n",
        "        \"input_data\": ffn_input_data_np,\n",
        "        \"residual_input\": ffn_input_data_np, # Name used by OP_ADD\n",
        "        \"required_nest\": NEST_LEVEL,\n",
        "        \"target_knowledge_group\": KNOWLEDGE_GROUP_IDX\n",
        "    }\n",
        "    ffn_result_test = vec_test_ffn.compute(ffn_0_processor_id_test, context=ffn_context_test)\n",
        "\n",
        "    # Check execution status\n",
        "    if not (ffn_result_test and ffn_result_test.get(\"status\") == \"completed\"):\n",
        "        raise RuntimeError(f\"FFN L0 failed: {ffn_result_test.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "\n",
        "    # Get final output and step context\n",
        "    ffn_output_test = ffn_result_test.get(\"data\") # This is the final output of the layer (FFN + Residual)\n",
        "    ffn_step_context = ffn_result_test.get(\"step_context\") # Context contains stored intermediates\n",
        "\n",
        "    if ffn_output_test is None: raise RuntimeError(f\"FFN L0 returned None data.\")\n",
        "    if ffn_step_context is None: raise RuntimeError(f\"FFN L0 did not return step_context.\")\n",
        "    print(\"    FFN L0 Execution OK.\")\n",
        "    log_tensor_stats_ffn(\"Final Output of FFN L0 (Veector)\", ffn_output_test)\n",
        "\n",
        "    # --- 6. Compare Intermediate MLP Outputs ---\n",
        "    print(\"\\n--- Comparing MLP Intermediate Outputs ---\")\n",
        "    # Define keys for comparison\n",
        "    # Key in step_ctx (Veector) : Key in ref_outputs (HF)\n",
        "    comparison_keys = {\n",
        "        'L0_dbg_mlp_gate_out': 'L0_mlp_gate_out',\n",
        "        'L0_dbg_mlp_up_out': 'L0_mlp_up_out',\n",
        "        'L0_dbg_mlp_activated': 'L0_mlp_act_mul_up',\n",
        "        'mlp_pre_residual_out': 'L0_mlp_down_out', # Compare output before residual add\n",
        "    }\n",
        "    for vec_key, ref_key in comparison_keys.items():\n",
        "        vec_intermediate_val = ffn_step_context.get(vec_key)\n",
        "        # Use the renamed comparison function\n",
        "        compare_values(key_ref=ref_key, key_vec=vec_key, vec_out=vec_intermediate_val, ref_outputs=hf_outputs_test_ffn)\n",
        "        if difference_found_test_ffn:\n",
        "             print(f\"    Difference found at intermediate step: {vec_key} vs {ref_key}\")\n",
        "             # break # Optionally stop after first intermediate difference\n",
        "\n",
        "    # --- 7. Compare Final Layer Output ---\n",
        "    print(\"\\n--- Comparing Final L0 Layer Output (Veector) with Reference L0 Layer Output ---\")\n",
        "    # Use the renamed comparison function\n",
        "    compare_values(key_ref=\"L0_layer_output\", key_vec=\"Final FFN L0 Output\", vec_out=ffn_output_test, ref_outputs=hf_outputs_test_ffn)\n",
        "\n",
        "# --- Error Handling ---\n",
        "except Exception as cell5_ffn_e:\n",
        "    print(f\"\\n---!!! ERROR during FFN Test execution: {cell5_ffn_e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "    error_occurred_test_ffn = True\n",
        "finally:\n",
        "    # --- 8. Cleanup ---\n",
        "    print(\"\\nCleaning up resources for FFN Test Cell...\")\n",
        "    if vec_test_ffn and hasattr(vec_test_ffn, 'db') and vec_test_ffn.db:\n",
        "        try: vec_test_ffn.db.close(); print(\"Veector DB connection closed.\")\n",
        "        except Exception as db_close_e: print(f\"Error closing DB connection: {db_close_e}\")\n",
        "    if 'vec_test_ffn' in locals(): del vec_test_ffn\n",
        "    if 'processor_map_test_ffn' in locals(): del processor_map_test_ffn\n",
        "    if 'hf_outputs_test_ffn' in locals(): del hf_outputs_test_ffn\n",
        "\n",
        "    gc.collect()\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"CUDA cache cleared.\")\n",
        "    print(\"Garbage collection run.\")\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    if not error_occurred_test_ffn:\n",
        "        if difference_found_test_ffn: print(\"\\n--- RESULT: Difference found in L0 FFN Block (check intermediate comparisons). ---\")\n",
        "        else: print(\"\\n--- RESULT: L0 FFN Block output and intermediates are CLOSE to reference! ---\")\n",
        "    else: print(\"\\n--- RESULT: Test not completed due to runtime errors. ---\")\n",
        "\n",
        "    end_cell_ffn_test_time = time.time()\n",
        "    print(f\"\\n--- Cell 5.1 (FFN Intermediate Test) Finished in {end_cell_ffn_test_time - start_cell_ffn_test_time:.2f} seconds ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Archive & Download ===\n",
        "# Creates a zip archive of the specified data directory and initiates download.\n",
        "# Relies on Cell 1 variables (DB_PATH, MODEL_NAME).\n",
        "\n",
        "import shutil\n",
        "from google.colab import files # For downloading in Colab\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration (Load from Cell 1 variables) ---\n",
        "if 'DB_PATH' not in globals(): raise NameError(\"DB_PATH not defined. Run Cell 1.\")\n",
        "if 'MODEL_NAME' not in globals(): raise NameError(\"MODEL_NAME not defined. Run Cell 1.\")\n",
        "\n",
        "# Define the directory to archive (usually the parent of DB_PATH)\n",
        "# e.g., if DB_PATH is /content/data/db, archive /content/data\n",
        "DIR_TO_ARCHIVE = DB_PATH.parent\n",
        "ARCHIVE_BASENAME = f\"veector_data_{MODEL_NAME}\" # Base name for the archive file\n",
        "ARCHIVE_FORMAT = \"zip\"\n",
        "ARCHIVE_FILENAME = f\"{ARCHIVE_BASENAME}.{ARCHIVE_FORMAT}\"\n",
        "\n",
        "print(f\"--- Running Cell 6: Archiving ---\")\n",
        "print(f\"    Source directory: {DIR_TO_ARCHIVE}\")\n",
        "print(f\"    Archive basename: {ARCHIVE_BASENAME}\")\n",
        "print(f\"    Format: {ARCHIVE_FORMAT}\")\n",
        "\n",
        "try:\n",
        "    # Create the archive\n",
        "    archive_path = shutil.make_archive(\n",
        "        base_name=ARCHIVE_BASENAME,\n",
        "        format=ARCHIVE_FORMAT,\n",
        "        root_dir=DIR_TO_ARCHIVE.parent, # Start archiving from the parent of DIR_TO_ARCHIVE\n",
        "        base_dir=DIR_TO_ARCHIVE.name   # The directory name within root_dir to archive\n",
        "    )\n",
        "    print(f\"Archive created successfully: {archive_path}\")\n",
        "\n",
        "    # Initiate download in Colab\n",
        "    print(f\"\\nInitiating download for {ARCHIVE_FILENAME}...\")\n",
        "    files.download(archive_path)\n",
        "    print(\"Download initiated. Check your browser.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR during archiving or download: {e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n--- Cell 6 Finished ---\")\n"
      ],
      "metadata": {
        "id": "uZ3-yYt13Get"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 7: Upload to Google Drive ===\n",
        "# Copies the created archive file to a specified Google Drive path.\n",
        "# Relies on Cell 1 (for Drive mount) and Cell 6 (for archive creation).\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration (Load from Cell 1/6 variables) ---\n",
        "# ARCHIVE_FILENAME should be defined based on Cell 6 execution\n",
        "# Or define it manually if running independently after archive exists\n",
        "if 'ARCHIVE_FILENAME' not in globals():\n",
        "    # Try to reconstruct from Cell 1 variables if Cell 6 wasn't run in this session\n",
        "    if 'MODEL_NAME' in globals():\n",
        "        ARCHIVE_BASENAME = f\"veector_data_{MODEL_NAME}\"\n",
        "        ARCHIVE_FORMAT = \"zip\"\n",
        "        ARCHIVE_FILENAME = f\"{ARCHIVE_BASENAME}.{ARCHIVE_FORMAT}\"\n",
        "        print(f\"WARN: ARCHIVE_FILENAME not found, reconstructed as {ARCHIVE_FILENAME}\")\n",
        "    else:\n",
        "        raise NameError(\"ARCHIVE_FILENAME or MODEL_NAME not defined. Run previous cells.\")\n",
        "\n",
        "# Define your target path on Google Drive\n",
        "# Ensure the 'models' directory (or your desired path) exists in 'My Drive'\n",
        "GDRIVE_DESTINATION_PATH = \"/content/drive/My Drive/veector_models/\" # Example path\n",
        "\n",
        "print(f\"--- Running Cell 7: Upload to Google Drive ---\")\n",
        "print(f\"    Archive file: {ARCHIVE_FILENAME}\")\n",
        "print(f\"    Destination: {GDRIVE_DESTINATION_PATH}\")\n",
        "\n",
        "# Ensure archive file exists locally\n",
        "local_archive_path = Path(f\"./{ARCHIVE_FILENAME}\")\n",
        "if not local_archive_path.is_file():\n",
        "    raise FileNotFoundError(f\"Archive file {local_archive_path} not found. Run Cell 6 first.\")\n",
        "\n",
        "try:\n",
        "    # Ensure Google Drive is mounted (might need re-authentication)\n",
        "    drive.mount('/content/drive', force_remount=True) # Force remount might be needed\n",
        "\n",
        "    # Create destination directory on Drive if it doesn't exist\n",
        "    dest_path_obj = Path(GDRIVE_DESTINATION_PATH)\n",
        "    dest_path_obj.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Ensured Google Drive directory exists: {dest_path_obj}\")\n",
        "\n",
        "    # Copy the file\n",
        "    print(f\"Copying {local_archive_path} to {dest_path_obj}...\")\n",
        "    shutil.copy(str(local_archive_path), str(dest_path_obj))\n",
        "    print(f\"🟢 [LOG] ✅ Archive uploaded successfully to Google Drive: {dest_path_obj / ARCHIVE_FILENAME}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR during Google Drive upload: {e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n--- Cell 7 Finished ---\")\n"
      ],
      "metadata": {
        "id": "IGaZ7kmL3KS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}