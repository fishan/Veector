{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjQ2xmLY5KMh3ZG14xttd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishan/Veector/blob/base/Veector_split_DeepSeek_R1_Distill_Qwen_1_5b_int8_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 0: Install Dependencies ===\n",
        "!pip install numpy psutil torch transformers accelerate bitsandbytes ipfshttpclient qiskit qiskit-aer requests huggingface_hub -q\n",
        "print(\"Dependencies installed/checked.\")"
      ],
      "metadata": {
        "id": "qXcbuXfgiZ2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Imports (Corrected and Simplified - FINAL) ===\n",
        "\n",
        "# --- Standard Imports ---\n",
        "import numpy as np\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "import random\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "from google.colab import drive, files, userdata # Keep Colab imports\n",
        "from huggingface_hub import login             # Keep HF import\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Keep Transformers imports\n",
        "\n",
        "print(\"Standard/External imports loaded.\")\n",
        "\n",
        "# --- Optional Imports ---\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"Warning: PyTorch not found. GPU features may be limited.\")\n",
        "\n",
        "try:\n",
        "    import ipfshttpclient\n",
        "    IPFS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IPFS_AVAILABLE = False\n",
        "    # print(\"Warning: ipfshttpclient not found. IPFS features disabled.\")\n",
        "\n",
        "try:\n",
        "    from qiskit import QuantumCircuit\n",
        "    from qiskit.providers.aer import Aer\n",
        "    from qiskit import execute\n",
        "    QISKIT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    QISKIT_AVAILABLE = False\n",
        "    # print(\"Warning: Qiskit not found. Quantum operations disabled.\")\n",
        "\n",
        "print(\"Optional imports checked.\")\n",
        "\n",
        "# --- Veector Project Imports (Single Correct Block) ---\n",
        "# Ensure core.py, tensors.py (v0.5.1+), veectordb.py (v0.7.1+),\n",
        "# operations.py, memory.py are uploaded and accessible.\n",
        "PROJECT_IMPORTS_OK = False\n",
        "try:\n",
        "    # Import core classes/functions needed by THIS script (converter/inference)\n",
        "    from core import Veector\n",
        "    from veectordb import VeectorDB # Needed if we re-initialize DB here? Usually not.\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, # Основные функции\n",
        "        validate_tensor, validate_tensor_tuple, get_tensor_hash, # Валидаторы и хеш\n",
        "        # Импортируем ТОЛЬКО ТЕ константы, которые реально используются\n",
        "        # и существуют в tensors.py v0.7.6\n",
        "        TAG_TYPE_PROCESSOR, TAG_TYPE_KNOWLEDGE, TAG_TYPE_CONVERTER, TAG_TYPE_STATE,\n",
        "        TAG_MODEL_QWEN2, TAG_MODEL_LLAMA3, TAG_MODEL_DEEPSEEK,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16,\n",
        "        TAG_PREC_INT8, TAG_PREC_INT4,\n",
        "        TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_EMBEDDING, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_ATTN_QKV,\n",
        "        TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LAYERNORM,\n",
        "        TAG_COMP_LM_HEAD,\n",
        "        TAG_FUNC_LINEAR, TAG_FUNC_ATTENTION, TAG_FUNC_FFN,\n",
        "        TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_CAST_DTYPE, TAG_FUNC_RESHAPE,\n",
        "        TAG_SEMANTIC_HIDDEN_STATE, TAG_SEMANTIC_LOGITS, TAG_SEMANTIC_TOKEN_IDS,\n",
        "        TAG_SEMANTIC_KV_CACHE,\n",
        "        tag_layer, # Функция для тега слоя\n",
        "        GROUP_IDX_QWEN_KNOWLEDGE, GROUP_IDX_QWEN_PROCESSOR, # ID Групп\n",
        "        GROUP_IDX_DEEPSEEK_KNOWLEDGE\n",
        "    )\n",
        "    # Only import from operations/memory if DIRECTLY used in THIS script, otherwise core.py handles it\n",
        "    # from operations import * # Generally not needed here\n",
        "    # from memory import Memory # Generally not needed here\n",
        "\n",
        "    print(\"Veector project components imported successfully for this script.\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"---!!! FATAL ERROR (ImportError) !!! ---\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "    print(f\"Could not import required name from core.py or tensors.py.\")\n",
        "    print(f\"Ensure files are UP-TO-DATE (tensors v0.5.1+, core v0.5.2+), CORRECT, and ACCESSIBLE.\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    # Optionally define dummies if needed for notebook structure\n",
        "except Exception as other_e:\n",
        "    print(f\"---!!! FATAL ERROR (Other Exception during Import) !!! ---\")\n",
        "    print(f\"Specific error: {other_e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"Check imported files for syntax errors.\")\n",
        "    print(f\"----------------------------------------------------------\")\n",
        "\n",
        "# Removed the redundant import check block ('Checking imports...')"
      ],
      "metadata": {
        "id": "BZOpQTOyid6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Очистка директории для чистоты эксперимента\n",
        "!rm -rf data/\n",
        "output_dir = \"data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "taSpnXjzisY2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Аутентификация с Hugging Face\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Добавь HF_TOKEN в секреты Colab!\")\n",
        "login(hf_token)\n",
        "print(\"Аутентификация прошла успешно\")\n",
        "\n",
        "# Подключение Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive подключён\")\n",
        "\n",
        "model_NAME = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# Определяем ОДИН основной путь к БД (например, в data/db/)\n",
        "DB_PATH = Path(\"./data/db/\")\n",
        "DB_PATH.mkdir(parents=True, exist_ok=True) # Создаем data/db, если ее нет\n",
        "print(f\"Using Main Veector DB Path: {DB_PATH.resolve()}\")\n",
        "\n",
        "# Set data type (bfloat16 might not be fully supported everywhere, float16 is safer)\n",
        "TORCH_DTYPE = torch.float16 # Use float16 for wider compatibility\n",
        "\n",
        "print(f\"Model to convert: {model_NAME}\")\n",
        "print(f\"Target Veector DB: {DB_PATH}\")\n",
        "print(f\"Target dtype: {TORCH_DTYPE}\")"
      ],
      "metadata": {
        "id": "3FiGWMogi8Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Tag Ontology and Mappings Definition (Sync with tensors.py v0.7.0) ===\n",
        "\n",
        "import torch # Ensure torch is imported for dtype checking if needed later\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union # Import typing for hints\n",
        "\n",
        "# --- Version (for tracking changes in this cell) ---\n",
        "CONVERTER_CELL2_VERSION = \"Synced with tensors.py v0.7.0\"\n",
        "print(f\"--- Running Converter Cell 2 v{CONVERTER_CELL2_VERSION} ---\")\n",
        "\n",
        "# --- Type Hint for Metadata Tuple (from tensors.py) ---\n",
        "# Needed if any functions within Colab cells might use this type hint\n",
        "MetadataTuple = Tuple[\n",
        "    List[Union[float, int]],         # [0] data_description\n",
        "    List[int],                       # [1] coord\n",
        "    List[int],                       # [2] shape\n",
        "    List[int],                       # [3] tags\n",
        "    Optional[Dict],                  # [4] ops_sequences\n",
        "    Optional[Dict],                  # [5] interface\n",
        "    Optional[List],                  # [6] filters\n",
        "    Optional[List],                  # [7] exit_gates\n",
        "    List[int],                       # [8] lifecycle\n",
        "    Optional[List[str]]              # [9] parents\n",
        "]\n",
        "\n",
        "# --- Simplified Tag Ontology (Flat Integers with Ranges - from tensors.py v0.7.0) ---\n",
        "# 1-9: Tensor Type\n",
        "TAG_TYPE_PROCESSOR = 1\n",
        "TAG_TYPE_KNOWLEDGE = 2\n",
        "TAG_TYPE_CONVERTER = 3\n",
        "TAG_TYPE_STATE = 4\n",
        "# 10-19: Model Family\n",
        "TAG_MODEL_QWEN2 = 10\n",
        "TAG_MODEL_LLAMA3 = 11\n",
        "TAG_MODEL_DEEPSEEK = 12\n",
        "# 20-29: Precision\n",
        "TAG_PREC_FLOAT32 = 20\n",
        "TAG_PREC_FLOAT16 = 21\n",
        "TAG_PREC_BFLOAT16 = 22\n",
        "TAG_PREC_INT8 = 23\n",
        "TAG_PREC_INT4 = 24\n",
        "# 30-49: Component Type\n",
        "TAG_COMP_WEIGHTS = 30\n",
        "TAG_COMP_BIAS = 31\n",
        "TAG_COMP_EMBEDDING = 32\n",
        "TAG_COMP_ATTN_Q = 33\n",
        "TAG_COMP_ATTN_K = 34\n",
        "TAG_COMP_ATTN_V = 35\n",
        "TAG_COMP_ATTN_O = 36\n",
        "TAG_COMP_ATTN_QKV = 37\n",
        "TAG_COMP_FFN_GATE = 38\n",
        "TAG_COMP_FFN_UP = 39\n",
        "TAG_COMP_FFN_DOWN = 40\n",
        "TAG_COMP_LAYERNORM = 41\n",
        "TAG_COMP_LM_HEAD = 42\n",
        "# 50-59: Function\n",
        "TAG_FUNC_LINEAR = 50\n",
        "TAG_FUNC_ATTENTION = 51\n",
        "TAG_FUNC_FFN = 52\n",
        "TAG_FUNC_EMBED_LOOKUP = 53\n",
        "TAG_FUNC_CAST_DTYPE = 54\n",
        "TAG_FUNC_RESHAPE = 55\n",
        "# 60-69: Data Semantic Type\n",
        "TAG_SEMANTIC_HIDDEN_STATE = 60\n",
        "TAG_SEMANTIC_LOGITS = 61\n",
        "TAG_SEMANTIC_TOKEN_IDS = 62\n",
        "TAG_SEMANTIC_KV_CACHE = 63\n",
        "# 100-999: Layer Index\n",
        "LAYER_IDX_TAG_OFFSET = 100\n",
        "\n",
        "def tag_layer(idx: int) -> int:\n",
        "    \"\"\"Generates a layer tag using an offset.\"\"\"\n",
        "    if not isinstance(idx, int): raise TypeError(f\"Layer index must be an integer, got {type(idx)}\")\n",
        "    if idx < 0: raise ValueError(f\"Invalid layer index for tagging: {idx}. Must be non-negative.\")\n",
        "    return LAYER_IDX_TAG_OFFSET + idx\n",
        "# 1000+: User Defined Tags\n",
        "USER_TAG_OFFSET = 1000\n",
        "# --- End of Tags ---\n",
        "print(\"Simplified tag ontology (flat integers) defined.\")\n",
        "\n",
        "# --- Group ID Constants (from tensors.py v0.7.0) ---\n",
        "GROUP_IDX_QWEN_KNOWLEDGE = 100\n",
        "GROUP_IDX_QWEN_PROCESSOR = 500\n",
        "GROUP_IDX_LLAMA_KNOWLEDGE = 101\n",
        "GROUP_IDX_LLAMA_PROCESSOR = 501\n",
        "GROUP_IDX_DEEPSEEK_KNOWLEDGE = 102 # Added constant\n",
        "# GROUP_IDX_DEEPSEEK_PROCESSOR = 502 # Optional\n",
        "GROUP_IDX_GENERIC_PROCESSOR = 50\n",
        "print(f\"Group Indices defined: QwenK={GROUP_IDX_QWEN_KNOWLEDGE}, QwenP={GROUP_IDX_QWEN_PROCESSOR}, DeepSeekK={GROUP_IDX_DEEPSEEK_KNOWLEDGE}\")\n",
        "\n",
        "\n",
        "# --- Mappings (from tensors.py v0.7.0) ---\n",
        "# 1. DATA_TYPE_MAPPING\n",
        "DATA_TYPE_MAPPING = {\n",
        "    \"knowledge\": 1,\n",
        "    \"processor\": 2,\n",
        "    \"converter\": 3,\n",
        "    \"state\": 4,\n",
        "}\n",
        "REVERSE_DATA_TYPE_MAPPING = {\n",
        "    1: \"knowledge\",\n",
        "    2: \"processor\",\n",
        "    3: \"converter\",\n",
        "    4: \"state\",\n",
        "}\n",
        "print(f\"DATA_TYPE_MAPPING defined: {DATA_TYPE_MAPPING}\")\n",
        "\n",
        "# 2. DTYPE_MAPPING\n",
        "DTYPE_MAPPING = {\n",
        "    # Standard Names\n",
        "    'float32': 1, 'float16': 2, 'bfloat16': 3, 'int8': 4, 'int4': 5,\n",
        "    'int32': 6, 'int64': 7, 'bool': 8, 'complex64': 9, 'complex128': 10,\n",
        "    # Numpy Types\n",
        "    np.float32: 1, np.float16: 2, np.int8: 4, np.int32: 6, np.int64: 7,\n",
        "    np.bool_: 8, np.complex64: 9, np.complex128: 10,\n",
        "    # PyTorch Types (as strings and potentially objects if torch loaded)\n",
        "    'torch.float32': 1, 'torch.float16': 2, 'torch.bfloat16': 3, 'torch.int8': 4,\n",
        "    'torch.int32': 6, 'torch.int64': 7, 'torch.bool': 8,\n",
        "    'torch.complex64': 9, 'torch.complex128': 10,\n",
        "}\n",
        "# Add torch objects if torch is available\n",
        "if 'torch' in globals():\n",
        "    DTYPE_MAPPING[torch.float32] = 1\n",
        "    DTYPE_MAPPING[torch.float16] = 2\n",
        "    DTYPE_MAPPING[torch.bfloat16] = 3\n",
        "    DTYPE_MAPPING[torch.int8] = 4\n",
        "    DTYPE_MAPPING[torch.int32] = 6\n",
        "    DTYPE_MAPPING[torch.int64] = 7\n",
        "    DTYPE_MAPPING[torch.bool] = 8\n",
        "    DTYPE_MAPPING[torch.complex64] = 9\n",
        "    DTYPE_MAPPING[torch.complex128] = 10\n",
        "\n",
        "REVERSE_DTYPE_MAPPING = {\n",
        "    1: 'float32', 2: 'float16', 3: 'bfloat16', 4: 'int8', 5: 'int4',\n",
        "    6: 'int32', 7: 'int64', 8: 'bool', 9: 'complex64', 10: 'complex128',\n",
        "}\n",
        "print(f\"DTYPE_MAPPING defined.\")\n",
        "\n",
        "# 3. STATUS_MAPPING\n",
        "STATUS_MAPPING = {\n",
        "    \"active\": 1,\n",
        "    \"archived\": 0\n",
        "}\n",
        "REVERSE_STATUS_MAPPING = {\n",
        "    1: \"active\",\n",
        "    0: \"archived\"\n",
        "}\n",
        "print(f\"STATUS_MAPPING defined: {STATUS_MAPPING}\")\n",
        "\n",
        "# --- Metadata Encoding Configuration (from tensors.py v0.7.0) ---\n",
        "METADATA_STRUCTURE_VERSION = 1.1\n",
        "print(f\"Metadata Structure Version: {METADATA_STRUCTURE_VERSION}\")\n",
        "\n",
        "print(\"Tag ontology, Group IDs, Mappings, and Config defined for Cell 2.\")"
      ],
      "metadata": {
        "id": "RW8d1Ziei9t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Initialize Veector (SINGLE Instance) ===\n",
        "from core import Veector # Импортируем класс Veector из core.py\n",
        "DB_PATH = Path(\"./data/db/\")\n",
        "try:\n",
        "    # Используем этот путь при инициализации\n",
        "    vec = Veector(db_dir=DB_PATH, ipfs_enabled=False)\n",
        "    print(f\"Veector core initialized using DB at: {DB_PATH.resolve()}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Veector initialization failed: {e}\")\n",
        "    raise RuntimeError(\"Veector Core failed to initialize\") from e"
      ],
      "metadata": {
        "id": "xkU0jKwTbBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Load Hugging Face Model ===\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(f\"deepseek-ai/{model_NAME}\", torch_dtype=TORCH_DTYPE, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"deepseek-ai/{model_NAME}\", trust_remote_code=True)\n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"Successfully loaded HF model: {model_NAME}\")\n",
        "    print(f\"Model config: {model.config}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Failed to load HF model '{model_NAME}': {e}\")\n",
        "    # Stop execution\n",
        "    raise RuntimeError(f\"Hugging Face model loading failed\") from e\n",
        "\n",
        "# Clean up GPU memory if possible after loading\n",
        "if TORCH_AVAILABLE and torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Model loaded and memory potentially cleaned.\")"
      ],
      "metadata": {
        "id": "hLSY0WSHjSml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python core.py"
      ],
      "metadata": {
        "id": "KIEZWbxmacx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Скрипт для прохода HF модели в float32 и сохранения ВСЕХ промежуточных выходов ===\n",
        "# Version: 2.1 (Fixed special token ID retrieval)\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Необходимые библиотеки ---\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e: print(f\"FATAL ERROR: Missing essential libraries: {e}\"); exit()\n",
        "\n",
        "# --- Конфигурация ---\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "TOKENIZER_SOURCE = MODEL_SOURCE\n",
        "DB_PATH_FOR_OUTPUT = Path(\"./data/db\")\n",
        "PROMPT = \"Hello, how are you?\"\n",
        "OUTPUT_FILENAME = f\"{MODEL_SOURCE.split('/')[-1]}_hf_reference_outputs_fp32.pkl\"\n",
        "OUTPUT_FILEPATH = DB_PATH_FOR_OUTPUT / OUTPUT_FILENAME\n",
        "\n",
        "# --- Создаем директорию ---\n",
        "try: DB_PATH_FOR_OUTPUT.mkdir(parents=True, exist_ok=True); print(f\"Output directory set to: {DB_PATH_FOR_OUTPUT.resolve()}\")\n",
        "except Exception as e: print(f\"Error creating output directory {DB_PATH_FOR_OUTPUT}: {e}\")\n",
        "\n",
        "# --- Загрузка Токенизатора ---\n",
        "print(\"\\\\n--- Loading Tokenizer ---\")\n",
        "tokenizer = None\n",
        "bos_token_id = None; eos_token_id = None; user_token_id = None; assistant_token_id = None\n",
        "try:\n",
        "    print(f\"Loading Tokenizer from: {TOKENIZER_SOURCE}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True, use_fast=False)\n",
        "    print(f\"Tokenizer class: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "    # --- ИЗМЕНЕНО: Получение ID спец токенов через convert_tokens_to_ids ---\n",
        "    bos_token_id = tokenizer.bos_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    user_token = \"<|User|>\"\n",
        "    assistant_token = \"<|Assistant|>\"\n",
        "    user_token_id = tokenizer.convert_tokens_to_ids(user_token)\n",
        "    assistant_token_id = tokenizer.convert_tokens_to_ids(assistant_token)\n",
        "\n",
        "    if isinstance(user_token_id, str) or user_token_id == tokenizer.unk_token_id: raise ValueError(f\"Could not find ID for token '{user_token}'\")\n",
        "    if isinstance(assistant_token_id, str) or assistant_token_id == tokenizer.unk_token_id: raise ValueError(f\"Could not find ID for token '{assistant_token}'\")\n",
        "    # --- Конец изменения ---\n",
        "\n",
        "    if tokenizer.pad_token_id is None: tokenizer.pad_token_id = eos_token_id if eos_token_id is not None else tokenizer.vocab_size\n",
        "    print(f\"Tokens: BOS={bos_token_id}, EOS={eos_token_id}, PAD={tokenizer.pad_token_id}, User={user_token_id}, Assistant={assistant_token_id}\") # Печатаем ID\n",
        "\n",
        "except Exception as e: print(f\"FATAL ERROR loading tokenizer or getting special tokens: {e}\"); exit()\n",
        "\n",
        "# --- Подготовка входных данных ---\n",
        "print(\"\\\\n--- Preparing Input IDs ---\")\n",
        "input_ids_torch = None; input_seq_len = 0\n",
        "try:\n",
        "    print(\"Manually constructing prompt tokens (GGUF-style)...\")\n",
        "    user_text_ids = tokenizer.encode(PROMPT, add_special_tokens=False)\n",
        "    input_ids_list = []\n",
        "    if bos_token_id is not None: input_ids_list.append(bos_token_id)\n",
        "    input_ids_list.append(user_token_id); input_ids_list.extend(user_text_ids); input_ids_list.append(assistant_token_id)\n",
        "    prompt_input_ids_np = np.array([input_ids_list], dtype=np.int64)\n",
        "    input_seq_len = prompt_input_ids_np.shape[1]\n",
        "    input_ids_torch = torch.tensor(prompt_input_ids_np)\n",
        "\n",
        "    print(f\"Input IDs shape: {input_ids_torch.shape}\")\n",
        "    print(f\"Input IDs list: {input_ids_list}\") # Печатаем список ID\n",
        "    print(f\"Decoded Input: '{tokenizer.decode(input_ids_list)}'\") # Декодируем список\n",
        "except Exception as e: print(f\"FATAL ERROR preparing input: {e}\"); exit()\n",
        "\n",
        "# --- Загрузка и Прогон Эталонной Модели в Float32 ---\n",
        "print(f\"\\\\n--- Loading and Running HF Model ({MODEL_SOURCE}) in float32 ---\")\n",
        "hf_outputs: Dict[str, np.ndarray] = {}; hook_handles: List[Any] = []; model_fp32 = None\n",
        "\n",
        "def get_hook(name: str): # Функция-хук без изменений\n",
        "    def hook_fn(module: nn.Module, input_args: Tuple[Any, ...], output: Any):\n",
        "        actual_output: Optional[torch.Tensor] = None\n",
        "        if isinstance(output, torch.Tensor): actual_output = output\n",
        "        elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], torch.Tensor): actual_output = output[0]\n",
        "        elif isinstance(output, dict) and 'last_hidden_state' in output and isinstance(output['last_hidden_state'], torch.Tensor): actual_output = output['last_hidden_state']\n",
        "        if actual_output is not None: hf_outputs[name] = actual_output.detach().cpu().numpy().astype(np.float32)\n",
        "        else: print(f\"  [HOOK] WARN: Could not capture tensor output for {name}. Output type: {type(output)}\")\n",
        "    return hook_fn\n",
        "\n",
        "try:\n",
        "    print(f\"Loading HF Model {MODEL_SOURCE} with float32...\")\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    model_fp32.eval(); device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); model_fp32.to(device); input_ids_torch = input_ids_torch.to(device)\n",
        "    print(f\"HF Model loaded to device: {model_fp32.device}\")\n",
        "\n",
        "    print(\"Registering detailed hooks for float32 model...\")\n",
        "    model_config = model_fp32.config; num_layers = model_config.num_hidden_layers\n",
        "    hook_handles.append(model_fp32.model.embed_tokens.register_forward_hook(get_hook(\"embed_tokens\")))\n",
        "    for i in range(num_layers):\n",
        "        layer = model_fp32.model.layers[i]\n",
        "        hook_handles.append(layer.input_layernorm.register_forward_hook(get_hook(f\"L{i}_input_norm_out\")))\n",
        "        hook_handles.append(layer.self_attn.register_forward_hook(get_hook(f\"L{i}_attn_out\")))\n",
        "        hook_handles.append(layer.post_attention_layernorm.register_forward_hook(get_hook(f\"L{i}_post_attn_norm_out\")))\n",
        "        hook_handles.append(layer.mlp.register_forward_hook(get_hook(f\"L{i}_mlp_out\")))\n",
        "        hook_handles.append(layer.register_forward_hook(get_hook(f\"L{i}_layer_output\")))\n",
        "    hook_handles.append(model_fp32.model.norm.register_forward_hook(get_hook(\"final_norm\")))\n",
        "    hook_handles.append(model_fp32.lm_head.register_forward_hook(get_hook(\"lm_head\")))\n",
        "    print(f\"Registered {len(hook_handles)} hooks.\")\n",
        "\n",
        "    print(\"Running HF model forward pass (float32)...\")\n",
        "    with torch.no_grad(): hf_model_output = model_fp32(input_ids_torch, use_cache=False)\n",
        "    print(\"HF forward pass complete.\")\n",
        "\n",
        "except Exception as e: print(f\"FATAL ERROR during HF float32 execution: {e}\"); traceback.print_exc()\n",
        "finally:\n",
        "    for handle in hook_handles: handle.remove()\n",
        "    print(f\"Removed {len(hook_handles)} hooks.\")\n",
        "    if 'model_fp32' in locals() and model_fp32 is not None:\n",
        "        del model_fp32\n",
        "        if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "        import gc; gc.collect(); print(\"Cleaned up float32 model.\")\n",
        "\n",
        "# --- Сохранение результатов ---\n",
        "if hf_outputs:\n",
        "    print(f\"\\\\n--- Saving Captured Float32 Outputs to {OUTPUT_FILEPATH} ---\")\n",
        "    try:\n",
        "        with open(OUTPUT_FILEPATH, 'wb') as f: pickle.dump(hf_outputs, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Successfully saved {len(hf_outputs)} captured outputs.\")\n",
        "    except Exception as e: print(f\"FATAL ERROR saving outputs: {e}\"); traceback.print_exc()\n",
        "else: print(\"\\\\n--- No outputs captured from HF model, skipping save. ---\")\n",
        "\n",
        "print(f\"\\\\n--- Reference Output Script Finished ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UGW7je5kI67Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Convert Parameters to Knowledge Tensors (Transposed Weights) ===\n",
        "\n",
        "import gc\n",
        "import pickle\n",
        "import time\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING,\n",
        "        TAG_TYPE_KNOWLEDGE, TAG_MODEL_DEEPSEEK, TAG_COMP_WEIGHTS, TAG_COMP_BIAS,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_LM_HEAD, TAG_COMP_LAYERNORM, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, tag_layer, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    if TENSORS_VERSION < \"0.7.6\":\n",
        "        raise ImportError(f\"Requires tensors v0.7.6+, found v{TENSORS_VERSION}\")\n",
        "    from core import Veector, CORE_VERSION\n",
        "    if CORE_VERSION < \"0.6.12\":\n",
        "        raise ImportError(f\"Requires core v0.6.12+, found v{CORE_VERSION}\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Import failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Версия Ячейки ---\n",
        "CONVERTER_CELL5_VERSION = \"Hybrid v0.7.6 + Quant + Transpose v2\"\n",
        "# --- Конец Версии ---\n",
        "\n",
        "print(f\"--- Running Converter Cell 5 v{CONVERTER_CELL5_VERSION} ---\")\n",
        "start_cell5_time = time.time()\n",
        "\n",
        "# --- Проверка необходимых переменных ---\n",
        "if 'vec' not in locals() or vec is None:\n",
        "    raise NameError(\"'vec' object not defined.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "    raise NameError(\"DB_PATH not defined or invalid.\")\n",
        "if 'model' not in locals() or model is None:\n",
        "    raise NameError(\"HF 'model' not loaded.\")\n",
        "if 'model_NAME' not in locals() or not model_NAME:\n",
        "    raise NameError(\"model_NAME not defined.\")\n",
        "\n",
        "# --- Переинициализация DB (если необходимо) ---\n",
        "if not hasattr(vec, 'db') or vec.db is None:\n",
        "    try:\n",
        "        print(\"Attempting DB re-init for Cell 5...\")\n",
        "        # Импортируем только если нужно, чтобы избежать ненужных импортов вверху\n",
        "        from veectordb import VeectorDB\n",
        "        vec.db = VeectorDB(db_dir=DB_PATH)\n",
        "        print(\"DB connection re-established.\")\n",
        "    except Exception as db_reinit_e:\n",
        "        raise AttributeError(f\"DB re-init failed: {db_reinit_e}\")\n",
        "else:\n",
        "    print(\"'vec' object found and DB connection seems active.\")\n",
        "\n",
        "# --- Инициализация ---\n",
        "ORIGINAL_NAME_TO_ID_MAP: Dict[str, int] = {}\n",
        "ID_TO_ORIGINAL_NAME_MAP: Dict[int, str] = {}\n",
        "NEXT_NAME_ID: int = 0\n",
        "print(\"Initialized Name <-> ID mapping dictionaries.\")\n",
        "\n",
        "knowledge_map: Dict[str, str] = {} # Карта Имя -> ID Знания\n",
        "param_count: int = 0\n",
        "conversion_errors: int = 0\n",
        "\n",
        "# --- Вспомогательная функция для ID ---\n",
        "def get_or_create_name_id(name: Optional[str]) -> int:\n",
        "    \"\"\"Assigns and returns a unique ID for a parameter name.\"\"\"\n",
        "    global NEXT_NAME_ID, ORIGINAL_NAME_TO_ID_MAP, ID_TO_ORIGINAL_NAME_MAP\n",
        "    if not name:\n",
        "        return -1\n",
        "    if name in ORIGINAL_NAME_TO_ID_MAP:\n",
        "        return ORIGINAL_NAME_TO_ID_MAP[name]\n",
        "    current_id = NEXT_NAME_ID\n",
        "    ORIGINAL_NAME_TO_ID_MAP[name] = current_id\n",
        "    ID_TO_ORIGINAL_NAME_MAP[current_id] = name\n",
        "    NEXT_NAME_ID += 1\n",
        "    return current_id\n",
        "\n",
        "# --- Параметры конвертации ---\n",
        "default_precision_tag = TAG_PREC_FLOAT16\n",
        "default_torch_dtype = torch.float16\n",
        "if 'TORCH_DTYPE' in locals(): # Определено в Cell 1\n",
        "    default_torch_dtype = TORCH_DTYPE\n",
        "    if TORCH_DTYPE == torch.float16: default_precision_tag = TAG_PREC_FLOAT16\n",
        "    elif TORCH_DTYPE == torch.bfloat16: default_precision_tag = TAG_PREC_BFLOAT16\n",
        "    elif TORCH_DTYPE == torch.float32: default_precision_tag = TAG_PREC_FLOAT32\n",
        "    elif TORCH_DTYPE == torch.int8: default_precision_tag = TAG_PREC_INT8\n",
        "\n",
        "knowledge_group_idx = GROUP_IDX_QWEN_KNOWLEDGE # 100\n",
        "model_tag = TAG_MODEL_DEEPSEEK # 12\n",
        "\n",
        "print(f\"\\n--- Creating Knowledge Tensors (Group: {knowledge_group_idx}) ---\")\n",
        "print(f\"    Model Tag: {model_tag}\")\n",
        "print(f\"    Default Precision Tag: {default_precision_tag}\")\n",
        "print(f\"    Quantizing Embed/LMHead to INT8. Transposing Linear Weights.\")\n",
        "\n",
        "# --- Основной цикл конвертации ---\n",
        "total_params = sum(1 for _ in model.named_parameters())\n",
        "print(f\"Found {total_params} parameters to process.\")\n",
        "\n",
        "for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "    loop_start_time = time.time()\n",
        "    print(f\"\\nProcessing Param {idx+1}/{total_params}: {name}\")\n",
        "    print(f\"  Original Shape: {param.shape} | Dtype: {param.dtype}\")\n",
        "\n",
        "    # Инициализация переменных цикла\n",
        "    param_data_fp32: Optional[np.ndarray] = None\n",
        "    knowledge_data_to_pass: Optional[np.ndarray] = None\n",
        "    tags: List[int] = []\n",
        "    metadata_extra_to_pass: Optional[Dict] = None\n",
        "    dtype_to_pass: Any = None\n",
        "    final_tags: List[int] = []\n",
        "    knowledge_coord: Optional[TensorCoordinate] = None\n",
        "    name_id: int = -1\n",
        "    create_result: Optional[List] = None\n",
        "    knowledge_id: Optional[str] = None\n",
        "    requires_transpose: bool = False\n",
        "\n",
        "    try:\n",
        "        # Шаг 1-3: Получение данных, ID, Тегов, Координат\n",
        "        param_data_fp32 = param.data.cpu().to(torch.float32).numpy()\n",
        "        name_id = get_or_create_name_id(name)\n",
        "        tags = [TAG_TYPE_KNOWLEDGE, model_tag]\n",
        "        layer_idx = -1\n",
        "        group_idx = knowledge_group_idx\n",
        "        coord_x = 0\n",
        "        current_nest = 1 # По умолчанию Nest=1 для знаний\n",
        "        is_weight = name.endswith(\".weight\")\n",
        "        is_bias = name.endswith(\".bias\")\n",
        "\n",
        "        if is_weight: tags.append(TAG_COMP_WEIGHTS)\n",
        "        elif is_bias: tags.append(TAG_COMP_BIAS)\n",
        "\n",
        "        # Определение компонента, X координа и флага транспонирования\n",
        "        if \"model.embed_tokens.weight\" in name:\n",
        "             tags.append(TAG_COMP_EMBEDDING); coord_x = 0\n",
        "        elif \"lm_head.weight\" in name:\n",
        "             tags.append(TAG_COMP_LM_HEAD); coord_x = 1; requires_transpose = True\n",
        "        elif \"model.norm.weight\" in name:\n",
        "             layer_idx = model.config.num_hidden_layers; tags.append(TAG_COMP_LAYERNORM); coord_x = 0\n",
        "        elif \".layers.\" in name:\n",
        "            try:\n",
        "                layer_part = name.split('.layers.')[1]\n",
        "                layer_idx = int(layer_part.split('.')[0])\n",
        "                if layer_idx >= 0: tags.append(tag_layer(layer_idx))\n",
        "                else: raise ValueError(f\"Invalid L idx: {layer_idx}\")\n",
        "\n",
        "                component_tag_layer = None\n",
        "                if \"self_attn\" in name:\n",
        "                    if \"q_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 10; requires_transpose = True\n",
        "                    elif \"q_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 11\n",
        "                    elif \"k_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 20; requires_transpose = True\n",
        "                    elif \"k_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 21\n",
        "                    elif \"v_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 30; requires_transpose = True\n",
        "                    elif \"v_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 31\n",
        "                    elif \"o_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_O; coord_x = 40; requires_transpose = True\n",
        "                elif \"mlp\" in name:\n",
        "                    if \"gate_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_GATE; coord_x = 50; requires_transpose = True\n",
        "                    elif \"up_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_UP; coord_x = 60; requires_transpose = True\n",
        "                    elif \"down_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_DOWN; coord_x = 70; requires_transpose = True\n",
        "                elif \"input_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 1\n",
        "                elif \"post_attention_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 2\n",
        "\n",
        "                if component_tag_layer: tags.append(component_tag_layer)\n",
        "                elif not is_weight and not is_bias: print(f\"  WARN: Unrecognized comp in L{layer_idx}: {name}\"); coord_x = 99\n",
        "            except Exception as parse_e:\n",
        "                print(f\"  Error parsing layer for {name}: {parse_e}\"); conversion_errors += 1; continue\n",
        "        else:\n",
        "            print(f\"  WARN: Param unmatched: {name}\"); layer_idx = -1; coord_x = 999\n",
        "\n",
        "        knowledge_coord = TensorCoordinate(layer=layer_idx, group=group_idx, nest=current_nest, x=coord_x)\n",
        "\n",
        "        # Шаг 4: Квантование / Приведение типов / Транспонирование\n",
        "        quantization_scale = None\n",
        "        current_precision_tag = default_precision_tag\n",
        "        data_before_save = None\n",
        "\n",
        "        if name == \"model.embed_tokens.weight\" or name == \"lm_head.weight\":\n",
        "            if np.issubdtype(param_data_fp32.dtype, np.floating):\n",
        "                try:\n",
        "                    abs_max = np.max(np.abs(param_data_fp32)); scale = 1.0\n",
        "                    if abs_max >= 1e-9: scale = abs_max / 127.0\n",
        "                    scale = max(scale, 1e-9) # Prevent division by zero\n",
        "                    quantized_data = np.round(param_data_fp32 / scale).astype(np.int8)\n",
        "                    data_before_save = quantized_data; dtype_to_pass = np.int8\n",
        "                    quantization_scale = float(scale); current_precision_tag = TAG_PREC_INT8\n",
        "                    metadata_extra_to_pass = {\"quantization_scale\": quantization_scale}\n",
        "                    # Транспонируем только LM Head ПОСЛЕ квантования\n",
        "                    if name == \"lm_head.weight\": # requires_transpose is True here\n",
        "                        print(\"  Transposing quantized LM Head weights...\")\n",
        "                        data_before_save = data_before_save.T\n",
        "                except Exception as quant_e:\n",
        "                     print(f\"  ERROR quantizing {name}: {quant_e}\"); conversion_errors += 1; continue\n",
        "            else: # Не float - не квантуем\n",
        "                 data_before_save = param_data_fp32; dtype_to_pass = data_before_save.dtype; current_precision_tag = DTYPE_MAPPING.get(dtype_to_pass, default_precision_tag); metadata_extra_to_pass = None\n",
        "                 if requires_transpose: # Все равно транспонируем, если нужно\n",
        "                      print(f\"  Transposing non-quantized {name}...\")\n",
        "                      data_before_save = data_before_save.T\n",
        "        else: # Не embedding и не lm_head\n",
        "            try:\n",
        "                target_np_dtype = default_torch_dtype.numpy_dtype if hasattr(default_torch_dtype, 'numpy_dtype') else np.float16\n",
        "                data_before_save = param_data_fp32.astype(target_np_dtype)\n",
        "                dtype_to_pass = data_before_save.dtype; current_precision_tag = default_precision_tag\n",
        "                metadata_extra_to_pass = None\n",
        "                # Транспонируем если нужно\n",
        "                if requires_transpose:\n",
        "                    print(f\"  Transposing {name} weights...\")\n",
        "                    data_before_save = data_before_save.T\n",
        "            except Exception as cast_e:\n",
        "                 print(f\"  ERROR casting/transposing {name}: {cast_e}\"); conversion_errors += 1; continue\n",
        "\n",
        "        # Финальные данные для сохранения\n",
        "        knowledge_data_to_pass = data_before_save\n",
        "        final_shape_to_save = knowledge_data_to_pass.shape if knowledge_data_to_pass is not None else None\n",
        "\n",
        "        # Шаг 5: Финализация тегов\n",
        "        final_tags = list(tags)\n",
        "        if current_precision_tag != default_precision_tag and default_precision_tag in final_tags:\n",
        "            final_tags.remove(default_precision_tag)\n",
        "        if current_precision_tag:\n",
        "            final_tags.append(current_precision_tag)\n",
        "        final_tags = sorted(list(set(final_tags)))\n",
        "\n",
        "        print(f\"  Final Tags: {final_tags}\"); print(f\"  Coordinate: {knowledge_coord}\")\n",
        "        print(f\"  Data to save: dtype={dtype_to_pass}, shape={final_shape_to_save}\") # Используем final_shape_to_save\n",
        "        if metadata_extra_to_pass: print(f\"  Extra Metadata: {metadata_extra_to_pass}\")\n",
        "\n",
        "        # Шаг 6: Создание Тензора\n",
        "        create_result = vec.create_tensor(\n",
        "             coord=knowledge_coord,\n",
        "             tensor_type=\"knowledge\",\n",
        "             knowledge_data=knowledge_data_to_pass, # Передаем возможно транспонированные данные\n",
        "             tags=final_tags,\n",
        "             dtype=dtype_to_pass,\n",
        "             shape=final_shape_to_save, # Передаем правильную форму\n",
        "             name_id=name_id,\n",
        "             metadata_extra=metadata_extra_to_pass,\n",
        "             status=\"active\"\n",
        "         )\n",
        "\n",
        "        # Шаг 8: Сохранение Тензора\n",
        "        knowledge_id = vec.save_tensor(create_result) # Передаем список\n",
        "\n",
        "        if knowledge_id:\n",
        "            knowledge_map[name] = knowledge_id\n",
        "            param_count += 1\n",
        "        else:\n",
        "            conversion_errors += 1\n",
        "            print(f\"  ERROR saving tensor for {name}\")\n",
        "\n",
        "    except Exception as create_save_e:\n",
        "        print(f\"  ERROR during create/save for {name}: {create_save_e}\")\n",
        "        traceback.print_exc(); conversion_errors += 1\n",
        "    finally:\n",
        "        if param_data_fp32 is not None:\n",
        "            del param_data_fp32 # Освобождаем память\n",
        "        loop_end_time = time.time()\n",
        "        # print(f\"  Param {idx+1} time: {loop_end_time - loop_start_time:.2f}s\") # Сократим лог\n",
        "\n",
        "# --- Конец Цикла ---\n",
        "\n",
        "print(f\"\\n--- Finished saving {param_count} knowledge tensors to {vec.db.db_root_path if vec.db else 'N/A'} ---\")\n",
        "if conversion_errors > 0:\n",
        "    print(f\"!!! WARNING: {conversion_errors} errors occurred during knowledge conversion !!!\")\n",
        "\n",
        "# --- Сохранение Name ID Map ---\n",
        "name_map_file = DB_PATH / f\"{model_NAME}_name_id_map.pkl\"\n",
        "try:\n",
        "    map_data_to_save = {\n",
        "        \"name_to_id\": ORIGINAL_NAME_TO_ID_MAP,\n",
        "        \"id_to_name\": ID_TO_ORIGINAL_NAME_MAP,\n",
        "        \"next_id\": NEXT_NAME_ID\n",
        "    }\n",
        "    with open(name_map_file, 'wb') as f:\n",
        "        pickle.dump(map_data_to_save, f)\n",
        "    print(f\"\\nName <-> ID map saved to {name_map_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving name ID map: {e}\")\n",
        "\n",
        "# --- Сохранение Knowledge Map (для Cell 5.5) ---\n",
        "# Имя файла определяется в Cell 4.5, но мы его здесь переопределим для надежности\n",
        "# ... (код сохранения knowledge_map) ...\n",
        "print(f\"--- Saving Knowledge Map (for Cell 5.5) ---\")\n",
        "knowledge_map_filename = f\"{model_NAME}_knowledge_map.pkl\"\n",
        "knowledge_map_filepath = DB_PATH / knowledge_map_filename\n",
        "try:\n",
        "    with open(knowledge_map_filepath, 'wb') as f:\n",
        "        pickle.dump(knowledge_map, f) # Убедись, что knowledge_map здесь актуальна\n",
        "    print(f\"  Knowledge map saved to {knowledge_map_filepath}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving knowledge map: {e}\")\n",
        "\n",
        "# --- ВАЖНО: Сохранение индекса знаний ---\n",
        "if 'vec' in locals() and vec.db:\n",
        "    knowledge_index_filename = f\"{model_NAME}_knowledge_index.pkl\" # Имя файла индекса знаний\n",
        "    knowledge_index_filepath = DB_PATH / knowledge_index_filename\n",
        "    print(f\"\\nINFO: Attempting to save knowledge index ({len(vec.db.index)} entries) via save_index_as to {knowledge_index_filepath}...\")\n",
        "    try:\n",
        "        # Вызов нового метода для сохранения индекса в отдельный файл\n",
        "        vec.db.save_index_as(knowledge_index_filepath)\n",
        "        print(f\"INFO: Call to save_index_as completed for {knowledge_index_filepath.name}.\")\n",
        "    except Exception as sia_e:\n",
        "        print(f\"ERROR during save_index_as: {sia_e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # --- ВАЖНО: Закрытие соединения ---\n",
        "    print(\"\\nClosing DB connection for Cell 5...\")\n",
        "    vec.db.close() # Сохраняет основной индекс (если он был изменен) и закрывает\n",
        "    print(\"DB connection closed.\")\n",
        "else:\n",
        "    print(\"Warning: vec or vec.db not found at the end of Cell 5.\")\n",
        "\n",
        "# --- Очистка памяти ---\n",
        "del vec # Удаляем объект, чтобы освободить ресурсы\n",
        "if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"\\nMemory cleanup attempted.\")\n",
        "print(f\"--- Cell 5 Finished ---\")\n",
        "\n",
        "# --- Завершение Ячейки 5 ---\n",
        "end_cell5_time = time.time()\n",
        "print(f\"--- Cell 5 Finished in {end_cell5_time - start_cell5_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "id": "bjqLbM14jyYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5.5: Save Intermediate Data for Cell 6 (Corrected) ===\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "print(\"\\\\n--- Running Cell 5.5: Saving Intermediate Data ---\")\n",
        "\n",
        "# --- Проверка наличия необходимых переменных из предыдущих ячеек ---\n",
        "# ИЗМЕНЕНО: Убрали 'name_id_map' из проверки, так как сама карта не нужна,\n",
        "# только имя ее файла, которое мы получаем из intermediate_data.\n",
        "required_vars = ['model_NAME', 'DB_PATH', 'knowledge_map']\n",
        "for var_name in required_vars:\n",
        "    if var_name not in locals():\n",
        "        raise NameError(f\"Variable '{var_name}' not found. Ensure previous cells ran successfully.\")\n",
        "    # Дополнительные проверки типов для надежности\n",
        "    if var_name == 'DB_PATH' and not isinstance(DB_PATH, Path):\n",
        "         raise TypeError(f\"'{var_name}' should be a Path object.\")\n",
        "    if var_name == 'knowledge_map' and not isinstance(locals()[var_name], dict):\n",
        "         raise TypeError(f\"'{var_name}' should be a dictionary.\")\n",
        "\n",
        "# --- Определяем пути ---\n",
        "# Используем model_NAME для уникальности файлов\n",
        "intermediate_data_filename = f\"{model_NAME}_cell6_input_data.pkl\"\n",
        "intermediate_data_filepath = DB_PATH / intermediate_data_filename\n",
        "\n",
        "# Имена файлов карт и индекса, которые должны были быть сохранены в Cell 5\n",
        "knowledge_map_filename = f\"{model_NAME}_knowledge_map.pkl\"\n",
        "name_map_filename = f\"{model_NAME}_name_id_map.pkl\" # Имя файла для карты имен\n",
        "knowledge_index_filename = f\"{model_NAME}_knowledge_index.pkl\" # Имя файла индекса знаний\n",
        "\n",
        "# --- Данные для сохранения ---\n",
        "# Сохраняем пути к файлам карт и индексного файла знаний, а не сами карты\n",
        "# Также сохраняем конфиг модели (если он нужен в Cell 6)\n",
        "model_config_to_save = None\n",
        "if 'model' in locals() and hasattr(model, 'config'):\n",
        "    model_config_to_save = model.config\n",
        "    print(\"Found model config to save.\")\n",
        "elif 'model_config' in locals(): # Если конфиг был загружен отдельно\n",
        "    model_config_to_save = model_config\n",
        "    print(\"Found separately loaded model_config to save.\")\n",
        "else:\n",
        "    print(\"Warning: Model config not found, Cell 6 might need it loaded separately.\")\n",
        "\n",
        "cell6_input_data = {\n",
        "    'model_name': model_NAME,\n",
        "    'db_path_str': str(DB_PATH.resolve()), # Сохраняем путь к БД как строку\n",
        "    'model_config': model_config_to_save, # Сохраняем конфиг (или None)\n",
        "    # Сохраняем имена файлов, чтобы Cell 6 знала, что загружать\n",
        "    'knowledge_map_filename': knowledge_map_filename,\n",
        "    'name_map_filename': name_map_filename, # Сохраняем имя файла карты имен\n",
        "    'knowledge_index_filename': knowledge_index_filename\n",
        "}\n",
        "\n",
        "# --- Сохранение ---\n",
        "try:\n",
        "    # Убедимся, что директория DB_PATH существует\n",
        "    DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving intermediate data for Cell 6 to: {intermediate_data_filepath}\")\n",
        "    with open(intermediate_data_filepath, 'wb') as f:\n",
        "        pickle.dump(cell6_input_data, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Intermediate data saved successfully.\")\n",
        "    print(f\"  Included model name: {model_NAME}\")\n",
        "    print(f\"  Included DB path: {cell6_input_data['db_path_str']}\")\n",
        "    print(f\"  Included knowledge map filename: {knowledge_map_filename}\")\n",
        "    print(f\"  Included name map filename: {name_map_filename}\") # Убедимся, что имя файла сохранено\n",
        "    print(f\"  Included knowledge index filename: {knowledge_index_filename}\")\n",
        "    if model_config_to_save:\n",
        "        print(f\"  Included Model Config Type: {type(model_config_to_save)}\")\n",
        "    else:\n",
        "        print(\"  Model Config was not included.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR saving intermediate data: {e} !!!---\")\n",
        "    traceback.print_exc()\n",
        "    # Можно добавить raise e, если критично прервать выполнение\n",
        "else:\n",
        "    print(\"--- Cell 5.5 Finished ---\")\n",
        "\n",
        "# --- Очистка памяти (опционально) ---\n",
        "# import gc\n",
        "# if 'model' in locals(): del model\n",
        "# if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "# print(\"Cleaned up model from memory (optional).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j_G7u5Qyj6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/db/g500"
      ],
      "metadata": {
        "id": "OJnlx5dVWmh-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qooJcIfBiMNj"
      },
      "outputs": [],
      "source": [
        "# === Cell 6 (Updated v6 - Correct Path Definition Order) ===\n",
        "# Создает процессоры Veector, загружая индекс знаний из отдельного файла\n",
        "# и передавая hidden_size в операцию Attention.\n",
        "# ПРЕДПОЛАГАЕТ, что переменные DB_PATH (Path object) и model_NAME (str)\n",
        "# УЖЕ ОПРЕДЕЛЕНЫ в предыдущих ячейках этой сессии Colab.\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Проверка наличия ГЛОБАЛЬНЫХ переменных ---\n",
        "# Эти переменные должны быть установлены в предыдущих ячейках (например, Cell 3 или 4)\n",
        "if 'DB_PATH' not in globals() or not isinstance(DB_PATH, Path):\n",
        "     # Если DB_PATH не определен глобально, попробуем установить значение по умолчанию\n",
        "     print(\"WARN: Global variable DB_PATH not found, using default './data/db'\")\n",
        "     DB_PATH = Path(\"./data/db\")\n",
        "     # raise NameError(\"Переменная DB_PATH не определена или не является объектом Path. Выполните ячейку с её определением.\")\n",
        "if 'model_NAME' not in globals() or not isinstance(model_NAME, str):\n",
        "     # Если model_NAME не определен глобально, попробуем установить значение по умолчанию\n",
        "     print(\"WARN: Global variable model_NAME not found, using default 'DeepSeek-R1-Distill-Qwen-1.5B'\")\n",
        "     model_NAME = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "     # raise NameError(\"Переменная model_NAME не определена или не является строкой. Выполните ячейку с её определением.\")\n",
        "\n",
        "print(f\"Using DB_PATH: {DB_PATH.resolve()}\")\n",
        "print(f\"Using model_NAME: {model_NAME}\")\n",
        "# --- Конец проверки ---\n",
        "\n",
        "# --- Необходимые библиотеки ---\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    from transformers import AutoTokenizer, AutoConfig\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Missing essential libraries (torch, transformers): {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Импорты проекта Veector (v0.7.13+, v0.9.8+) ---\n",
        "try:\n",
        "    from core import Veector, CORE_VERSION\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING, get_tensor_hash,\n",
        "        TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_ATTENTION,\n",
        "        TAG_FUNC_FFN, TAG_FUNC_LINEAR, TAG_COMP_LAYERNORM, TAG_MODEL_DEEPSEEK,\n",
        "        tag_layer, GROUP_IDX_QWEN_PROCESSOR, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LM_HEAD,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "    from operations import OPERATIONS_VERSION\n",
        "\n",
        "    print(f\"Using Core: {CORE_VERSION}, Tensors: {TENSORS_VERSION}, Ops: {OPERATIONS_VERSION}, DB: {VEECTORDB_VERSION}\")\n",
        "    if CORE_VERSION < \"0.7.13\": raise ImportError(\"Core version (expected 0.7.13+) too old\")\n",
        "    if OPERATIONS_VERSION < \"0.8.9\": print(\"WARN: Expected operations v0.8.9+\")\n",
        "    if TENSORS_VERSION < \"0.7.6\": raise ImportError(\"Tensors version too old\")\n",
        "    if VEECTORDB_VERSION < \"0.9.8\": raise ImportError(\"VeectorDB version (expected 0.9.8+) too old\")\n",
        "    print(\"Veector components imported successfully.\")\n",
        "\n",
        "    # OP Kody\n",
        "    OP_ADD=[0,0,2]; OP_LINEAR=OP_MATRIX_MULTIPLY=[30,0,0]; OP_EMBEDDING_LOOKUP=[40,6,0];\n",
        "    OP_LINEAR_HEAD=OP_LINEAR; META_OP_CATEGORY=99; OP_STORE=[99,0,0]; OP_LOAD=[99,0,1];\n",
        "    OP_QWEN2_RMSNORM = [300, 0, 0]; OP_QWEN2_ATTENTION = [300, 1, 0]; OP_QWEN2_MLP = [300, 2, 0]\n",
        "    OP_GET_TUPLE_ELEM_0 = [99, 3, 0]; OP_GET_TUPLE_ELEM_1 = [99, 3, 1]; OP_GET_TUPLE_ELEM_2 = [99, 3, 2]\n",
        "\n",
        "except ImportError as e: print(f\"FATAL ERROR: Failed to import Veector components: {e}\"); raise\n",
        "except Exception as e_other: print(f\"FATAL ERROR during Veector imports: {e_other}\"); raise\n",
        "\n",
        "# --- Загрузка Промежуточных Данных из Cell 5.5 ---\n",
        "print(\"\\\\n--- Loading Intermediate Data from Cell 5.5 ---\")\n",
        "intermediate_data = None\n",
        "# model_NAME and DB_PATH should be defined globally from previous cells\n",
        "model_config = None\n",
        "knowledge_map_filename = None\n",
        "name_map_filename = None\n",
        "knowledge_index_filename = None\n",
        "knowledge_map = None\n",
        "name_id_map_data = None\n",
        "intermediate_data_filepath = None # Initialize None\n",
        "\n",
        "try:\n",
        "    # ИЗМЕНЕНО: Формируем путь здесь, используя ГЛОБАЛЬНЫЕ DB_PATH и model_NAME\n",
        "    intermediate_data_filepath = DB_PATH / f\"{model_NAME}_cell6_input_data.pkl\"\n",
        "\n",
        "    if not intermediate_data_filepath.is_file():\n",
        "        raise FileNotFoundError(f\"Intermediate data file not found at path: {intermediate_data_filepath}. Please ensure Cell 5.5 ran successfully and DB_PATH/model_NAME are correct.\")\n",
        "\n",
        "    print(f\"Loading intermediate data from: {intermediate_data_filepath}\")\n",
        "    with open(intermediate_data_filepath, 'rb') as f:\n",
        "        intermediate_data = pickle.load(f)\n",
        "\n",
        "    # Извлекаем данные из загруженного словаря\n",
        "    loaded_model_name = intermediate_data.get('model_name')\n",
        "    loaded_db_path_str = intermediate_data.get('db_path_str')\n",
        "    model_config = intermediate_data.get('model_config')\n",
        "    knowledge_map_filename = intermediate_data.get('knowledge_map_filename')\n",
        "    name_map_filename = intermediate_data.get('name_map_filename')\n",
        "    knowledge_index_filename = intermediate_data.get('knowledge_index_filename')\n",
        "\n",
        "    # Проверяем согласованность и наличие ключей\n",
        "    if loaded_model_name != model_NAME: print(f\"WARN: model_NAME mismatch ('{model_NAME}' vs loaded '{loaded_model_name}')\")\n",
        "    if loaded_db_path_str != str(DB_PATH.resolve()): print(f\"WARN: DB_PATH mismatch ('{str(DB_PATH.resolve())}' vs loaded '{loaded_db_path_str}')\")\n",
        "    if not all([model_config, knowledge_map_filename, name_map_filename, knowledge_index_filename]):\n",
        "        raise ValueError(\"Intermediate data file is missing required keys (model_config, map filenames, index filename).\")\n",
        "\n",
        "    # Загружаем карты знаний и имен\n",
        "    knowledge_map_filepath = DB_PATH / knowledge_map_filename\n",
        "    name_map_filepath = DB_PATH / name_map_filename\n",
        "    with open(knowledge_map_filepath, 'rb') as f: knowledge_map = pickle.load(f)\n",
        "    if name_map_filepath.is_file():\n",
        "         with open(name_map_filepath, 'rb') as f: name_id_map_data = pickle.load(f)\n",
        "         print(f\"Loaded name ID map from {name_map_filepath}\")\n",
        "    else: print(f\"Warning: Name ID map file not found at {name_map_filepath}\")\n",
        "\n",
        "    print(\"Intermediate data loaded successfully.\")\n",
        "    print(f\"  Knowledge Map Entries: {len(knowledge_map)}\")\n",
        "    print(f\"  Knowledge Index File: {knowledge_index_filename}\")\n",
        "\n",
        "except FileNotFoundError as e: print(f\"FATAL ERROR: {e}\"); raise\n",
        "except Exception as e: print(f\"FATAL ERROR loading intermediate data or maps: {e}\"); traceback.print_exc(); raise\n",
        "\n",
        "# --- Извлечение параметров модели из конфига ---\n",
        "try:\n",
        "    num_layers = model_config.num_hidden_layers\n",
        "    num_attention_heads = model_config.num_attention_heads\n",
        "    num_key_value_heads = getattr(model_config, 'num_key_value_heads', num_attention_heads)\n",
        "    hidden_size = model_config.hidden_size\n",
        "    head_dim = hidden_size // num_attention_heads\n",
        "    rms_norm_eps = model_config.rms_norm_eps\n",
        "    print(f\"Model Config Params: L={num_layers}, H={num_attention_heads}, KVH={num_key_value_heads}, HDim={head_dim}, Hidden={hidden_size}, Epsilon={rms_norm_eps}\")\n",
        "except AttributeError as e: print(f\"FATAL ERROR: Could not get required parameters from loaded model_config: {e}\"); raise\n",
        "\n",
        "# --- Инициализация Veector с Загрузкой Индекса Знаний ---\n",
        "print(\"\\\\n--- Initializing Veector for Processor Tensors ---\")\n",
        "vec_processor = None\n",
        "knowledge_index_filepath = DB_PATH / knowledge_index_filename\n",
        "main_index_filepath = DB_PATH / VeectorDB.INDEX_FILENAME\n",
        "\n",
        "try:\n",
        "    print(f\"Loading initial index from: '{knowledge_index_filepath.name}'\")\n",
        "    vec_processor = Veector(db_dir=DB_PATH, initial_index_path=knowledge_index_filepath)\n",
        "    if len(vec_processor.db.index) == 0: print(f\"WARNING: Loaded knowledge index from '{knowledge_index_filepath.name}' is empty. Ensure Cell 5 ran and saved its index correctly.\")\n",
        "    else: print(f\"Successfully loaded {len(vec_processor.db.index)} entries from knowledge index.\")\n",
        "    vec_processor.db.index_path = main_index_filepath\n",
        "    print(f\"Default save path set to: '{vec_processor.db.index_path.name}'\")\n",
        "    vec_processor.db._index_dirty = True\n",
        "except Exception as e: print(f\"FATAL: Veector initialization failed: {e}\"); traceback.print_exc(); raise\n",
        "\n",
        "# --- Вспомогательная функция для поиска ID знаний ---\n",
        "def find_knowledge_id(hf_param_name: str) -> Optional[str]:\n",
        "    if knowledge_map is None: print(\"ERROR: knowledge_map is None in find_knowledge_id!\"); return None\n",
        "    return knowledge_map.get(hf_param_name)\n",
        "\n",
        "# --- Определение и Сохранение Процессоров Veector ---\n",
        "print(\"\\\\n--- Defining and Saving Veector Processor Tensors (using High-Level OPs) ---\")\n",
        "processor_errors = 0\n",
        "processor_map: Dict[str, str] = {}\n",
        "\n",
        "def create_and_save_processor(name: str, coord: TensorCoordinate, tags: List[int], interface: Dict, ops_sequences: Dict):\n",
        "    global processor_errors, processor_map, vec_processor\n",
        "    proc_id = None\n",
        "    try:\n",
        "        print(f\"  Defining Processor: {name} at {coord}\")\n",
        "        tensor_structure = vec_processor.create_tensor(coord=coord, tensor_type=\"processor\", tags=tags, interface=interface, ops_sequences=ops_sequences, status=\"active\", name_id=-1)\n",
        "        if not validate_tensor(tensor_structure): raise ValueError(f\"Invalid list structure created for {name}\")\n",
        "        proc_id = vec_processor.save_tensor(tensor_structure)\n",
        "        if proc_id:\n",
        "            map_key = \"\"\n",
        "            if \"Embedding\" in name: map_key = \"embedding\"\n",
        "            elif \"Final Norm\" in name: map_key = \"final_norm\"\n",
        "            elif \"LM Head\" in name: map_key = \"lm_head\"\n",
        "            elif \"Attention Processor L\" in name:\n",
        "              try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"attn_{layer_idx}\"\n",
        "              except: pass\n",
        "            elif \"FFN Processor L\" in name:\n",
        "              try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"ffn_{layer_idx}\"\n",
        "              except: pass\n",
        "            if map_key: processor_map[map_key] = proc_id; print(f\"    SUCCESS: Saved {name} with ID: {proc_id} (Key: {map_key})\")\n",
        "            else: print(f\"    WARN: Saved {name} with ID: {proc_id}, but could not determine map key.\")\n",
        "        else: processor_errors += 1; print(f\"    ERROR saving {name}\")\n",
        "    except Exception as e: print(f\"    ERROR during definition/saving of {name}: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "    return proc_id\n",
        "\n",
        "# --- Параметры для процессоров ---\n",
        "processor_group_idx = GROUP_IDX_QWEN_PROCESSOR; model_tag = TAG_MODEL_DEEPSEEK\n",
        "prec_tag_weights = TAG_PREC_FLOAT16; prec_tag_quant = TAG_PREC_INT8\n",
        "\n",
        "# --- 1. Embedding Processor ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=0)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, model_tag]\n",
        "    param_name = \"embedding_matrix\"; hf_name = \"model.embed_tokens.weight\"\n",
        "    kn_tags = [TAG_COMP_EMBEDDING, model_tag, TAG_COMP_WEIGHTS, prec_tag_quant]\n",
        "    kid = find_knowledge_id(hf_name)\n",
        "    if not kid: raise ValueError(f\"Embedding knowledge tensor ID not found in map for '{hf_name}'.\")\n",
        "    interface = {\"inputs\": [{\"name\":\"token_ids\", \"dtype\":\"int64\"}], \"outputs\": [{\"name\":\"hidden_states\", \"dtype\":\"float16\"}], \"knowledge_needed\": [{\"param_name\": param_name, \"tags\": kn_tags, \"knowledge_id\": kid}]}\n",
        "    ops_sequences = {'default': [[OP_EMBEDDING_LOOKUP, {\"embedding_matrix\": param_name}]]}\n",
        "    create_and_save_processor(\"Embedding Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining Embedding Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "# --- 2. Слои Transformera ---\n",
        "print(f\"\\\\n--- Defining Transformer Layer Processors (0 to {num_layers-1}) using High-Level OPs ---\")\n",
        "for layer_idx in range(num_layers):\n",
        "    layer_tag = tag_layer(layer_idx)\n",
        "    print(f\"  Processing Layer {layer_idx}...\")\n",
        "    # --- 2.A Attention Processor ---\n",
        "    try:\n",
        "        coord_attn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=0)\n",
        "        tags_attn = [TAG_TYPE_PROCESSOR, TAG_FUNC_ATTENTION, layer_tag, model_tag]\n",
        "        kn_defs_attn = [\n",
        "            {\"p\":f\"L{layer_idx}_input_norm_w\", \"t\":[TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.input_layernorm.weight\"},\n",
        "            {\"p\":f\"L{layer_idx}_q_w\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"},\n",
        "            {\"p\":f\"L{layer_idx}_q_b\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.bias\", \"opt\": True},\n",
        "            {\"p\":f\"L{layer_idx}_k_w\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"},\n",
        "            {\"p\":f\"L{layer_idx}_k_b\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.bias\", \"opt\": True},\n",
        "            {\"p\":f\"L{layer_idx}_v_w\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"},\n",
        "            {\"p\":f\"L{layer_idx}_v_b\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.bias\", \"opt\": True},\n",
        "            {\"p\":f\"L{layer_idx}_o_w\",   \"t\":[TAG_COMP_ATTN_O, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"},\n",
        "        ]\n",
        "        knowledge_needs_attn = []\n",
        "        missing_essential = False\n",
        "        for kdef in kn_defs_attn:\n",
        "            kid = find_knowledge_id(kdef[\"f\"])\n",
        "            is_opt = kdef.get(\"opt\", False)\n",
        "            if kid: knowledge_needs_attn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "            elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for Attn L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "        if not missing_essential:\n",
        "            interface_attn = {\"inputs\": [ {\"name\": \"hidden_state_in\"}, {\"name\": \"residual_input\"}, {\"name\": \"position_ids\"}, {\"name\": \"past_key\", \"optional\": True}, {\"name\": \"past_value\", \"optional\": True}, {\"name\": \"start_pos\", \"dtype\": \"int\", \"optional\": True}, {\"name\": \"total_seq_len\", \"dtype\": \"int\", \"optional\": True} ], \"outputs\": [{\"name\": \"attn_block_output\"}], \"knowledge_needed\": knowledge_needs_attn }\n",
        "            ops_sequences_attn = {'default': [\n",
        "                [OP_STORE, 'residual_attn'],\n",
        "                [OP_QWEN2_RMSNORM, {\"norm_weight\": f\"L{layer_idx}_input_norm_w\", \"eps\": rms_norm_eps}],\n",
        "                [OP_QWEN2_ATTENTION, {\"q_weights\": f\"L{layer_idx}_q_w\", \"k_weights\": f\"L{layer_idx}_k_w\", \"v_weights\": f\"L{layer_idx}_v_w\", \"o_weights\": f\"L{layer_idx}_o_w\", \"q_bias\": f\"L{layer_idx}_q_b\", \"k_bias\": f\"L{layer_idx}_k_b\", \"v_bias\": f\"L{layer_idx}_v_b\", \"position_ids\": \"position_ids\", \"past_key\": \"past_key\", \"past_value\": \"past_value\", \"start_pos\": \"start_pos\", \"total_seq_len\": \"total_seq_len\", \"num_heads\": num_attention_heads, \"num_kv_heads\": num_key_value_heads, \"head_dim\": head_dim, \"hidden_size\": hidden_size, \"layer_idx\": layer_idx, \"rope_theta\": model_config.rope_theta}],\n",
        "                [OP_STORE, 'attn_tuple_output'], [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_1], [OP_STORE, 'k_cache_out'],\n",
        "                [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_2], [OP_STORE, 'v_cache_out'],\n",
        "                [OP_LOAD, 'attn_tuple_output'], [OP_GET_TUPLE_ELEM_0],\n",
        "                [OP_ADD, {\"input_a\": \"residual_attn\", \"input_b\": \"_\"}]\n",
        "            ]}\n",
        "            create_and_save_processor(f\"Attention Processor L{layer_idx}\", coord_attn, tags_attn, interface_attn, ops_sequences_attn)\n",
        "        else: processor_errors += 1\n",
        "    except Exception as e: print(f\"Error defining Attn L{layer_idx}: {e}\"); processor_errors += 1\n",
        "    # --- 2.B FFN Processor ---\n",
        "    try:\n",
        "        coord_ffn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=1)\n",
        "        tags_ffn = [TAG_TYPE_PROCESSOR, TAG_FUNC_FFN, layer_tag, model_tag]\n",
        "        kn_defs_ffn = [\n",
        "            {\"p\": f\"L{layer_idx}_post_attn_norm_w\", \"t\": [TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\": f\"model.layers.{layer_idx}.post_attention_layernorm.weight\"},\n",
        "            {\"p\": f\"L{layer_idx}_gate_w\", \"t\": [TAG_COMP_FFN_GATE, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.gate_proj.weight\"},\n",
        "            {\"p\": f\"L{layer_idx}_up_w\",   \"t\": [TAG_COMP_FFN_UP, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],    \"f\": f\"model.layers.{layer_idx}.mlp.up_proj.weight\"},\n",
        "            {\"p\": f\"L{layer_idx}_down_w\", \"t\": [TAG_COMP_FFN_DOWN, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.down_proj.weight\"},\n",
        "        ]\n",
        "        knowledge_needs_ffn = []\n",
        "        missing_essential = False\n",
        "        for kdef in kn_defs_ffn:\n",
        "            kid = find_knowledge_id(kdef[\"f\"])\n",
        "            is_opt = kdef.get(\"opt\", False)\n",
        "            if kid: knowledge_needs_ffn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "            elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for FFN L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "        if not missing_essential:\n",
        "            interface_ffn = {\"inputs\": [{\"name\":\"attn_block_output\"}, {\"name\":\"residual_input\"}], \"outputs\": [{\"name\":\"layer_output\"}], \"knowledge_needed\": knowledge_needs_ffn}\n",
        "            ops_sequences_ffn = {'default': [\n",
        "                [OP_STORE, 'residual_ffn'],\n",
        "                [OP_QWEN2_RMSNORM, {\"norm_weight\": f\"L{layer_idx}_post_attn_norm_w\", \"eps\": rms_norm_eps}],\n",
        "                [OP_QWEN2_MLP, {\"gate_weights\": f\"L{layer_idx}_gate_w\", \"up_weights\": f\"L{layer_idx}_up_w\", \"down_weights\": f\"L{layer_idx}_down_w\", \"hidden_act\": model_config.hidden_act}],\n",
        "                [OP_ADD, {\"input_a\": \"residual_ffn\", \"input_b\": \"_\"}]\n",
        "            ]}\n",
        "            create_and_save_processor(f\"FFN Processor L{layer_idx}\", coord_ffn, tags_ffn, interface_ffn, ops_sequences_ffn)\n",
        "        else: processor_errors += 1\n",
        "    except Exception as e: print(f\"Error defining FFN L{layer_idx}: {e}\"); processor_errors += 1\n",
        "\n",
        "# --- 3. Final Norm Processor ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=1)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_COMP_LAYERNORM, model_tag]\n",
        "    kn_tags = [TAG_COMP_LAYERNORM, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights]\n",
        "    hf_name = \"model.norm.weight\"; kid = find_knowledge_id(hf_name)\n",
        "    if not kid: raise ValueError(f\"Final Norm knowledge tensor ID not found in map for '{hf_name}'.\")\n",
        "    knowledge_needs = [{\"param_name\": \"norm_weight\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "    interface = {\"inputs\": [{\"name\":\"final_hidden_state\"}], \"outputs\": [{\"name\":\"final_normed_state\"}], \"knowledge_needed\": knowledge_needs}\n",
        "    ops_sequences = {'default': [[OP_QWEN2_RMSNORM, {\"norm_weight\": \"norm_weight\", \"eps\": rms_norm_eps}]]}\n",
        "    create_and_save_processor(\"Final Norm Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining Final Norm Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "# --- 4. LM Head Processor ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=2)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_LINEAR, model_tag]\n",
        "    kn_tags = [TAG_COMP_LM_HEAD, model_tag, TAG_COMP_WEIGHTS, prec_tag_quant]\n",
        "    hf_name = \"lm_head.weight\"; kid = find_knowledge_id(hf_name)\n",
        "    if not kid: raise ValueError(f\"LM Head knowledge tensor ID not found in map for '{hf_name}'.\")\n",
        "    knowledge_needs = [{\"param_name\": \"lm_head_weights\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "    interface = {\"inputs\": [{\"name\":\"final_normed_state\"}], \"outputs\": [{\"name\":\"logits\"}], \"knowledge_needed\": knowledge_needs}\n",
        "    ops_sequences = {'default': [[OP_LINEAR_HEAD, {\"weights\": \"lm_head_weights\"}]]}\n",
        "    create_and_save_processor(\"LM Head Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining LM Head Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- Финализация ---\n",
        "print(f\"\\\\n--- Finalizing Cell 6 ({processor_errors} errors during processor creation) ---\")\n",
        "\n",
        "# Сохранение карты процессоров\n",
        "processor_map_filepath = DB_PATH / f\"{model_NAME}_proc_map.pkl\"\n",
        "try:\n",
        "    if processor_errors == 0:\n",
        "        expected_proc_count = 3 + 2 * num_layers\n",
        "        if len(processor_map) == expected_proc_count:\n",
        "             with open(processor_map_filepath, 'wb') as f: pickle.dump(processor_map, f)\n",
        "             print(f\"Processor map saved to {processor_map_filepath} ({len(processor_map)} entries)\")\n",
        "        else:\n",
        "             print(f\"WARN: Processor map has incorrect entry count ({len(processor_map)} vs {expected_proc_count}). NOT SAVED.\")\n",
        "    else: print(f\"Processor map NOT saved due to {processor_errors} errors during creation.\")\n",
        "except Exception as e: print(f\"Error saving processor map: {e}\")\n",
        "\n",
        "# Проверка файла эталонных выходов\n",
        "ref_output_filename = f\"{model_NAME}_hf_reference_outputs_fp32.pkl\"\n",
        "ref_output_path = DB_PATH / ref_output_filename\n",
        "try:\n",
        "    if not ref_output_path.is_file():\n",
        "         print(f\"Warning: Reference output file {ref_output_path} not found. Comparison cell might fail.\")\n",
        "    else:\n",
        "         print(f\"Reference HF outputs assumed to exist at {ref_output_path}\")\n",
        "except Exception as e: print(f\"Error checking reference outputs: {e}\")\n",
        "\n",
        "# --- Закрытие соединения с БД ---\n",
        "if 'vec_processor' in locals() and vec_processor and hasattr(vec_processor, 'db') and vec_processor.db:\n",
        "    print(\"\\\\nClosing Veector DB connection...\")\n",
        "    print(f\"Index size before final save in Cell 6: {len(vec_processor.db.index)}\")\n",
        "    vec_processor.db.close() # Сохраняет основной индекс tensor_index.pkl\n",
        "    print(\"DB connection closed by Cell 6.\")\n",
        "else: print(\"\\\\nWarning: Veector instance for processors not found or already closed.\")\n",
        "\n",
        "# --- Очистка ---\n",
        "gc.collect()\n",
        "if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "print(\"\\\\nMemory cleanup attempted.\")\n",
        "\n",
        "if processor_errors == 0: print(f\"\\\\n--- Cell 6 Finished Successfully ---\")\n",
        "else: print(f\"\\\\n--- Cell 6 Finished with {processor_errors} ERRORS ---\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Inference Cell (Detailed Compare & Stop v7 - Add Special Tokens Fix) ===\n",
        "# Цель: Запуск инференса Veector и сравнение ВСЕХ промежуточных выходов\n",
        "#       с эталонными HF, остановка при первом расхождении.\n",
        "# ИЗМЕНЕНО: Загрузка токенизатора с HF Hub и явное добавление <|User|>, <|Assistant|>\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Необходимые библиотеки ---\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoConfig, PreTrainedTokenizer\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e: print(f\"FATAL ERROR: Missing essential libraries: {e}\"); raise\n",
        "\n",
        "# --- Импорты проекта Veector ---\n",
        "PROJECT_IMPORTS_OK = False\n",
        "CORE_VERSION_REQ = \"0.7.13\"; TENSORS_VERSION_REQ = \"0.7.6\"; VEECTORDB_VERSION_REQ = \"0.9.8\"; OPERATIONS_VERSION_REQ = \"0.8.9\"\n",
        "try:\n",
        "    from core import Veector, CORE_VERSION\n",
        "    from tensors import TensorCoordinate, TENSORS_VERSION, GROUP_IDX_QWEN_KNOWLEDGE\n",
        "    from operations import OPERATIONS_VERSION, softmax\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "    print(f\"Using Core: {CORE_VERSION}, Tensors: {TENSORS_VERSION}, Ops: {OPERATIONS_VERSION}, DB: {VEECTORDB_VERSION}\")\n",
        "    if CORE_VERSION < CORE_VERSION_REQ: raise ImportError(f\"Core version too old (req: {CORE_VERSION_REQ})\")\n",
        "    if TENSORS_VERSION < TENSORS_VERSION_REQ: raise ImportError(f\"Tensors version too old (req: {TENSORS_VERSION_REQ})\")\n",
        "    if VEECTORDB_VERSION < VEECTORDB_VERSION_REQ: raise ImportError(f\"VeectorDB version too old (req: {VEECTORDB_VERSION_REQ})\")\n",
        "    if OPERATIONS_VERSION < OPERATIONS_VERSION_REQ: print(f\"WARN: operations version {OPERATIONS_VERSION} < {OPERATIONS_VERSION_REQ}\")\n",
        "    print(\"Veector components imported successfully.\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "except ImportError as e: print(f\"FATAL ERROR (ImportError): {e}\"); raise\n",
        "except Exception as import_e: print(f\"FATAL ERROR (Other Import Error): {import_e}\"); traceback.print_exc(); raise\n",
        "\n",
        "# --- Вспомогательные функции ---\n",
        "def log_memory_usage(stage: str):\n",
        "    try: process = psutil.Process(os.getpid()); mem_info = process.memory_info(); vmem = psutil.virtual_memory(); print(f\"  [MEM_LOG] {stage}: RSS={mem_info.rss / (1024**2):.2f} MB, RAM Used={vmem.percent:.1f}%\")\n",
        "    except Exception as e: print(f\"  [MEM_LOG] Error getting memory usage: {e}\")\n",
        "\n",
        "def sample_top_p(logits: np.ndarray, temperature: float, top_p: float) -> int:\n",
        "    if np.any(np.isnan(logits)): print(\"ERROR: NaN detected in logits before sampling! Returning argmax.\"); return int(np.argmax(logits))\n",
        "    if temperature < 1e-9: return int(np.argmax(logits))\n",
        "    logits_f32 = logits.astype(np.float32); scaled_logits = logits_f32 / temperature; probabilities = softmax(scaled_logits)\n",
        "    if np.any(np.isnan(probabilities)): print(\"ERROR: NaN detected in probabilities after softmax! Returning argmax.\"); return int(np.argmax(logits_f32))\n",
        "    if 0.0 < top_p < 1.0:\n",
        "        sorted_indices = np.argsort(probabilities)[::-1]; sorted_probabilities = probabilities[sorted_indices]; cumulative_probabilities = np.cumsum(sorted_probabilities); cutoff_index = np.searchsorted(cumulative_probabilities, top_p); cutoff_index = min(cutoff_index, len(sorted_probabilities) - 1); cutoff_prob = sorted_probabilities[cutoff_index]; probabilities[probabilities < cutoff_prob] = 0.0\n",
        "    prob_sum = np.sum(probabilities)\n",
        "    if prob_sum > 1e-9: final_probabilities = probabilities / prob_sum\n",
        "    else: print(\"Warning: All probabilities became zero after top-p. Using argmax.\"); return int(np.argmax(logits_f32))\n",
        "    if np.any(np.isnan(final_probabilities)): print(\"ERROR: NaN detected in final_probabilities before choice! Using argmax.\"); return int(np.argmax(logits_f32))\n",
        "    vocab_size = len(final_probabilities); token_ids = np.arange(vocab_size)\n",
        "    try: final_probabilities /= final_probabilities.sum(); predicted_token_id = np.random.choice(token_ids, p=final_probabilities)\n",
        "    except ValueError as e: print(f\"ERROR in np.random.choice (Top-P): {e}. Prob sum: {np.sum(final_probabilities)}. Using argmax.\"); predicted_token_id = np.argmax(logits_f32)\n",
        "    return int(predicted_token_id)\n",
        "\n",
        "def log_tensor_stats(name: str, tensor: Optional[np.ndarray], log_values: bool = False):\n",
        "    if tensor is None: print(f\"  [STATS] {name}: None\"); return\n",
        "    has_nan = np.any(np.isnan(tensor)); shape_str = str(tensor.shape); dtype_str = str(tensor.dtype)\n",
        "    print(f\"  [STATS] {name}: shape={shape_str}, dtype={dtype_str}, NaN={has_nan}\")\n",
        "    if (has_nan or log_values) and tensor.size > 0 :\n",
        "        try: sample_slice = tensor.flatten()[:5].tolist(); print(f\"          Sample: {sample_slice}\")\n",
        "        except Exception as e: print(f\"          Error getting sample: {e}\")\n",
        "\n",
        "# --- Основная функция инференса и сравнения ---\n",
        "\n",
        "def run_inference_comparison_cell(\n",
        "    text: str,\n",
        "    db_path_str: str = \"./data/db\",\n",
        "    model_name_hf: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    nest_level: int = 1, # float16\n",
        "    temperature: float = 0.1,\n",
        "    top_p: float = 0.9,\n",
        "    max_new_tokens: int = 10,\n",
        "    max_seq_len: Optional[int] = None,\n",
        "    use_kv_cache: bool = True,\n",
        "    compare_outputs: bool = True,\n",
        "    atol: float = 5e-3,\n",
        "    rtol: float = 1e-3\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Запускает инференс Veector и сравнивает ВСЕ промежуточные выходы с эталонными HF.\n",
        "    Останавливается при первом расхождении.\n",
        "    \"\"\"\n",
        "    print(f\"--- Running Inference & Detailed Comparison Cell ---\")\n",
        "    log_memory_usage(\"Start of function\")\n",
        "\n",
        "    db_path = Path(db_path_str)\n",
        "    map_model_name = model_name_hf.split('/')[-1]\n",
        "\n",
        "    # --- Параметры ---\n",
        "    print(f\"Prompt: '{text}'\")\n",
        "    print(f\"DB Path: {db_path.resolve()}\")\n",
        "    print(f\"Model Source: {model_name_hf}\")\n",
        "    print(f\"Nest Level (Precision): {nest_level}\")\n",
        "    print(f\"Sampling: Temp={temperature}, TopP={top_p}\")\n",
        "    print(f\"Max New Tokens: {max_new_tokens}\")\n",
        "    print(f\"Use KV Cache: {use_kv_cache}\")\n",
        "    print(f\"Compare Outputs: {compare_outputs}\")\n",
        "    if compare_outputs: print(f\"Comparison Tolerances: atol={atol}, rtol={rtol}\")\n",
        "\n",
        "    if not db_path.is_dir(): print(f\"ERROR: DB directory not found: {db_path.resolve()}\"); return\n",
        "\n",
        "    # --- Загрузка Токенизатора, Конфига, Карты Процессоров ---\n",
        "    tokenizer = None; model_config = None; processor_map = None\n",
        "    num_layers = 0; num_kv_heads = 0; head_dim = 0; eos_token_id = None; bos_token_id = None\n",
        "    user_token_id = None; assistant_token_id = None; # ID для User/Assistant\n",
        "    fallback_max_seq_len = 2048\n",
        "    try:\n",
        "        # Загружаем токенизатор с HF Hub\n",
        "        print(f\"\\nLoading Tokenizer directly from HF Hub: {model_name_hf}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name_hf, trust_remote_code=True, use_fast=False)\n",
        "        print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "        # --- ИЗМЕНЕНО: Явно добавляем спецтокены перед получением ID ---\n",
        "        user_token = \"<|User|>\"\n",
        "        assistant_token = \"<|Assistant|>\"\n",
        "        num_added = tokenizer.add_special_tokens({\n",
        "            'additional_special_tokens': [user_token, assistant_token]\n",
        "        })\n",
        "        print(f\"Added {num_added} special tokens explicitly ('{user_token}', '{assistant_token}').\")\n",
        "        # ВАЖНО: Если num_added = 0, значит токены уже были известны токенизатору.\n",
        "        # Если num_added > 0, словарь токенизатора был расширен.\n",
        "\n",
        "        # Теперь получаем ID (должны найтись)\n",
        "        bos_token_id = tokenizer.bos_token_id\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "        user_token_id = tokenizer.convert_tokens_to_ids(user_token)\n",
        "        assistant_token_id = tokenizer.convert_tokens_to_ids(assistant_token)\n",
        "\n",
        "        if isinstance(user_token_id, str) or user_token_id == tokenizer.unk_token_id: raise ValueError(f\"Could not find ID for token '{user_token}' even after adding.\")\n",
        "        if isinstance(assistant_token_id, str) or assistant_token_id == tokenizer.unk_token_id: raise ValueError(f\"Could not find ID for token '{assistant_token}' even after adding.\")\n",
        "        # --- Конец изменения ---\n",
        "\n",
        "        if tokenizer.pad_token_id is None: tokenizer.pad_token_id = eos_token_id if eos_token_id is not None else tokenizer.vocab_size\n",
        "        print(f\"Tokens: BOS={bos_token_id}, EOS={eos_token_id}, PAD={tokenizer.pad_token_id}, User={user_token_id}, Assistant={assistant_token_id}\")\n",
        "\n",
        "        print(f\"\\nLoading Config from: {model_name_hf}\")\n",
        "        model_config = AutoConfig.from_pretrained(model_name_hf, trust_remote_code=True)\n",
        "        num_layers = model_config.num_hidden_layers; num_attention_heads = model_config.num_attention_heads\n",
        "        num_kv_heads = getattr(model_config, 'num_key_value_heads', num_attention_heads)\n",
        "        hidden_size = model_config.hidden_size; head_dim = hidden_size // num_attention_heads\n",
        "        rms_norm_eps = model_config.rms_norm_eps\n",
        "        if max_seq_len is None: max_seq_len = getattr(model_config, 'max_position_embeddings', fallback_max_seq_len)\n",
        "        print(f\"Config: L={num_layers}, H={num_attention_heads}, KVH={num_kv_heads}, HDim={head_dim}, Hidden={hidden_size}, MaxSeqLen={max_seq_len}\")\n",
        "\n",
        "        proc_map_file = db_path / f\"{map_model_name}_proc_map.pkl\"\n",
        "        if not proc_map_file.is_file(): raise FileNotFoundError(f\"Processor map file not found: {proc_map_file}\")\n",
        "        with open(proc_map_file, 'rb') as f: processor_map = pickle.load(f)\n",
        "        print(f\"\\nLoaded processor map ({len(processor_map)} entries)\")\n",
        "\n",
        "    except Exception as e: print(f\"ERROR loading prerequisites: {e}\"); traceback.print_exc(); return\n",
        "\n",
        "    # --- Загрузка Эталонных Выходов ---\n",
        "    hf_outputs = None\n",
        "    if compare_outputs:\n",
        "        print(f\"\\n--- Loading Reference HF Outputs ---\")\n",
        "        ref_output_filename = f\"{map_model_name}_hf_reference_outputs_fp32.pkl\"\n",
        "        ref_output_path = db_path / ref_output_filename\n",
        "        print(f\"Attempting to load reference outputs from: {ref_output_path}\")\n",
        "        try:\n",
        "            if not ref_output_path.is_file(): print(f\"Warning: Reference output file not found: {ref_output_path}. Comparison will be skipped.\"); compare_outputs = False\n",
        "            else:\n",
        "                with open(ref_output_path, 'rb') as f: hf_outputs = pickle.load(f)\n",
        "                if not isinstance(hf_outputs, dict): print(\"Warning: Loaded reference data is not a dictionary.\"); hf_outputs = None; compare_outputs = False\n",
        "                else: print(f\"Successfully loaded {len(hf_outputs)} reference outputs.\")\n",
        "        except Exception as e: print(f\"Warning: Error loading reference outputs: {e}.\"); hf_outputs = None; compare_outputs = False\n",
        "\n",
        "    # --- Инициализация Veector ---\n",
        "    vec: Optional[Veector] = None\n",
        "    try:\n",
        "        vec = Veector(db_dir=db_path)\n",
        "        print(f\"\\nVeector core v{CORE_VERSION} initialized using DB at: {vec.db.db_root_path.resolve()}\")\n",
        "        print(f\"  Index size loaded: {len(vec.db.index)}\")\n",
        "        if len(vec.db.index) == 0: print(\"ERROR: Loaded main index (tensor_index.pkl) is empty!\"); return\n",
        "    except Exception as e: print(f\"FATAL: Veector init failed: {e}\"); traceback.print_exc(); return\n",
        "\n",
        "    # --- Проверка процессоров ---\n",
        "    required_proc_keys = [\"embedding\", \"final_norm\", \"lm_head\"] + [f\"attn_{i}\" for i in range(num_layers)] + [f\"ffn_{i}\" for i in range(num_layers)]\n",
        "    missing_procs = [key for key in required_proc_keys if key not in processor_map]\n",
        "    if missing_procs: print(f\"ERROR: Required processors missing from map: {missing_procs}\"); vec.db.close(); return\n",
        "    embedding_processor_id = processor_map[\"embedding\"]; final_norm_id = processor_map[\"final_norm\"]; lm_head_id = processor_map[\"lm_head\"]\n",
        "    print(\"All required processor IDs found in map.\")\n",
        "\n",
        "    # --- Подготовка Входных Данных ---\n",
        "    prompt_input_ids_np: Optional[np.ndarray] = None\n",
        "    input_ids_list_for_log = []\n",
        "    try:\n",
        "        # --- Используем формат с <|User|> и <|Assistant|> ---\n",
        "        print(\"\\nConstructing prompt tokens (User/Assistant format)...\")\n",
        "        user_text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "        input_ids_list = []\n",
        "        if bos_token_id is not None: input_ids_list.append(bos_token_id)\n",
        "        # Используем ID, полученные после add_special_tokens\n",
        "        input_ids_list.append(user_token_id)\n",
        "        input_ids_list.extend(user_text_ids)\n",
        "        input_ids_list.append(assistant_token_id)\n",
        "        # --- Конец изменения ---\n",
        "\n",
        "        prompt_input_ids_np = np.array([input_ids_list], dtype=np.int64)\n",
        "        input_ids_list_for_log = input_ids_list\n",
        "        print(f\"\\n--- Prepared Input ---\"); print(f\"Input IDs shape: {prompt_input_ids_np.shape}\")\n",
        "        print(f\"Input IDs list: {input_ids_list_for_log}\") # Печатаем список ID\n",
        "        print(f\"Decoded String: '{tokenizer.decode(input_ids_list_for_log)}'\") # Проверяем декодирование\n",
        "    except Exception as e: print(f\"Error constructing prompt tokens: {e}\"); traceback.print_exc(); vec.db.close(); return\n",
        "\n",
        "    # --- Инициализация KV Кэша ---\n",
        "    kv_cache_list: Optional[List[Tuple[np.ndarray, np.ndarray]]] = None\n",
        "    if use_kv_cache:\n",
        "        kv_cache_list = []\n",
        "        cache_dtype = np.float16; batch_size = prompt_input_ids_np.shape[0]\n",
        "        print(f\"\\nInitializing KV Cache for {num_layers} layers...\"); cache_shape = (batch_size, num_kv_heads, max_seq_len, head_dim)\n",
        "        print(f\"  Shape per layer: K={cache_shape}, V={cache_shape}, dtype={cache_dtype}\")\n",
        "        for i in range(num_layers): kv_cache_list.append((np.zeros(cache_shape, dtype=cache_dtype), np.zeros(cache_shape, dtype=cache_dtype)))\n",
        "        print(\"KV Cache initialized.\"); log_memory_usage(\"After KV Cache Init\")\n",
        "    else: print(\"\\nKV Cache is disabled.\")\n",
        "\n",
        "    # --- Запуск Генерации и Сравнения ---\n",
        "    start_inference_time = time.time(); knowledge_group_id = GROUP_IDX_QWEN_KNOWLEDGE\n",
        "    print(f\"\\n--- Starting Autoregressive Generation & Comparison ---\")\n",
        "    generated_ids: List[int] = []; current_input_ids_for_step: np.ndarray = prompt_input_ids_np\n",
        "    prompt_len = current_input_ids_for_step.shape[1]; total_seq_len = prompt_len\n",
        "    full_response_ids = list(prompt_input_ids_np[0])\n",
        "    error_occurred = False; difference_found = False\n",
        "\n",
        "    # --- Функция для сравнения ---\n",
        "    def compare_and_log(key: str, vec_out: Optional[np.ndarray]) -> bool:\n",
        "        nonlocal difference_found\n",
        "        if difference_found or not compare_outputs or hf_outputs is None: return difference_found\n",
        "        print(f\"  Comparing: {key}\")\n",
        "        hf_out = hf_outputs.get(key)\n",
        "        if hf_out is None or vec_out is None: print(f\"    ERROR: Output missing for comparison (HF: {'OK' if hf_out is not None else 'MISSING'}, Veector: {'OK' if vec_out is not None else 'MISSING'})\"); difference_found = True; return True\n",
        "        current_len_vec = vec_out.shape[1] if vec_out.ndim > 1 else 1; current_len_hf = hf_out.shape[1] if hf_out.ndim > 1 else 1\n",
        "        compare_len = current_len_vec if current_len_vec <= current_len_hf else current_len_hf\n",
        "        if vec_out.ndim > 1 and hf_out.ndim > 1: hf_out_sliced = hf_out[:, :compare_len, ...]; vec_out_sliced = vec_out[:, :compare_len, ...]\n",
        "        elif vec_out.ndim == hf_out.ndim : hf_out_sliced = hf_out; vec_out_sliced = vec_out\n",
        "        else: print(f\"    ERROR: Dimension mismatch for slicing {key} (HF: {hf_out.ndim}D, Vec: {vec_out.ndim}D)\"); difference_found = True; return True\n",
        "        print(f\"    HF Shape (fp32): {hf_out_sliced.shape}, dtype: {hf_out_sliced.dtype}\"); print(f\"    Veector Shape (fp16): {vec_out_sliced.shape}, dtype: {vec_out_sliced.dtype}\")\n",
        "        if hf_out_sliced.shape != vec_out_sliced.shape: print(f\"    ERROR: Shape mismatch for {key} after slicing!\"); difference_found = True; return True\n",
        "        try:\n",
        "            hf_out_f32 = hf_out_sliced; vec_out_f32 = vec_out_sliced.astype(np.float32)\n",
        "            are_close = np.allclose(hf_out_f32, vec_out_f32, atol=atol, rtol=rtol)\n",
        "            print(f\"    Result: {'CLOSE' if are_close else '!!! DIFFERENT !!!'}\")\n",
        "            if not are_close: diff = np.abs(hf_out_f32 - vec_out_f32); max_diff = np.max(diff); mean_diff = np.mean(diff); print(f\"      Max Abs Difference:  {max_diff:.6f}\"); print(f\"      Mean Abs Difference: {mean_diff:.6f}\"); print(f\"      HF Sample (fp32):      {hf_out_sliced.flatten()[:5].tolist()}\"); print(f\"      Veector Sample (fp16): {vec_out_sliced.flatten()[:5].tolist()}\"); difference_found = True; return True\n",
        "        except Exception as cmp_e: print(f\"    ERROR during comparison for {key}: {cmp_e}\"); difference_found = True; return True\n",
        "        return False\n",
        "\n",
        "    # --- Основной цикл ---\n",
        "    try:\n",
        "        veector_step0_outputs = {}\n",
        "        for step in range(max_new_tokens):\n",
        "            step_start_time = time.time(); current_seq_length = current_input_ids_for_step.shape[1]\n",
        "            start_pos = total_seq_len - current_seq_length; position_ids = np.arange(start_pos, total_seq_len, dtype=np.int64).reshape(1, current_seq_length)\n",
        "            if total_seq_len > max_seq_len: print(f\"\\nERROR: total_seq_len ({total_seq_len}) exceeds max_seq_len ({max_seq_len}).\"); break\n",
        "            print(f\"\\n--- Step {step + 1}/{max_new_tokens} (Pos: {start_pos}..{total_seq_len-1}) ---\")\n",
        "\n",
        "            # 1. Embedding\n",
        "            print(f\"  Running Embedding...\"); compute_context_embed = { \"input_data\": current_input_ids_for_step, \"required_nest\": nest_level, \"target_knowledge_group\": knowledge_group_id }; embed_result = vec.compute(embedding_processor_id, context=compute_context_embed)\n",
        "            if not (embed_result and embed_result.get(\"status\") == \"completed\"): raise RuntimeError(f\"Embedding failed: {embed_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            current_hidden_states = embed_result.get(\"data\");\n",
        "            if current_hidden_states is None: raise RuntimeError(f\"Embedding returned None data.\")\n",
        "            print(\"    Embedding OK.\");\n",
        "            if step == 0: veector_step0_outputs[\"embed_tokens\"] = current_hidden_states\n",
        "            if step == 0 and compare_outputs and compare_and_log(\"embed_tokens\", current_hidden_states): break\n",
        "\n",
        "            # 2. Слои Трансформера\n",
        "            residual_input = current_hidden_states\n",
        "            for layer_idx in range(num_layers):\n",
        "                print(f\"  Running Layer {layer_idx}...\")\n",
        "                if current_hidden_states is None: raise RuntimeError(f\"Input for Layer {layer_idx} is None.\")\n",
        "                attn_proc_id = processor_map[f\"attn_{layer_idx}\"]; ffn_proc_id = processor_map[f\"ffn_{layer_idx}\"]\n",
        "                attn_context = { \"input_data\": current_hidden_states, \"residual_input\": residual_input, \"required_nest\": nest_level, \"target_knowledge_group\": knowledge_group_id, \"position_ids\": position_ids, \"total_seq_len\": total_seq_len }\n",
        "                if use_kv_cache and kv_cache_list: attn_context[\"past_key\"], attn_context[\"past_value\"] = kv_cache_list[layer_idx]; attn_context[\"start_pos\"] = start_pos\n",
        "                attn_result = vec.compute(attn_proc_id, context=attn_context)\n",
        "                if not (attn_result and attn_result.get(\"status\") == \"completed\"): raise RuntimeError(f\"Attn L{layer_idx} failed: {attn_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "                attn_block_output = attn_result.get(\"data\");\n",
        "                if attn_block_output is None: raise RuntimeError(f\"Attn L{layer_idx} returned None data.\")\n",
        "                result_step_context = attn_result.get(\"step_context\", {})\n",
        "                if use_kv_cache and kv_cache_list: new_k, new_v = result_step_context.get('k_cache_out'), result_step_context.get('v_cache_out'); kv_cache_list[layer_idx] = (new_k, new_v) if new_k is not None and new_v is not None else kv_cache_list[layer_idx]\n",
        "                print(f\"    Attn L{layer_idx} OK.\")\n",
        "                ffn_input = attn_block_output; residual_input_ffn = ffn_input\n",
        "                ffn_context = { \"input_data\": ffn_input, \"residual_input\": residual_input_ffn, \"required_nest\": nest_level, \"target_knowledge_group\": knowledge_group_id }\n",
        "                ffn_result = vec.compute(ffn_proc_id, context=ffn_context)\n",
        "                if not (ffn_result and ffn_result.get(\"status\") == \"completed\"): raise RuntimeError(f\"FFN L{layer_idx} failed: {ffn_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "                layer_output = ffn_result.get(\"data\")\n",
        "                if layer_output is None: raise RuntimeError(f\"FFN L{layer_idx} returned None data.\")\n",
        "                print(f\"    FFN L{layer_idx} OK.\")\n",
        "                current_hidden_states = layer_output; residual_input = layer_output\n",
        "                if step == 0: veector_step0_outputs[f\"L{layer_idx}_layer_output\"] = layer_output\n",
        "                if step == 0 and compare_outputs and compare_and_log(f\"L{layer_idx}_layer_output\", layer_output): break\n",
        "            if difference_found: break\n",
        "\n",
        "            if difference_found: break\n",
        "\n",
        "            # 3. Final Norm\n",
        "            print(\"  Running Final Norm...\"); norm_context = { \"input_data\": current_hidden_states, \"required_nest\": nest_level, \"target_knowledge_group\": knowledge_group_id }; norm_result = vec.compute(final_norm_id, context=norm_context)\n",
        "            if not (norm_result and norm_result.get(\"status\") == \"completed\"): raise RuntimeError(f\"Final Norm failed: {norm_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            final_normed_states = norm_result.get(\"data\");\n",
        "            if final_normed_states is None: raise RuntimeError(f\"Final Norm returned None data.\")\n",
        "            print(\"    Final Norm OK.\");\n",
        "            if step == 0: veector_step0_outputs[\"final_norm\"] = final_normed_states\n",
        "            if step == 0 and compare_outputs and compare_and_log(\"final_norm\", final_normed_states): break\n",
        "\n",
        "            # 4. LM Head\n",
        "            print(\"  Running LM Head...\"); last_token_hidden_state = final_normed_states[:, -1:, :]; lm_head_context = { \"input_data\": last_token_hidden_state, \"required_nest\": nest_level, \"target_knowledge_group\": knowledge_group_id }; logits_result = vec.compute(lm_head_id, context=lm_head_context)\n",
        "            if not (logits_result and logits_result.get(\"status\") == \"completed\"): raise RuntimeError(f\"LM Head failed: {logits_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            final_logits = logits_result.get(\"data\");\n",
        "            if final_logits is None: raise RuntimeError(f\"LM Head returned None data.\")\n",
        "            print(\"    LM Head OK.\");\n",
        "            if step == 0: veector_step0_outputs[\"lm_head\"] = final_logits\n",
        "            if step == 0 and compare_outputs:\n",
        "                hf_lm_head_out = hf_outputs.get(\"lm_head\")\n",
        "                if hf_lm_head_out is not None: hf_last_token_logits = hf_lm_head_out[:, -1:, :]; difference_found = compare_and_log(\"lm_head\", final_logits)\n",
        "                else: print(\"    WARN: Reference 'lm_head' output not found for comparison.\")\n",
        "                if difference_found: break\n",
        "\n",
        "            # 5. Семплирование\n",
        "            print(\"  Sampling next token...\"); last_token_logits = final_logits[0, -1, :]; predicted_token_id = sample_top_p(logits=last_token_logits, temperature=temperature, top_p=top_p)\n",
        "            print(f\"  --> Generated token ID = {predicted_token_id}, Decoded = '{tokenizer.decode([predicted_token_id])}'\")\n",
        "\n",
        "            # 6. Остановка по EOS\n",
        "            if eos_token_id is not None and predicted_token_id == eos_token_id: print(f\"\\nEOS token generated. Stopping.\"); break\n",
        "\n",
        "            # 7. Подготовка к следующей итерации\n",
        "            generated_ids.append(predicted_token_id); full_response_ids.append(predicted_token_id); current_input_ids_for_step = np.array([[predicted_token_id]], dtype=np.int64); total_seq_len += 1\n",
        "            current_token_str = tokenizer.decode([predicted_token_id]); print(current_token_str, end='', flush=True)\n",
        "            if vec: vec.clear_cache(clear_knowledge=False, clear_compute=True)\n",
        "            log_memory_usage(f\"End of Step {step+1}\"); print(f\"  Step {step+1} time: {time.time() - step_start_time:.3f}s\")\n",
        "            if total_seq_len >= max_seq_len: print(f\"\\nMax sequence length reached.\"); break\n",
        "        # --- Konec cikla generacii ---\n",
        "        print()\n",
        "\n",
        "        # --- Вывод результата ---\n",
        "        print(\"\\n--- Final Generated Sequence (Decoded) ---\"); generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True); print(f\"Generated Text Only: '{generated_text}'\")\n",
        "        full_response = tokenizer.decode(full_response_ids, skip_special_tokens=False); print(f\"\\nFull Response (incl. prompt): '{full_response}'\")\n",
        "\n",
        "    except Exception as e: print(f\"\\n--- ERROR during inference execution ---\"); print(f\"{e}\"); traceback.print_exc(); error_occurred = True\n",
        "    finally:\n",
        "        if vec and hasattr(vec, 'db') and vec.db:\n",
        "          try: vec.db.close(); print(\"\\nDatabase connection closed.\")\n",
        "          except Exception as db_close_e: print(f\"Error closing DB connection: {db_close_e}\")\n",
        "\n",
        "    end_inference_time = time.time(); print(f\"\\n--- Inference & Comparison Cell Finished in {end_inference_time - start_inference_time:.3f} seconds ---\"); log_memory_usage(\"End of function\")\n",
        "\n",
        "    # --- Финальный вердикт сравнения ---\n",
        "    if compare_outputs and not error_occurred:\n",
        "        if difference_found: print(\"\\\\n--- RESULT: Differences found during comparison. Stopped at first mismatch. ---\")\n",
        "        else: print(\"\\\\n--- RESULT: All compared outputs are CLOSE! ---\")\n",
        "    elif error_occurred: print(\"\\\\n--- RESULT: Comparison not completed due to runtime errors. ---\")\n",
        "    else: print(\"\\\\n--- RESULT: Comparison was disabled or reference file not found. ---\")\n",
        "\n",
        "\n",
        "# --- Пример вызова ---\n",
        "print(\"\\n--- Starting Example Inference & Comparison Run ---\")\n",
        "run_inference_comparison_cell(\n",
        "    text=\"Hello, how are you?\", # Используем тот же промпт, что и для эталона\n",
        "    db_path_str=\"./data/db\",\n",
        "    model_name_hf=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    nest_level=1, # float16\n",
        "    temperature=0.1,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=10,\n",
        "    use_kv_cache=True,\n",
        "    compare_outputs=True\n",
        ")\n",
        "print(\"--- Example Inference & Comparison Run Finished ---\")\n"
      ],
      "metadata": {
        "id": "z-GMdM34yDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del vec_processor\n",
        "del vec_out_f32\n",
        "del vec\n",
        "if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"\\nMemory cleanup attempted.\")"
      ],
      "metadata": {
        "id": "aK4wuJkbNR3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Inference Cell (Adapted from qwen_inference.py v0.2.42) ===\n",
        "# Цель: Запуск инференса модели Qwen2 с использованием Veector Core\n",
        "#       и высокоуровневых процессоров, созданных в предыдущих ячейках.\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Необходимые библиотеки ---\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoConfig, PreTrainedTokenizer\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Missing essential libraries (torch, transformers): {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Импорты проекта Veector ---\n",
        "# Убедитесь, что файлы проекта доступны в среде Colab (например, в /content/)\n",
        "PROJECT_IMPORTS_OK = False\n",
        "CORE_VERSION_REQ = \"0.7.13\" # Требуем версию с исправленным импортом\n",
        "TENSORS_VERSION_REQ = \"0.7.6\"\n",
        "VEECTORDB_VERSION_REQ = \"0.9.8\" # Требуем версию с поддержкой initial_index_path\n",
        "OPERATIONS_VERSION_REQ = \"0.8.9\"\n",
        "\n",
        "try:\n",
        "    from core import Veector, CORE_VERSION\n",
        "    print(f\"  Imported Core (v{CORE_VERSION})\")\n",
        "    if CORE_VERSION < CORE_VERSION_REQ:\n",
        "         raise ImportError(f\"Inference cell requires core v{CORE_VERSION_REQ}+, found v{CORE_VERSION}\")\n",
        "\n",
        "    from tensors import TensorCoordinate, TENSORS_VERSION, GROUP_IDX_QWEN_KNOWLEDGE\n",
        "    print(f\"  Imported Tensors (v{TENSORS_VERSION})\")\n",
        "    if TENSORS_VERSION < TENSORS_VERSION_REQ:\n",
        "         raise ImportError(f\"Inference cell requires tensors v{TENSORS_VERSION_REQ}+, found v{TENSORS_VERSION}\")\n",
        "\n",
        "    from operations import OPERATIONS_VERSION, softmax\n",
        "    print(f\"  Imported operations (v{OPERATIONS_VERSION})\")\n",
        "    if OPERATIONS_VERSION < OPERATIONS_VERSION_REQ:\n",
        "         print(f\"WARN: operations.py version is {OPERATIONS_VERSION}, but v{OPERATIONS_VERSION_REQ}+ is recommended.\")\n",
        "\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "    print(f\"  Imported VeectorDB (v{VEECTORDB_VERSION})\")\n",
        "    if VEECTORDB_VERSION < VEECTORDB_VERSION_REQ:\n",
        "         raise ImportError(f\"Inference cell requires VeectorDB v{VEECTORDB_VERSION_REQ}+, found v{VEECTORDB_VERSION}\")\n",
        "\n",
        "    print(\"Veector components imported successfully.\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"---!!! FATAL ERROR (ImportError in Inference Cell) !!! ---\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "    print(f\"Ensure files (core v{CORE_VERSION_REQ}+, tensors v{TENSORS_VERSION_REQ}+, operations v{OPERATIONS_VERSION_REQ}+, veectordb v{VEECTORDB_VERSION_REQ}+) are UP-TO-DATE and ACCESSIBLE.\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    raise # Прерываем выполнение ячейки\n",
        "except Exception as import_e:\n",
        "    print(f\"---!!! FATAL ERROR (Other Exception during Import) !!! ---\")\n",
        "    print(f\"Specific error: {import_e}\")\n",
        "    traceback.print_exc()\n",
        "    print(f\"----------------------------------------------------------\")\n",
        "    raise\n",
        "\n",
        "# --- Вспомогательные функции (копия из qwen_inference.py) ---\n",
        "\n",
        "def log_memory_usage(stage: str):\n",
        "    \"\"\"Logiruet tekushhee ispol'zovanie RAM.\"\"\"\n",
        "    try:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem_info = process.memory_info()\n",
        "        vmem = psutil.virtual_memory()\n",
        "        print(f\"  [MEM_LOG] {stage}: RSS={mem_info.rss / (1024**2):.2f} MB, RAM Used={vmem.percent:.1f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [MEM_LOG] Error getting memory usage: {e}\")\n",
        "\n",
        "def sample_top_p(logits: np.ndarray, temperature: float, top_p: float) -> int:\n",
        "    \"\"\"Primenjaet temperature scaling i top-p sampling.\"\"\"\n",
        "    if np.any(np.isnan(logits)):\n",
        "        print(\"ERROR: NaN detected in logits before sampling! Returning argmax.\")\n",
        "        return int(np.argmax(logits))\n",
        "\n",
        "    if temperature < 1e-9:\n",
        "        return int(np.argmax(logits))\n",
        "\n",
        "    logits_f32 = logits.astype(np.float32)\n",
        "    scaled_logits = logits_f32 / temperature\n",
        "    probabilities = softmax(scaled_logits) # Используем softmax из operations\n",
        "\n",
        "    if np.any(np.isnan(probabilities)):\n",
        "        print(\"ERROR: NaN detected in probabilities after softmax! Returning argmax.\")\n",
        "        return int(np.argmax(logits_f32))\n",
        "\n",
        "    if 0.0 < top_p < 1.0:\n",
        "        sorted_indices = np.argsort(probabilities)[::-1]\n",
        "        sorted_probabilities = probabilities[sorted_indices]\n",
        "        cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
        "        cutoff_index = np.searchsorted(cumulative_probabilities, top_p)\n",
        "        cutoff_index = min(cutoff_index, len(sorted_probabilities) - 1)\n",
        "        cutoff_prob = sorted_probabilities[cutoff_index]\n",
        "        probabilities[probabilities < cutoff_prob] = 0.0\n",
        "\n",
        "    prob_sum = np.sum(probabilities)\n",
        "    if prob_sum > 1e-9:\n",
        "        final_probabilities = probabilities / prob_sum\n",
        "    else:\n",
        "        print(\"Warning: All probabilities became zero after top-p. Using argmax.\")\n",
        "        return int(np.argmax(logits_f32))\n",
        "\n",
        "    if np.any(np.isnan(final_probabilities)):\n",
        "        print(\"ERROR: NaN detected in final_probabilities before choice! Using argmax.\")\n",
        "        return int(np.argmax(logits_f32))\n",
        "\n",
        "    vocab_size = len(final_probabilities)\n",
        "    token_ids = np.arange(vocab_size)\n",
        "    try:\n",
        "        # Убедимся, что сумма вероятностей равна 1 перед np.random.choice\n",
        "        final_probabilities /= final_probabilities.sum()\n",
        "        predicted_token_id = np.random.choice(token_ids, p=final_probabilities)\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR in np.random.choice (Top-P): {e}. Prob sum: {np.sum(final_probabilities)}. Using argmax.\")\n",
        "        predicted_token_id = np.argmax(logits_f32)\n",
        "\n",
        "    # Явно преобразуем в int, так как np.random.choice может вернуть numpy.int64\n",
        "    return int(predicted_token_id)\n",
        "\n",
        "def log_tensor_stats(name: str, tensor: Optional[np.ndarray], log_values: bool = False):\n",
        "    \"\"\"Logiruet formu, tip, nalichie NaN i primernye znachenija tenzora.\"\"\"\n",
        "    if tensor is None:\n",
        "        print(f\"  [STATS] {name}: None\")\n",
        "        return\n",
        "    has_nan = np.any(np.isnan(tensor))\n",
        "    shape_str = str(tensor.shape)\n",
        "    dtype_str = str(tensor.dtype)\n",
        "    print(f\"  [STATS] {name}: shape={shape_str}, dtype={dtype_str}, NaN={has_nan}\")\n",
        "    if (has_nan or log_values) and tensor.size > 0 :\n",
        "        try:\n",
        "            # Берем срез и преобразуем в список для печати\n",
        "            sample_slice = tensor.flatten()[:5].tolist()\n",
        "            print(f\"          Sample: {sample_slice}\")\n",
        "        except Exception as e:\n",
        "            print(f\"          Error getting sample: {e}\")\n",
        "\n",
        "# --- Основная функция инференса для Colab ---\n",
        "\n",
        "def run_inference_cell(\n",
        "    text: str,\n",
        "    db_path_str: str = \"./data/db\", # Путь к БД\n",
        "    model_name_hf: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", # Имя для загрузки токенизатора/конфига и карт\n",
        "    nest_level: int = 1, # Уровень точности (1=float16)\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.95,\n",
        "    max_new_tokens: int = 50, # Увеличил немного для примера\n",
        "    max_seq_len: Optional[int] = None, # Макс. длина посл-ти (для кеша), None = автоопределение\n",
        "    use_kv_cache: bool = True # Использовать ли KV кеш\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Запускает инференс модели Qwen2 с использованием Veector в среде Colab.\n",
        "\n",
        "    Args:\n",
        "        text: Входной текст (промпт пользователя).\n",
        "        db_path_str: Путь к директории базы данных Veector.\n",
        "        model_name_hf: Идентификатор модели на Hugging Face или локальный путь\n",
        "                       (используется для загрузки токенизатора, конфига и имен файлов карт).\n",
        "        nest_level: Целевой уровень точности для процессоров Veector (0=int8, 1=fp16, 2=fp32).\n",
        "        temperature: Температура для семплирования.\n",
        "        top_p: Параметр Top-P для семплирования.\n",
        "        max_new_tokens: Максимальное количество новых токенов для генерации.\n",
        "        max_seq_len: Максимальная длина последовательности для KV кеша. Если None, берется из конфига модели.\n",
        "        use_kv_cache: Использовать ли KV кеширование.\n",
        "    \"\"\"\n",
        "    print(f\"--- Running Inference Cell ---\")\n",
        "    log_memory_usage(\"Start of inference function\")\n",
        "\n",
        "    db_path = Path(db_path_str)\n",
        "    map_model_name = model_name_hf.split('/')[-1] # Получаем имя для файлов карт\n",
        "\n",
        "    # --- Параметры ---\n",
        "    print(f\"Prompt: '{text}'\")\n",
        "    print(f\"DB Path: {db_path.resolve()}\")\n",
        "    print(f\"Model Source: {model_name_hf}\")\n",
        "    print(f\"Nest Level (Precision): {nest_level}\")\n",
        "    print(f\"Sampling: Temp={temperature}, TopP={top_p}\")\n",
        "    print(f\"Max New Tokens: {max_new_tokens}\")\n",
        "    print(f\"Use KV Cache: {use_kv_cache}\")\n",
        "\n",
        "    if not db_path.is_dir():\n",
        "        print(f\"ERROR: DB directory not found: {db_path.resolve()}\")\n",
        "        return\n",
        "\n",
        "    # --- Загрузка Токенизатора и Конфига ---\n",
        "    tokenizer: Optional[PreTrainedTokenizer] = None\n",
        "    model_config: Optional[AutoConfig] = None\n",
        "    num_layers = 0\n",
        "    num_kv_heads = 0\n",
        "    head_dim = 0\n",
        "    eos_token_id: Optional[int] = None\n",
        "    bos_token_id: Optional[int] = None\n",
        "    user_token_id: Optional[int] = None\n",
        "    assistant_token_id: Optional[int] = None\n",
        "    fallback_max_seq_len = 2048\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nLoading Tokenizer from: {model_name_hf}\")\n",
        "        # Указываем use_fast=False, если FastTokenizer вызывает проблемы\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name_hf, trust_remote_code=True, use_fast=False)\n",
        "        print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "        bos_token_id = tokenizer.bos_token_id\n",
        "        # Получаем ID спец токенов вручную, если они есть\n",
        "        try:\n",
        "            user_token_id = tokenizer.encode(\"<|User|>\", add_special_tokens=False)[0]\n",
        "            assistant_token_id = tokenizer.encode(\"<|Assistant|>\", add_special_tokens=False)[0]\n",
        "        except Exception as tok_e:\n",
        "             print(f\"Warning: Could not encode special tokens '<|User|>' or '<|Assistant|>' directly: {tok_e}\")\n",
        "             # Пытаемся получить из added_tokens_decoder, если они там есть\n",
        "             user_token_id = tokenizer.added_tokens_decoder.get(tokenizer.vocab.get(\"<|User|>\"), None)\n",
        "             assistant_token_id = tokenizer.added_tokens_decoder.get(tokenizer.vocab.get(\"<|Assistant|>\"), None)\n",
        "\n",
        "        if user_token_id is None or assistant_token_id is None:\n",
        "             print(\"ERROR: Could not determine User/Assistant token IDs.\")\n",
        "             return\n",
        "\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = eos_token_id if eos_token_id is not None else tokenizer.vocab_size\n",
        "            print(f\"Set pad_token_id to {tokenizer.pad_token_id}\")\n",
        "\n",
        "        print(f\"BOS ID: {bos_token_id}, EOS ID: {eos_token_id}, PAD ID: {tokenizer.pad_token_id}\")\n",
        "        print(f\"User ID: {user_token_id}, Assistant ID: {assistant_token_id}\")\n",
        "\n",
        "        print(f\"\\nLoading Config from: {model_name_hf}\")\n",
        "        model_config = AutoConfig.from_pretrained(model_name_hf, trust_remote_code=True)\n",
        "        num_layers = model_config.num_hidden_layers\n",
        "        num_kv_heads = getattr(model_config, 'num_key_value_heads', model_config.num_attention_heads)\n",
        "        head_dim = model_config.hidden_size // model_config.num_attention_heads\n",
        "        if max_seq_len is None:\n",
        "            max_seq_len = getattr(model_config, 'max_position_embeddings', fallback_max_seq_len)\n",
        "        print(f\"Config loaded: L={num_layers}, KVH={num_kv_heads}, HDim={head_dim}, MaxSeqLen={max_seq_len}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading tokenizer or config: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Загрузка Карты Процессоров ---\n",
        "    processor_map: Optional[Dict[str, str]] = None\n",
        "    proc_map_file = db_path / f\"{map_model_name}_proc_map.pkl\"\n",
        "    if proc_map_file.is_file():\n",
        "        try:\n",
        "            with open(proc_map_file, 'rb') as f:\n",
        "                processor_map = pickle.load(f)\n",
        "            print(f\"\\nLoaded processor map ({len(processor_map)} entries) from {proc_map_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load processor map: {e}.\")\n",
        "            return # Карта процессоров обязательна\n",
        "    else:\n",
        "        print(f\"ERROR: Processor map file not found: {proc_map_file}\")\n",
        "        return\n",
        "\n",
        "    # --- Инициализация Veector ---\n",
        "    # Используем основной индексный файл по умолчанию для инференса\n",
        "    vec: Optional[Veector] = None\n",
        "    try:\n",
        "        vec = Veector(db_dir=db_path) # Загружает tensor_index.pkl\n",
        "        print(f\"\\nVeector core v{CORE_VERSION} initialized using DB at: {vec.db.db_root_path.resolve()}\")\n",
        "        print(f\"  Index size loaded: {len(vec.db.index)}\")\n",
        "        if len(vec.db.index) == 0:\n",
        "             print(\"ERROR: Loaded main index (tensor_index.pkl) is empty. Ensure Cell 5 and Cell 6 ran correctly.\")\n",
        "             return\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL: Veector init failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Проверка наличия процессоров в карте ---\n",
        "    required_proc_keys = [\"embedding\", \"final_norm\", \"lm_head\"]\n",
        "    for i in range(num_layers):\n",
        "        required_proc_keys.extend([f\"attn_{i}\", f\"ffn_{i}\"])\n",
        "    missing_procs = [key for key in required_proc_keys if key not in processor_map]\n",
        "    if missing_procs:\n",
        "        print(f\"ERROR: Required processors missing from map: {missing_procs}\")\n",
        "        vec.db.close()\n",
        "        return\n",
        "    embedding_processor_id = processor_map[\"embedding\"]\n",
        "    final_norm_id = processor_map[\"final_norm\"]\n",
        "    lm_head_id = processor_map[\"lm_head\"]\n",
        "    print(\"All required processor IDs found in map.\")\n",
        "\n",
        "    # --- Подготовка Входных Данных (Ручное Формирование) ---\n",
        "    prompt_input_ids_np: Optional[np.ndarray] = None\n",
        "    try:\n",
        "        print(\"\\nManually constructing prompt tokens (GGUF-style)...\")\n",
        "        user_text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "        input_ids_list = []\n",
        "        if bos_token_id is not None:\n",
        "            input_ids_list.append(bos_token_id)\n",
        "\n",
        "        input_ids_list.append(user_token_id)\n",
        "        input_ids_list.extend(user_text_ids)\n",
        "        input_ids_list.append(assistant_token_id)\n",
        "\n",
        "        prompt_input_ids_np = np.array([input_ids_list], dtype=np.int64)\n",
        "\n",
        "        print(f\"\\n--- Prepared Input ---\")\n",
        "        print(f\"Input IDs shape: {prompt_input_ids_np.shape}\")\n",
        "        # print(f\"Input IDs: {prompt_input_ids_np[0].tolist()}\")\n",
        "        print(f\"Decoded Tokens: {tokenizer.convert_ids_to_tokens(prompt_input_ids_np[0].tolist())}\")\n",
        "        print(f\"Decoded String: '{tokenizer.decode(prompt_input_ids_np[0])}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error constructing prompt tokens: {e}\")\n",
        "        traceback.print_exc()\n",
        "        vec.db.close()\n",
        "        return\n",
        "\n",
        "    # --- Инициализация KV Кэша (если используется) ---\n",
        "    kv_cache_list: Optional[List[Tuple[np.ndarray, np.ndarray]]] = None\n",
        "    if use_kv_cache:\n",
        "        kv_cache_list = []\n",
        "        cache_dtype = np.float16 # Используем float16 для кеша\n",
        "        batch_size = prompt_input_ids_np.shape[0] # Обычно 1\n",
        "        print(f\"\\nInitializing KV Cache for {num_layers} layers...\")\n",
        "        cache_shape = (batch_size, num_kv_heads, max_seq_len, head_dim)\n",
        "        print(f\"  Shape per layer: K={cache_shape}, V={cache_shape}, dtype={cache_dtype}\")\n",
        "        for i in range(num_layers):\n",
        "            k_cache_layer = np.zeros(cache_shape, dtype=cache_dtype)\n",
        "            v_cache_layer = np.zeros(cache_shape, dtype=cache_dtype)\n",
        "            kv_cache_list.append((k_cache_layer, v_cache_layer))\n",
        "        print(\"KV Cache initialized.\")\n",
        "        log_memory_usage(\"After KV Cache Init\")\n",
        "    else:\n",
        "        print(\"\\nKV Cache is disabled.\")\n",
        "\n",
        "    # --- Запуск Авторегрессионной Генерации ---\n",
        "    start_inference_time = time.time()\n",
        "    knowledge_group_id = GROUP_IDX_QWEN_KNOWLEDGE # ID группы знаний для Qwen\n",
        "\n",
        "    print(f\"\\n--- Starting Autoregressive Generation ---\")\n",
        "    generated_ids: List[int] = []\n",
        "    current_input_ids_for_step: np.ndarray = prompt_input_ids_np\n",
        "    prompt_len = current_input_ids_for_step.shape[1]\n",
        "    total_seq_len = prompt_len # Текущая общая длина последовательности\n",
        "\n",
        "    full_response_ids = list(prompt_input_ids_np[0]) # Начинаем с ID промпта\n",
        "\n",
        "    try:\n",
        "        for step in range(max_new_tokens):\n",
        "            step_start_time = time.time()\n",
        "            current_seq_length = current_input_ids_for_step.shape[1] # Длина текущего инпута (1 для шагов > 0)\n",
        "            start_pos = total_seq_len - current_seq_length # Позиция начала текущих токенов\n",
        "\n",
        "            # Генерируем position_ids для текущего шага\n",
        "            position_ids = np.arange(start_pos, total_seq_len, dtype=np.int64).reshape(1, current_seq_length)\n",
        "\n",
        "            if total_seq_len > max_seq_len:\n",
        "                print(f\"\\nERROR: total_seq_len ({total_seq_len}) exceeds max_seq_len ({max_seq_len}). Cannot continue.\")\n",
        "                break\n",
        "\n",
        "            print(f\"\\n--- Step {step + 1}/{max_new_tokens} (Pos: {start_pos}..{total_seq_len-1}) ---\")\n",
        "            log_tensor_stats(\"Input IDs\", current_input_ids_for_step)\n",
        "\n",
        "            # 1. Embedding\n",
        "            print(f\"  Running Embedding...\")\n",
        "            compute_context_embed = {\n",
        "                \"input_data\": current_input_ids_for_step,\n",
        "                \"required_nest\": nest_level,\n",
        "                \"target_knowledge_group\": knowledge_group_id\n",
        "            }\n",
        "            embed_result = vec.compute(embedding_processor_id, context=compute_context_embed)\n",
        "            if not (embed_result and embed_result.get(\"status\") == \"completed\"):\n",
        "                raise RuntimeError(f\"Embedding failed at step {step+1}: {embed_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            current_hidden_states = embed_result.get(\"data\")\n",
        "            log_tensor_stats(\"Embedding Output\", current_hidden_states, log_values=(step < 1)) # Логгируем только на первом шаге\n",
        "            if current_hidden_states is None:\n",
        "                raise RuntimeError(f\"Embedding returned None data at step {step+1}.\")\n",
        "\n",
        "            # 2. Слои Трансформера\n",
        "            residual_input = current_hidden_states # Вход для первого residual\n",
        "\n",
        "            for layer_idx in range(num_layers):\n",
        "                # Упрощенное логгирование для слоев\n",
        "                if step > 0 and layer_idx % 5 != 0 and layer_idx != num_layers -1 :\n",
        "                     continue # Пропускаем лог для промежуточных слоев на шагах > 0\n",
        "                print(f\"\\n  Layer {layer_idx}: Processing...\")\n",
        "                log_tensor_stats(f\"L{layer_idx} Input HS\", current_hidden_states, log_values=(step < 1 and layer_idx < 2)) # Логгируем только в начале\n",
        "\n",
        "                attn_proc_id = processor_map[f\"attn_{layer_idx}\"]\n",
        "                ffn_proc_id = processor_map[f\"ffn_{layer_idx}\"]\n",
        "\n",
        "                # Подготовка контекста для Attention\n",
        "                attn_context = {\n",
        "                    \"input_data\": current_hidden_states,\n",
        "                    \"residual_input\": residual_input, # Передаем вход слоя для первого residual\n",
        "                    \"required_nest\": nest_level,\n",
        "                    \"target_knowledge_group\": knowledge_group_id,\n",
        "                    \"position_ids\": position_ids,\n",
        "                    \"total_seq_len\": total_seq_len # Общая длина для маски и RoPE\n",
        "                }\n",
        "                # Добавляем KV кеш в контекст, если он используется\n",
        "                if use_kv_cache and kv_cache_list:\n",
        "                    past_key, past_value = kv_cache_list[layer_idx]\n",
        "                    attn_context[\"past_key\"] = past_key\n",
        "                    attn_context[\"past_value\"] = past_value\n",
        "                    attn_context[\"start_pos\"] = start_pos # Указываем, куда писать в кеш\n",
        "\n",
        "                # Выполнение Attention процессора\n",
        "                attn_result = vec.compute(attn_proc_id, context=attn_context)\n",
        "\n",
        "                if not (attn_result and attn_result.get(\"status\") == \"completed\"):\n",
        "                    prov = attn_result.get(\"provenance\", {})\n",
        "                    error_msg = prov.get(\"error\", \"Unknown error\")\n",
        "                    print(f\"    ERROR: Attn L{layer_idx} failed at step {step+1}: Status={attn_result.get('status')}, Error='{error_msg}'\")\n",
        "                    raise RuntimeError(f\"Attn L{layer_idx} failed at step {step+1}\")\n",
        "\n",
        "                attn_block_output = attn_result.get(\"data\") # Выход Attention + первый Residual Add\n",
        "                result_step_context = attn_result.get(\"step_context\", {})\n",
        "                log_tensor_stats(f\"L{layer_idx} Attn Block Output\", attn_block_output, log_values=(step < 1 and layer_idx < 2))\n",
        "\n",
        "                # Обновление KV кеша (если используется)\n",
        "                if use_kv_cache and kv_cache_list:\n",
        "                    new_key = result_step_context.get('k_cache_out')\n",
        "                    new_value = result_step_context.get('v_cache_out')\n",
        "                    if new_key is not None and new_value is not None:\n",
        "                        if np.any(np.isnan(new_key)) or np.any(np.isnan(new_value)):\n",
        "                            print(f\"    ERROR: NaN detected in new K/V cache for L{layer_idx}! NOT updating cache.\")\n",
        "                            log_tensor_stats(f\"L{layer_idx} NaN New Key\", new_key, log_values=True)\n",
        "                            log_tensor_stats(f\"L{layer_idx} NaN New Value\", new_value, log_values=True)\n",
        "                        else:\n",
        "                            kv_cache_list[layer_idx] = (new_key, new_value) # Обновляем кеш в списке\n",
        "                            # print(f\"    KV Cache updated for L{layer_idx}\")\n",
        "                    else:\n",
        "                        print(f\"    WARN: K/V cache values ('k_cache_out', 'v_cache_out') not found in attn_result step_context for L{layer_idx}. Cache NOT updated.\")\n",
        "\n",
        "                if attn_block_output is None:\n",
        "                    raise RuntimeError(f\"Attn L{layer_idx} returned None data at step {step+1}.\")\n",
        "\n",
        "                # Вход для FFN - это выход Attention блока (уже с residual)\n",
        "                ffn_input = attn_block_output\n",
        "                residual_input_ffn = ffn_input # Вход для второго residual\n",
        "\n",
        "                # Выполнение FFN процессора\n",
        "                ffn_context = {\n",
        "                    \"input_data\": ffn_input,\n",
        "                    \"residual_input\": residual_input_ffn, # Передаем для второго residual\n",
        "                    \"required_nest\": nest_level,\n",
        "                    \"target_knowledge_group\": knowledge_group_id\n",
        "                }\n",
        "                ffn_result = vec.compute(ffn_proc_id, context=ffn_context)\n",
        "\n",
        "                if not (ffn_result and ffn_result.get(\"status\") == \"completed\"):\n",
        "                    prov = ffn_result.get(\"provenance\", {})\n",
        "                    error_msg = prov.get(\"error\", \"Unknown error\")\n",
        "                    print(f\"    ERROR: FFN L{layer_idx} failed at step {step+1}: Status={ffn_result.get('status')}, Error='{error_msg}'\")\n",
        "                    raise RuntimeError(f\"FFN L{layer_idx} failed at step {step+1}\")\n",
        "\n",
        "                layer_output = ffn_result.get(\"data\") # Выход FFN + второй Residual Add\n",
        "                log_tensor_stats(f\"L{layer_idx} FFN Block Output\", layer_output, log_values=(step < 1 and layer_idx < 2))\n",
        "                if layer_output is None:\n",
        "                    raise RuntimeError(f\"FFN L{layer_idx} returned None data at step {step+1}.\")\n",
        "\n",
        "                # Выход текущего слоя становится входом для следующего\n",
        "                current_hidden_states = layer_output\n",
        "                residual_input = layer_output # Обновляем вход для residual следующего слоя\n",
        "                # --- Konec cikla po slojam ---\n",
        "\n",
        "            # 3. Final Norm\n",
        "            print(\"  Running Final Norm...\")\n",
        "            log_tensor_stats(\"Input to Final Norm\", current_hidden_states, log_values=(step < 1))\n",
        "            norm_context = {\n",
        "                \"input_data\": current_hidden_states,\n",
        "                \"required_nest\": nest_level,\n",
        "                \"target_knowledge_group\": knowledge_group_id\n",
        "            }\n",
        "            norm_result = vec.compute(final_norm_id, context=norm_context)\n",
        "            if not (norm_result and norm_result.get(\"status\") == \"completed\"):\n",
        "                raise RuntimeError(f\"Final Norm failed at step {step+1}: {norm_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            final_normed_states = norm_result.get(\"data\")\n",
        "            log_tensor_stats(\"Final Norm Output\", final_normed_states, log_values=(step < 1))\n",
        "            if final_normed_states is None:\n",
        "                raise RuntimeError(f\"Final Norm returned None data at step {step+1}.\")\n",
        "\n",
        "            # 4. LM Head\n",
        "            print(\"  Running LM Head...\")\n",
        "            # Берем скрытое состояние ТОЛЬКО последнего токена\n",
        "            last_token_hidden_state = final_normed_states[:, -1:, :]\n",
        "            log_tensor_stats(\"Input to LM Head\", last_token_hidden_state, log_values=(step < 1))\n",
        "            lm_head_context = {\n",
        "                \"input_data\": last_token_hidden_state,\n",
        "                \"required_nest\": nest_level,\n",
        "                \"target_knowledge_group\": knowledge_group_id\n",
        "            }\n",
        "            logits_result = vec.compute(lm_head_id, context=lm_head_context)\n",
        "\n",
        "            if not (logits_result and logits_result.get(\"status\") == \"completed\"):\n",
        "                raise RuntimeError(f\"LM Head failed at step {step+1}: {logits_result.get('provenance', {}).get('error', 'Unknown error')}\")\n",
        "            final_logits = logits_result.get(\"data\")\n",
        "            log_tensor_stats(\"LM Head Output (Logits)\", final_logits, log_values=(step < 1))\n",
        "            if final_logits is None:\n",
        "                raise RuntimeError(f\"LM Head returned None data at step {step+1}.\")\n",
        "\n",
        "            # 5. Семплирование следующего токена\n",
        "            print(\"  Sampling next token...\")\n",
        "            # Логиты для последнего токена в последовательности (batch=0, seq=-1)\n",
        "            last_token_logits = final_logits[0, -1, :]\n",
        "            log_tensor_stats(f\"Logits for Sampling (Step {step+1})\", last_token_logits, log_values=True) # Логгируем всегда\n",
        "\n",
        "            predicted_token_id = sample_top_p(\n",
        "                logits=last_token_logits,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            # predicted_token_id уже int\n",
        "\n",
        "            print(f\"  --> Generated token ID = {predicted_token_id}, Decoded = '{tokenizer.decode([predicted_token_id])}'\")\n",
        "\n",
        "            # 6. Проверка условия остановки (EOS)\n",
        "            if eos_token_id is not None and predicted_token_id == eos_token_id:\n",
        "                print(f\"\\nEOS token ({eos_token_id}) generated. Stopping generation.\")\n",
        "                break\n",
        "\n",
        "            # 7. Подготовка к следующей итерации\n",
        "            generated_ids.append(predicted_token_id)\n",
        "            full_response_ids.append(predicted_token_id) # Добавляем в полный ответ\n",
        "            # Вход для следующего шага - только последний сгенерированный токен\n",
        "            current_input_ids_for_step = np.array([[predicted_token_id]], dtype=np.int64)\n",
        "            total_seq_len += 1 # Увеличиваем общую длину последовательности\n",
        "\n",
        "            # Печать сгенерированного токена\n",
        "            current_token_str = tokenizer.decode([predicted_token_id])\n",
        "            print(current_token_str, end='', flush=True)\n",
        "\n",
        "            # Очистка кеша вычислений Veector (не кеша знаний)\n",
        "            if vec:\n",
        "                vec.clear_cache(clear_knowledge=False, clear_compute=True)\n",
        "\n",
        "            log_memory_usage(f\"End of Step {step+1}\")\n",
        "            print(f\"  Step {step+1} time: {time.time() - step_start_time:.3f}s\")\n",
        "\n",
        "            # Проверка максимальной длины\n",
        "            if total_seq_len >= max_seq_len:\n",
        "                print(f\"\\nMaximum sequence length ({max_seq_len}) reached. Stopping generation.\")\n",
        "                break\n",
        "        # --- Konec cikla generacii ---\n",
        "        print() # Перевод строки после генерации\n",
        "\n",
        "        # --- Вывод результата ---\n",
        "        print(\"\\n--- Final Generated Sequence (Decoded) ---\")\n",
        "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        print(f\"Generated Text Only: '{generated_text}'\")\n",
        "\n",
        "        full_response = tokenizer.decode(full_response_ids, skip_special_tokens=False)\n",
        "        print(f\"\\nFull Response (incl. prompt): '{full_response}'\")\n",
        "        # print(f\"Generated IDs: {generated_ids}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- ERROR during inference execution ---\")\n",
        "        print(f\"{e}\")\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        # Закрываем соединение с БД Veector\n",
        "        if vec and hasattr(vec, 'db') and vec.db:\n",
        "            try:\n",
        "                vec.db.close()\n",
        "                print(\"\\nDatabase connection closed.\")\n",
        "            except Exception as db_close_e:\n",
        "                print(f\"Error closing DB connection: {db_close_e}\")\n",
        "\n",
        "    end_inference_time = time.time()\n",
        "    print(f\"\\n--- Inference Cell Finished in {end_inference_time - start_inference_time:.3f} seconds ---\")\n",
        "    log_memory_usage(\"End of inference function\")\n",
        "\n",
        "# --- Пример вызова функции (можно разместить в другой ячейке) ---\n",
        "# print(\"\\n--- Starting Example Inference Run ---\")\n",
        "# run_inference_cell(\n",
        "#     text=\"Explain the concept of superposition in quantum computing.\",\n",
        "#     db_path_str=\"./data/db\",\n",
        "#     model_name_hf=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "#     nest_level=1, # float16\n",
        "#     temperature=0.1, # Низкая температура для более детерминированного ответа\n",
        "#     top_p=0.9,\n",
        "#     max_new_tokens=100,\n",
        "#     use_kv_cache=True\n",
        "# )\n",
        "# print(\"--- Example Inference Run Finished ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gfOUfyfuyCZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архивация и скачивание\n",
        "import shutil\n",
        "shutil.make_archive(\"model_DeepSeek-r1-distill-1.5b\", \"zip\", \"data\")\n",
        "zip_name = \"model_DeepSeek-r1-distill-1.5b.zip\""
      ],
      "metadata": {
        "id": "PQxNzSYRmGM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выгрузка на Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "destination_path = f\"/content/drive/My Drive/models/\"\n",
        "shutil.copy(zip_name, destination_path)\n",
        "print(f\"🟢 [LOG] ✅ Архив загружен на Google Drive: {destination_path}\")"
      ],
      "metadata": {
        "id": "fQ5P4_x-mMcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python core.py"
      ],
      "metadata": {
        "id": "j0MjBxQOHM4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python qwen_inference.py"
      ],
      "metadata": {
        "id": "YXxfmkdnJpuL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}