{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwQ2zuezo45rfkfGb5YK5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishan/Veector/blob/base/Veector_split_DeepSeek_R1_Distill_Qwen_1_5b_int8_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 0: Install Dependencies ===\n",
        "!pip install numpy psutil torch transformers accelerate bitsandbytes ipfshttpclient qiskit qiskit-aer requests huggingface_hub -q\n",
        "print(\"Dependencies installed/checked.\")"
      ],
      "metadata": {
        "id": "qXcbuXfgiZ2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Imports (Corrected and Simplified - FINAL) ===\n",
        "\n",
        "# --- Standard Imports ---\n",
        "import numpy as np\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "import random\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "from google.colab import drive, files, userdata # Keep Colab imports\n",
        "from huggingface_hub import login             # Keep HF import\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Keep Transformers imports\n",
        "\n",
        "print(\"Standard/External imports loaded.\")\n",
        "\n",
        "# --- Optional Imports ---\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"Warning: PyTorch not found. GPU features may be limited.\")\n",
        "\n",
        "try:\n",
        "    import ipfshttpclient\n",
        "    IPFS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IPFS_AVAILABLE = False\n",
        "    # print(\"Warning: ipfshttpclient not found. IPFS features disabled.\")\n",
        "\n",
        "try:\n",
        "    from qiskit import QuantumCircuit\n",
        "    from qiskit.providers.aer import Aer\n",
        "    from qiskit import execute\n",
        "    QISKIT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    QISKIT_AVAILABLE = False\n",
        "    # print(\"Warning: Qiskit not found. Quantum operations disabled.\")\n",
        "\n",
        "print(\"Optional imports checked.\")\n",
        "\n",
        "# --- Veector Project Imports (Single Correct Block) ---\n",
        "# Ensure core.py, tensors.py (v0.5.1+), veectordb.py (v0.7.1+),\n",
        "# operations.py, memory.py are uploaded and accessible.\n",
        "PROJECT_IMPORTS_OK = False\n",
        "try:\n",
        "    # Import core classes/functions needed by THIS script (converter/inference)\n",
        "    from core import Veector\n",
        "    from veectordb import VeectorDB # Needed if we re-initialize DB here? Usually not.\n",
        "    from tensors import (\n",
        "        TensorCoordinate, create_tensor, # Needed for creating tensors\n",
        "        # Import ALL necessary TAG and GROUP constants for use in this script\n",
        "        TAG_CAT_TYPE, TAG_CAT_COMPONENT, TAG_CAT_PRECISION, TAG_CAT_MODEL_FAMILY,\n",
        "        TAG_CAT_LAYER_IDX, TAG_CAT_FUNCTION, TAG_CAT_DATA_SEMANTIC, TAG_CAT_USER,\n",
        "        TAG_TYPE_PROCESSOR, TAG_TYPE_KNOWLEDGE, TAG_TYPE_CONVERTER, TAG_TYPE_STATE,\n",
        "        TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_EMBEDDING, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_ATTN_QKV,\n",
        "        TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LAYERNORM,\n",
        "        TAG_COMP_LM_HEAD, TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16,\n",
        "        TAG_PREC_INT8, TAG_PREC_INT4, TAG_MODEL_QWEN2, TAG_MODEL_LLAMA3,\n",
        "        TAG_MODEL_DEEPSEEK, TAG_FUNC_LINEAR, TAG_FUNC_ATTENTION, TAG_FUNC_FFN,\n",
        "        TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_CAST_DTYPE, TAG_FUNC_RESHAPE,\n",
        "        TAG_SEMANTIC_HIDDEN_STATE, TAG_SEMANTIC_LOGITS, TAG_SEMANTIC_TOKEN_IDS,\n",
        "        TAG_SEMANTIC_KV_CACHE, tag_layer,\n",
        "        GROUP_IDX_QWEN_KNOWLEDGE, GROUP_IDX_QWEN_PROCESSOR\n",
        "    )\n",
        "    # Only import from operations/memory if DIRECTLY used in THIS script, otherwise core.py handles it\n",
        "    # from operations import * # Generally not needed here\n",
        "    # from memory import Memory # Generally not needed here\n",
        "\n",
        "    print(\"Veector project components imported successfully for this script.\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"---!!! FATAL ERROR (ImportError) !!! ---\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "    print(f\"Could not import required name from core.py or tensors.py.\")\n",
        "    print(f\"Ensure files are UP-TO-DATE (tensors v0.5.1+, core v0.5.2+), CORRECT, and ACCESSIBLE.\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    # Optionally define dummies if needed for notebook structure\n",
        "except Exception as other_e:\n",
        "    print(f\"---!!! FATAL ERROR (Other Exception during Import) !!! ---\")\n",
        "    print(f\"Specific error: {other_e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"Check imported files for syntax errors.\")\n",
        "    print(f\"----------------------------------------------------------\")\n",
        "\n",
        "# Removed the redundant import check block ('Checking imports...')"
      ],
      "metadata": {
        "id": "BZOpQTOyid6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Очистка директории для чистоты эксперимента\n",
        "!rm -rf data/\n",
        "output_dir = \"data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "taSpnXjzisY2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Аутентификация с Hugging Face\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Добавь HF_TOKEN в секреты Colab!\")\n",
        "login(hf_token)\n",
        "print(\"Аутентификация прошла успешно\")\n",
        "\n",
        "# Подключение Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive подключён\")\n",
        "\n",
        "model_NAME = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# Определяем ОДИН основной путь к БД (например, в data/db/)\n",
        "DB_PATH = Path(\"./data/db/\")\n",
        "DB_PATH.mkdir(parents=True, exist_ok=True) # Создаем data/db, если ее нет\n",
        "print(f\"Using Main Veector DB Path: {DB_PATH.resolve()}\")\n",
        "\n",
        "# Set data type (bfloat16 might not be fully supported everywhere, float16 is safer)\n",
        "TORCH_DTYPE = torch.float16 # Use float16 for wider compatibility\n",
        "\n",
        "print(f\"Model to convert: {model_NAME}\")\n",
        "print(f\"Target Veector DB: {DB_PATH}\")\n",
        "print(f\"Target dtype: {TORCH_DTYPE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FiGWMogi8Bc",
        "outputId": "a96f729e-723a-4a77-a39e-cd56fe8eae79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Аутентификация прошла успешно\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive подключён\n",
            "Using Main Veector DB Path: /content/data/db\n",
            "Model to convert: DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Target Veector DB: data/db\n",
            "Target dtype: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Tag Ontology and Mappings Definition (Sync with tensors.py v0.7.0) ===\n",
        "\n",
        "import torch # Ensure torch is imported for dtype checking if needed later\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union # Import typing for hints\n",
        "\n",
        "# --- Version (for tracking changes in this cell) ---\n",
        "CONVERTER_CELL2_VERSION = \"Synced with tensors.py v0.7.0\"\n",
        "print(f\"--- Running Converter Cell 2 v{CONVERTER_CELL2_VERSION} ---\")\n",
        "\n",
        "# --- Type Hint for Metadata Tuple (from tensors.py) ---\n",
        "# Needed if any functions within Colab cells might use this type hint\n",
        "MetadataTuple = Tuple[\n",
        "    List[Union[float, int]],         # [0] data_description\n",
        "    List[int],                       # [1] coord\n",
        "    List[int],                       # [2] shape\n",
        "    List[int],                       # [3] tags\n",
        "    Optional[Dict],                  # [4] ops_sequences\n",
        "    Optional[Dict],                  # [5] interface\n",
        "    Optional[List],                  # [6] filters\n",
        "    Optional[List],                  # [7] exit_gates\n",
        "    List[int],                       # [8] lifecycle\n",
        "    Optional[List[str]]              # [9] parents\n",
        "]\n",
        "\n",
        "# --- Simplified Tag Ontology (Flat Integers with Ranges - from tensors.py v0.7.0) ---\n",
        "# 1-9: Tensor Type\n",
        "TAG_TYPE_PROCESSOR = 1\n",
        "TAG_TYPE_KNOWLEDGE = 2\n",
        "TAG_TYPE_CONVERTER = 3\n",
        "TAG_TYPE_STATE = 4\n",
        "# 10-19: Model Family\n",
        "TAG_MODEL_QWEN2 = 10\n",
        "TAG_MODEL_LLAMA3 = 11\n",
        "TAG_MODEL_DEEPSEEK = 12\n",
        "# 20-29: Precision\n",
        "TAG_PREC_FLOAT32 = 20\n",
        "TAG_PREC_FLOAT16 = 21\n",
        "TAG_PREC_BFLOAT16 = 22\n",
        "TAG_PREC_INT8 = 23\n",
        "TAG_PREC_INT4 = 24\n",
        "# 30-49: Component Type\n",
        "TAG_COMP_WEIGHTS = 30\n",
        "TAG_COMP_BIAS = 31\n",
        "TAG_COMP_EMBEDDING = 32\n",
        "TAG_COMP_ATTN_Q = 33\n",
        "TAG_COMP_ATTN_K = 34\n",
        "TAG_COMP_ATTN_V = 35\n",
        "TAG_COMP_ATTN_O = 36\n",
        "TAG_COMP_ATTN_QKV = 37\n",
        "TAG_COMP_FFN_GATE = 38\n",
        "TAG_COMP_FFN_UP = 39\n",
        "TAG_COMP_FFN_DOWN = 40\n",
        "TAG_COMP_LAYERNORM = 41\n",
        "TAG_COMP_LM_HEAD = 42\n",
        "# 50-59: Function\n",
        "TAG_FUNC_LINEAR = 50\n",
        "TAG_FUNC_ATTENTION = 51\n",
        "TAG_FUNC_FFN = 52\n",
        "TAG_FUNC_EMBED_LOOKUP = 53\n",
        "TAG_FUNC_CAST_DTYPE = 54\n",
        "TAG_FUNC_RESHAPE = 55\n",
        "# 60-69: Data Semantic Type\n",
        "TAG_SEMANTIC_HIDDEN_STATE = 60\n",
        "TAG_SEMANTIC_LOGITS = 61\n",
        "TAG_SEMANTIC_TOKEN_IDS = 62\n",
        "TAG_SEMANTIC_KV_CACHE = 63\n",
        "# 100-999: Layer Index\n",
        "LAYER_IDX_TAG_OFFSET = 100\n",
        "\n",
        "def tag_layer(idx: int) -> int:\n",
        "    \"\"\"Generates a layer tag using an offset.\"\"\"\n",
        "    if not isinstance(idx, int): raise TypeError(f\"Layer index must be an integer, got {type(idx)}\")\n",
        "    if idx < 0: raise ValueError(f\"Invalid layer index for tagging: {idx}. Must be non-negative.\")\n",
        "    return LAYER_IDX_TAG_OFFSET + idx\n",
        "# 1000+: User Defined Tags\n",
        "USER_TAG_OFFSET = 1000\n",
        "# --- End of Tags ---\n",
        "print(\"Simplified tag ontology (flat integers) defined.\")\n",
        "\n",
        "# --- Group ID Constants (from tensors.py v0.7.0) ---\n",
        "GROUP_IDX_QWEN_KNOWLEDGE = 100\n",
        "GROUP_IDX_QWEN_PROCESSOR = 500\n",
        "GROUP_IDX_LLAMA_KNOWLEDGE = 101\n",
        "GROUP_IDX_LLAMA_PROCESSOR = 501\n",
        "GROUP_IDX_DEEPSEEK_KNOWLEDGE = 102 # Added constant\n",
        "# GROUP_IDX_DEEPSEEK_PROCESSOR = 502 # Optional\n",
        "GROUP_IDX_GENERIC_PROCESSOR = 50\n",
        "print(f\"Group Indices defined: QwenK={GROUP_IDX_QWEN_KNOWLEDGE}, QwenP={GROUP_IDX_QWEN_PROCESSOR}, DeepSeekK={GROUP_IDX_DEEPSEEK_KNOWLEDGE}\")\n",
        "\n",
        "\n",
        "# --- Mappings (from tensors.py v0.7.0) ---\n",
        "# 1. DATA_TYPE_MAPPING\n",
        "DATA_TYPE_MAPPING = {\n",
        "    \"knowledge\": 1,\n",
        "    \"processor\": 2,\n",
        "    \"converter\": 3,\n",
        "    \"state\": 4,\n",
        "}\n",
        "REVERSE_DATA_TYPE_MAPPING = {\n",
        "    1: \"knowledge\",\n",
        "    2: \"processor\",\n",
        "    3: \"converter\",\n",
        "    4: \"state\",\n",
        "}\n",
        "print(f\"DATA_TYPE_MAPPING defined: {DATA_TYPE_MAPPING}\")\n",
        "\n",
        "# 2. DTYPE_MAPPING\n",
        "DTYPE_MAPPING = {\n",
        "    # Standard Names\n",
        "    'float32': 1, 'float16': 2, 'bfloat16': 3, 'int8': 4, 'int4': 5,\n",
        "    'int32': 6, 'int64': 7, 'bool': 8, 'complex64': 9, 'complex128': 10,\n",
        "    # Numpy Types\n",
        "    np.float32: 1, np.float16: 2, np.int8: 4, np.int32: 6, np.int64: 7,\n",
        "    np.bool_: 8, np.complex64: 9, np.complex128: 10,\n",
        "    # PyTorch Types (as strings and potentially objects if torch loaded)\n",
        "    'torch.float32': 1, 'torch.float16': 2, 'torch.bfloat16': 3, 'torch.int8': 4,\n",
        "    'torch.int32': 6, 'torch.int64': 7, 'torch.bool': 8,\n",
        "    'torch.complex64': 9, 'torch.complex128': 10,\n",
        "}\n",
        "# Add torch objects if torch is available\n",
        "if 'torch' in globals():\n",
        "    DTYPE_MAPPING[torch.float32] = 1\n",
        "    DTYPE_MAPPING[torch.float16] = 2\n",
        "    DTYPE_MAPPING[torch.bfloat16] = 3\n",
        "    DTYPE_MAPPING[torch.int8] = 4\n",
        "    DTYPE_MAPPING[torch.int32] = 6\n",
        "    DTYPE_MAPPING[torch.int64] = 7\n",
        "    DTYPE_MAPPING[torch.bool] = 8\n",
        "    DTYPE_MAPPING[torch.complex64] = 9\n",
        "    DTYPE_MAPPING[torch.complex128] = 10\n",
        "\n",
        "REVERSE_DTYPE_MAPPING = {\n",
        "    1: 'float32', 2: 'float16', 3: 'bfloat16', 4: 'int8', 5: 'int4',\n",
        "    6: 'int32', 7: 'int64', 8: 'bool', 9: 'complex64', 10: 'complex128',\n",
        "}\n",
        "print(f\"DTYPE_MAPPING defined.\")\n",
        "\n",
        "# 3. STATUS_MAPPING\n",
        "STATUS_MAPPING = {\n",
        "    \"active\": 1,\n",
        "    \"archived\": 0\n",
        "}\n",
        "REVERSE_STATUS_MAPPING = {\n",
        "    1: \"active\",\n",
        "    0: \"archived\"\n",
        "}\n",
        "print(f\"STATUS_MAPPING defined: {STATUS_MAPPING}\")\n",
        "\n",
        "# --- Metadata Encoding Configuration (from tensors.py v0.7.0) ---\n",
        "METADATA_STRUCTURE_VERSION = 1.1\n",
        "print(f\"Metadata Structure Version: {METADATA_STRUCTURE_VERSION}\")\n",
        "\n",
        "print(\"Tag ontology, Group IDs, Mappings, and Config defined for Cell 2.\")"
      ],
      "metadata": {
        "id": "RW8d1Ziei9t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Initialize Veector (SINGLE Instance) ===\n",
        "from core import Veector # Импортируем класс Veector из core.py\n",
        "try:\n",
        "    # Используем этот путь при инициализации\n",
        "    vec = Veector(db_dir=DB_PATH, ipfs_enabled=False)\n",
        "    print(f\"Veector core initialized using DB at: {DB_PATH.resolve()}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Veector initialization failed: {e}\")\n",
        "    raise RuntimeError(\"Veector Core failed to initialize\") from e"
      ],
      "metadata": {
        "id": "mrtI7PZ6jLjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Load Hugging Face Model ===\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(f\"deepseek-ai/{model_NAME}\", torch_dtype=TORCH_DTYPE, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"deepseek-ai/{model_NAME}\", trust_remote_code=True)\n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"Successfully loaded HF model: {model_NAME}\")\n",
        "    print(f\"Model config: {model.config}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Failed to load HF model '{model_NAME}': {e}\")\n",
        "    # Stop execution\n",
        "    raise RuntimeError(f\"Hugging Face model loading failed\") from e\n",
        "\n",
        "# Clean up GPU memory if possible after loading\n",
        "if TORCH_AVAILABLE and torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Model loaded and memory potentially cleaned.\")"
      ],
      "metadata": {
        "id": "hLSY0WSHjSml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Skript dlja prohoda HF modeli v float32 i sohranenija promezhutochnyh vyhodov ===\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "# --- Neobhodimye biblioteki ---\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Missing essential libraries (torch, transformers): {e}\")\n",
        "    print(\"Please install them: pip install torch transformers accelerate\")\n",
        "    exit()\n",
        "\n",
        "# --- Konfiguracija ---\n",
        "# Ubedites', chto eti peremennye sootvetstvujut vashemu okruzheniju\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "TOKENIZER_SOURCE = MODEL_SOURCE\n",
        "\n",
        "PROMPT = \"Hello, how are you?\" # Tot zhe prompt, chto i v skripte sravnenija\n",
        "# >>> IZMENENO: Novoe imja fajla dlja float32 vyhodov <<<\n",
        "OUTPUT_FILENAME = \"hf_reference_outputs_fp32.pkl\"\n",
        "\n",
        "# --- Zagruzka Tokenizatora ---\n",
        "print(\"\\\\n--- Loading Tokenizer ---\")\n",
        "tokenizer = None\n",
        "try:\n",
        "    print(f\"Loading Tokenizer from: {TOKENIZER_SOURCE}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True)\n",
        "    print(f\"Tokenizer class: {tokenizer.__class__.__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Podgotovka vhodnyh dannyh ---\n",
        "print(\"\\\\n--- Preparing Input IDs ---\")\n",
        "input_ids_torch = None\n",
        "input_seq_len = 0\n",
        "try:\n",
        "    messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
        "    print(\"Applying chat template...\")\n",
        "    prompt_input_ids_np = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    if prompt_input_ids_np.ndim == 1:\n",
        "        prompt_input_ids_np = np.expand_dims(prompt_input_ids_np, axis=0)\n",
        "\n",
        "    input_seq_len = prompt_input_ids_np.shape[1]\n",
        "    # Poka ostavljaem na CPU, model' budet zagruzhena na CPU ili GPU nizhe\n",
        "    input_ids_torch = torch.tensor(prompt_input_ids_np)\n",
        "\n",
        "    print(f\"Input IDs shape: {input_ids_torch.shape}\")\n",
        "    print(f\"Input Sequence Length: {input_seq_len}\")\n",
        "    print(f\"Decoded Input: '{tokenizer.decode(input_ids_torch[0].cpu().numpy())}'\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR preparing input: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Zagruzka i Progon Etalonnoj Modeli v Float32 ---\n",
        "print(f\"\\\\n--- Loading and Running HF Model ({MODEL_SOURCE}) in float32 ---\")\n",
        "hf_outputs = {}\n",
        "hook_handles = []\n",
        "model_fp32 = None\n",
        "\n",
        "def get_hook(name):\n",
        "    def hook_fn(module, input, output):\n",
        "        actual_output = output[0] if isinstance(output, tuple) else output\n",
        "        print(f\"  [HOOK] Captured output for: {name} (Shape: {actual_output.shape}, Device: {actual_output.device})\")\n",
        "        # Sohranjaem na CPU v formate NumPy float32\n",
        "        hf_outputs[name] = actual_output.detach().cpu().numpy().astype(np.float32)\n",
        "    return hook_fn\n",
        "\n",
        "try:\n",
        "    print(f\"Loading HF Model {MODEL_SOURCE} with float32...\")\n",
        "    # >>> IZMENENO: Zagruzhaem s torch_dtype=torch.float32 <<<\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    model_fp32.eval()\n",
        "    # Peremestite na GPU, esli neobhodimo i vozmozhno\n",
        "    # model_fp32.to('cuda')\n",
        "    # input_ids_torch = input_ids_torch.to(model_fp32.device) # Peremestit' vhodnye dannye tozhe\n",
        "    print(f\"HF Model loaded to device: {model_fp32.device}\")\n",
        "\n",
        "    # Registracija Hukov\n",
        "    print(\"Registering hooks for float32 model...\")\n",
        "    model_config = model_fp32.config\n",
        "    num_layers = model_config.num_hidden_layers\n",
        "    hook_handles.append(model_fp32.model.embed_tokens.register_forward_hook(get_hook(\"embed_tokens\")))\n",
        "    for i in range(num_layers):\n",
        "        hook_handles.append(model_fp32.model.layers[i].register_forward_hook(get_hook(f\"layer_{i}_output\")))\n",
        "    hook_handles.append(model_fp32.model.norm.register_forward_hook(get_hook(\"final_norm\")))\n",
        "    hook_handles.append(model_fp32.lm_head.register_forward_hook(get_hook(\"lm_head\")))\n",
        "    print(f\"Registered {len(hook_handles)} hooks.\")\n",
        "\n",
        "    # Prjamoj prohod\n",
        "    print(\"Running HF model forward pass (float32)...\")\n",
        "    with torch.no_grad():\n",
        "        hf_model_output = model_fp32(input_ids_torch.to(model_fp32.device), use_cache=False) # Ubedimsja chto input na tom zhe device\n",
        "    print(\"HF forward pass complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during HF float32 execution: {e}\")\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    # Vsegda udaljajem huki i model' posle ispol'zovanija\n",
        "    for handle in hook_handles: handle.remove()\n",
        "    print(\"Hooks removed.\")\n",
        "    if 'model_fp32' in locals() and model_fp32 is not None:\n",
        "        del model_fp32\n",
        "        if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"Cleaned up float32 model.\")\n",
        "\n",
        "# --- Sohranenie rezul'tatov ---\n",
        "if hf_outputs: # Sohranjaem tol'ko esli chto-to sobrali\n",
        "    print(f\"\\\\n--- Saving Captured Float32 Outputs to {OUTPUT_FILENAME} ---\")\n",
        "    try:\n",
        "        with open(OUTPUT_FILENAME, 'wb') as f:\n",
        "            pickle.dump(hf_outputs, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Successfully saved {len(hf_outputs)} captured outputs.\")\n",
        "        print(\"Saved keys:\", list(hf_outputs.keys()))\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR saving outputs: {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\\\n--- No outputs captured from HF model, skipping save. ---\")\n",
        "\n",
        "\n",
        "print(f\"\\\\n--- Script Finished ---\")\n"
      ],
      "metadata": {
        "id": "AhR8PBMejmvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Convert Parameters to Knowledge Tensors (Transposed Weights) ===\n",
        "\n",
        "import gc\n",
        "import pickle\n",
        "import time\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Импорты из проекта (убедись, что версии >= 0.6.12 и >= 0.7.6) ---\n",
        "try:\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING,\n",
        "        TAG_TYPE_KNOWLEDGE, TAG_MODEL_DEEPSEEK, TAG_COMP_WEIGHTS, TAG_COMP_BIAS,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_LM_HEAD, TAG_COMP_LAYERNORM, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, tag_layer, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    if TENSORS_VERSION < \"0.7.6\":\n",
        "        raise ImportError(f\"Requires tensors v0.7.6+, found v{TENSORS_VERSION}\")\n",
        "    from core import Veector, CORE_VERSION\n",
        "    if CORE_VERSION < \"0.6.12\":\n",
        "        raise ImportError(f\"Requires core v0.6.12+, found v{CORE_VERSION}\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Import failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Версия Ячейки ---\n",
        "CONVERTER_CELL5_VERSION = \"Hybrid v0.7.6 + Quant + Transpose v2\"\n",
        "# --- Конец Версии ---\n",
        "\n",
        "print(f\"--- Running Converter Cell 5 v{CONVERTER_CELL5_VERSION} ---\")\n",
        "start_cell5_time = time.time()\n",
        "\n",
        "# --- Проверка необходимых переменных ---\n",
        "if 'vec' not in locals() or vec is None:\n",
        "    raise NameError(\"'vec' object not defined.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "    raise NameError(\"DB_PATH not defined or invalid.\")\n",
        "if 'model' not in locals() or model is None:\n",
        "    raise NameError(\"HF 'model' not loaded.\")\n",
        "if 'model_NAME' not in locals() or not model_NAME:\n",
        "    raise NameError(\"model_NAME not defined.\")\n",
        "\n",
        "# --- Переинициализация DB (если необходимо) ---\n",
        "if not hasattr(vec, 'db') or vec.db is None:\n",
        "    try:\n",
        "        print(\"Attempting DB re-init for Cell 5...\")\n",
        "        # Импортируем только если нужно, чтобы избежать ненужных импортов вверху\n",
        "        from veectordb import VeectorDB\n",
        "        vec.db = VeectorDB(db_dir=DB_PATH)\n",
        "        print(\"DB connection re-established.\")\n",
        "    except Exception as db_reinit_e:\n",
        "        raise AttributeError(f\"DB re-init failed: {db_reinit_e}\")\n",
        "else:\n",
        "    print(\"'vec' object found and DB connection seems active.\")\n",
        "\n",
        "# --- Инициализация ---\n",
        "ORIGINAL_NAME_TO_ID_MAP: Dict[str, int] = {}\n",
        "ID_TO_ORIGINAL_NAME_MAP: Dict[int, str] = {}\n",
        "NEXT_NAME_ID: int = 0\n",
        "print(\"Initialized Name <-> ID mapping dictionaries.\")\n",
        "\n",
        "knowledge_map: Dict[str, str] = {} # Карта Имя -> ID Знания\n",
        "param_count: int = 0\n",
        "conversion_errors: int = 0\n",
        "\n",
        "# --- Вспомогательная функция для ID ---\n",
        "def get_or_create_name_id(name: Optional[str]) -> int:\n",
        "    \"\"\"Assigns and returns a unique ID for a parameter name.\"\"\"\n",
        "    global NEXT_NAME_ID, ORIGINAL_NAME_TO_ID_MAP, ID_TO_ORIGINAL_NAME_MAP\n",
        "    if not name:\n",
        "        return -1\n",
        "    if name in ORIGINAL_NAME_TO_ID_MAP:\n",
        "        return ORIGINAL_NAME_TO_ID_MAP[name]\n",
        "    current_id = NEXT_NAME_ID\n",
        "    ORIGINAL_NAME_TO_ID_MAP[name] = current_id\n",
        "    ID_TO_ORIGINAL_NAME_MAP[current_id] = name\n",
        "    NEXT_NAME_ID += 1\n",
        "    return current_id\n",
        "\n",
        "# --- Параметры конвертации ---\n",
        "default_precision_tag = TAG_PREC_FLOAT16\n",
        "default_torch_dtype = torch.float16\n",
        "if 'TORCH_DTYPE' in locals(): # Определено в Cell 1\n",
        "    default_torch_dtype = TORCH_DTYPE\n",
        "    if TORCH_DTYPE == torch.float16: default_precision_tag = TAG_PREC_FLOAT16\n",
        "    elif TORCH_DTYPE == torch.bfloat16: default_precision_tag = TAG_PREC_BFLOAT16\n",
        "    elif TORCH_DTYPE == torch.float32: default_precision_tag = TAG_PREC_FLOAT32\n",
        "    elif TORCH_DTYPE == torch.int8: default_precision_tag = TAG_PREC_INT8\n",
        "\n",
        "knowledge_group_idx = GROUP_IDX_QWEN_KNOWLEDGE # 100\n",
        "model_tag = TAG_MODEL_DEEPSEEK # 12\n",
        "\n",
        "print(f\"\\n--- Creating Knowledge Tensors (Group: {knowledge_group_idx}) ---\")\n",
        "print(f\"    Model Tag: {model_tag}\")\n",
        "print(f\"    Default Precision Tag: {default_precision_tag}\")\n",
        "print(f\"    Quantizing Embed/LMHead to INT8. Transposing Linear Weights.\")\n",
        "\n",
        "# --- Основной цикл конвертации ---\n",
        "total_params = sum(1 for _ in model.named_parameters())\n",
        "print(f\"Found {total_params} parameters to process.\")\n",
        "\n",
        "for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "    loop_start_time = time.time()\n",
        "    print(f\"\\nProcessing Param {idx+1}/{total_params}: {name}\")\n",
        "    print(f\"  Original Shape: {param.shape} | Dtype: {param.dtype}\")\n",
        "\n",
        "    # Инициализация переменных цикла\n",
        "    param_data_fp32: Optional[np.ndarray] = None\n",
        "    knowledge_data_to_pass: Optional[np.ndarray] = None\n",
        "    tags: List[int] = []\n",
        "    metadata_extra_to_pass: Optional[Dict] = None\n",
        "    dtype_to_pass: Any = None\n",
        "    final_tags: List[int] = []\n",
        "    knowledge_coord: Optional[TensorCoordinate] = None\n",
        "    name_id: int = -1\n",
        "    create_result: Optional[List] = None\n",
        "    knowledge_id: Optional[str] = None\n",
        "    requires_transpose: bool = False\n",
        "\n",
        "    try:\n",
        "        # Шаг 1-3: Получение данных, ID, Тегов, Координат\n",
        "        param_data_fp32 = param.data.cpu().to(torch.float32).numpy()\n",
        "        name_id = get_or_create_name_id(name)\n",
        "        tags = [TAG_TYPE_KNOWLEDGE, model_tag]\n",
        "        layer_idx = -1\n",
        "        group_idx = knowledge_group_idx\n",
        "        coord_x = 0\n",
        "        current_nest = 1 # По умолчанию Nest=1 для знаний\n",
        "        is_weight = name.endswith(\".weight\")\n",
        "        is_bias = name.endswith(\".bias\")\n",
        "\n",
        "        if is_weight: tags.append(TAG_COMP_WEIGHTS)\n",
        "        elif is_bias: tags.append(TAG_COMP_BIAS)\n",
        "\n",
        "        # Определение компонента, X координа и флага транспонирования\n",
        "        if \"model.embed_tokens.weight\" in name:\n",
        "             tags.append(TAG_COMP_EMBEDDING); coord_x = 0\n",
        "        elif \"lm_head.weight\" in name:\n",
        "             tags.append(TAG_COMP_LM_HEAD); coord_x = 1; requires_transpose = True\n",
        "        elif \"model.norm.weight\" in name:\n",
        "             layer_idx = model.config.num_hidden_layers; tags.append(TAG_COMP_LAYERNORM); coord_x = 0\n",
        "        elif \".layers.\" in name:\n",
        "            try:\n",
        "                layer_part = name.split('.layers.')[1]\n",
        "                layer_idx = int(layer_part.split('.')[0])\n",
        "                if layer_idx >= 0: tags.append(tag_layer(layer_idx))\n",
        "                else: raise ValueError(f\"Invalid L idx: {layer_idx}\")\n",
        "\n",
        "                component_tag_layer = None\n",
        "                if \"self_attn\" in name:\n",
        "                    if \"q_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 10; requires_transpose = True\n",
        "                    elif \"q_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 11\n",
        "                    elif \"k_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 20; requires_transpose = True\n",
        "                    elif \"k_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 21\n",
        "                    elif \"v_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 30; requires_transpose = True\n",
        "                    elif \"v_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 31\n",
        "                    elif \"o_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_O; coord_x = 40; requires_transpose = True\n",
        "                elif \"mlp\" in name:\n",
        "                    if \"gate_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_GATE; coord_x = 50; requires_transpose = True\n",
        "                    elif \"up_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_UP; coord_x = 60; requires_transpose = True\n",
        "                    elif \"down_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_DOWN; coord_x = 70; requires_transpose = True\n",
        "                elif \"input_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 1\n",
        "                elif \"post_attention_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 2\n",
        "\n",
        "                if component_tag_layer: tags.append(component_tag_layer)\n",
        "                elif not is_weight and not is_bias: print(f\"  WARN: Unrecognized comp in L{layer_idx}: {name}\"); coord_x = 99\n",
        "            except Exception as parse_e:\n",
        "                print(f\"  Error parsing layer for {name}: {parse_e}\"); conversion_errors += 1; continue\n",
        "        else:\n",
        "            print(f\"  WARN: Param unmatched: {name}\"); layer_idx = -1; coord_x = 999\n",
        "\n",
        "        knowledge_coord = TensorCoordinate(layer=layer_idx, group=group_idx, nest=current_nest, x=coord_x)\n",
        "\n",
        "        # Шаг 4: Квантование / Приведение типов / Транспонирование\n",
        "        quantization_scale = None\n",
        "        current_precision_tag = default_precision_tag\n",
        "        data_before_save = None\n",
        "\n",
        "        if name == \"model.embed_tokens.weight\" or name == \"lm_head.weight\":\n",
        "            if np.issubdtype(param_data_fp32.dtype, np.floating):\n",
        "                try:\n",
        "                    abs_max = np.max(np.abs(param_data_fp32)); scale = 1.0\n",
        "                    if abs_max >= 1e-9: scale = abs_max / 127.0\n",
        "                    scale = max(scale, 1e-9) # Prevent division by zero\n",
        "                    quantized_data = np.round(param_data_fp32 / scale).astype(np.int8)\n",
        "                    data_before_save = quantized_data; dtype_to_pass = np.int8\n",
        "                    quantization_scale = float(scale); current_precision_tag = TAG_PREC_INT8\n",
        "                    metadata_extra_to_pass = {\"quantization_scale\": quantization_scale}\n",
        "                    # Транспонируем только LM Head ПОСЛЕ квантования\n",
        "                    if name == \"lm_head.weight\": # requires_transpose is True here\n",
        "                        print(\"  Transposing quantized LM Head weights...\")\n",
        "                        data_before_save = data_before_save.T\n",
        "                except Exception as quant_e:\n",
        "                     print(f\"  ERROR quantizing {name}: {quant_e}\"); conversion_errors += 1; continue\n",
        "            else: # Не float - не квантуем\n",
        "                 data_before_save = param_data_fp32; dtype_to_pass = data_before_save.dtype; current_precision_tag = DTYPE_MAPPING.get(dtype_to_pass, default_precision_tag); metadata_extra_to_pass = None\n",
        "                 if requires_transpose: # Все равно транспонируем, если нужно\n",
        "                      print(f\"  Transposing non-quantized {name}...\")\n",
        "                      data_before_save = data_before_save.T\n",
        "        else: # Не embedding и не lm_head\n",
        "            try:\n",
        "                target_np_dtype = default_torch_dtype.numpy_dtype if hasattr(default_torch_dtype, 'numpy_dtype') else np.float16\n",
        "                data_before_save = param_data_fp32.astype(target_np_dtype)\n",
        "                dtype_to_pass = data_before_save.dtype; current_precision_tag = default_precision_tag\n",
        "                metadata_extra_to_pass = None\n",
        "                # Транспонируем если нужно\n",
        "                if requires_transpose:\n",
        "                    print(f\"  Transposing {name} weights...\")\n",
        "                    data_before_save = data_before_save.T\n",
        "            except Exception as cast_e:\n",
        "                 print(f\"  ERROR casting/transposing {name}: {cast_e}\"); conversion_errors += 1; continue\n",
        "\n",
        "        # Финальные данные для сохранения\n",
        "        knowledge_data_to_pass = data_before_save\n",
        "        final_shape_to_save = knowledge_data_to_pass.shape if knowledge_data_to_pass is not None else None\n",
        "\n",
        "        # Шаг 5: Финализация тегов\n",
        "        final_tags = list(tags)\n",
        "        if current_precision_tag != default_precision_tag and default_precision_tag in final_tags:\n",
        "            final_tags.remove(default_precision_tag)\n",
        "        if current_precision_tag:\n",
        "            final_tags.append(current_precision_tag)\n",
        "        final_tags = sorted(list(set(final_tags)))\n",
        "\n",
        "        print(f\"  Final Tags: {final_tags}\"); print(f\"  Coordinate: {knowledge_coord}\")\n",
        "        print(f\"  Data to save: dtype={dtype_to_pass}, shape={final_shape_to_save}\") # Используем final_shape_to_save\n",
        "        if metadata_extra_to_pass: print(f\"  Extra Metadata: {metadata_extra_to_pass}\")\n",
        "\n",
        "        # Шаг 6: Создание Тензора\n",
        "        create_result = vec.create_tensor(\n",
        "             coord=knowledge_coord,\n",
        "             tensor_type=\"knowledge\",\n",
        "             knowledge_data=knowledge_data_to_pass, # Передаем возможно транспонированные данные\n",
        "             tags=final_tags,\n",
        "             dtype=dtype_to_pass,\n",
        "             shape=final_shape_to_save, # Передаем правильную форму\n",
        "             name_id=name_id,\n",
        "             metadata_extra=metadata_extra_to_pass,\n",
        "             status=\"active\"\n",
        "         )\n",
        "\n",
        "        # Шаг 8: Сохранение Тензора\n",
        "        knowledge_id = vec.save_tensor(create_result) # Передаем список\n",
        "\n",
        "        if knowledge_id:\n",
        "            knowledge_map[name] = knowledge_id\n",
        "            param_count += 1\n",
        "        else:\n",
        "            conversion_errors += 1\n",
        "            print(f\"  ERROR saving tensor for {name}\")\n",
        "\n",
        "    except Exception as create_save_e:\n",
        "        print(f\"  ERROR during create/save for {name}: {create_save_e}\")\n",
        "        traceback.print_exc(); conversion_errors += 1\n",
        "    finally:\n",
        "        if param_data_fp32 is not None:\n",
        "            del param_data_fp32 # Освобождаем память\n",
        "        loop_end_time = time.time()\n",
        "        # print(f\"  Param {idx+1} time: {loop_end_time - loop_start_time:.2f}s\") # Сократим лог\n",
        "\n",
        "# --- Конец Цикла ---\n",
        "\n",
        "print(f\"\\n--- Finished saving {param_count} knowledge tensors to {vec.db.db_root_path if vec.db else 'N/A'} ---\")\n",
        "if conversion_errors > 0:\n",
        "    print(f\"!!! WARNING: {conversion_errors} errors occurred during knowledge conversion !!!\")\n",
        "\n",
        "# --- Сохранение Name ID Map ---\n",
        "name_map_file = DB_PATH / f\"{model_NAME}_name_id_map.pkl\"\n",
        "try:\n",
        "    map_data_to_save = {\n",
        "        \"name_to_id\": ORIGINAL_NAME_TO_ID_MAP,\n",
        "        \"id_to_name\": ID_TO_ORIGINAL_NAME_MAP,\n",
        "        \"next_id\": NEXT_NAME_ID\n",
        "    }\n",
        "    with open(name_map_file, 'wb') as f:\n",
        "        pickle.dump(map_data_to_save, f)\n",
        "    print(f\"\\nName <-> ID map saved to {name_map_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving name ID map: {e}\")\n",
        "\n",
        "# --- Сохранение Knowledge Map (для Cell 5.5) ---\n",
        "# Имя файла определяется в Cell 4.5, но мы его здесь переопределим для надежности\n",
        "knowledge_map_filename = f\"{model_NAME}_knowledge_map.pkl\"\n",
        "knowledge_map_filepath = DB_PATH / knowledge_map_filename\n",
        "try:\n",
        "    print(f\"\\n--- Saving Knowledge Map (for Cell 5.5) ---\")\n",
        "    with open(knowledge_map_filepath, 'wb') as f:\n",
        "        pickle.dump(knowledge_map, f)\n",
        "    print(f\"  Knowledge map saved to {knowledge_map_filepath}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving knowledge map: {e}\")\n",
        "    # Важно: если карта не сохранилась, Cell 6 не сможет загрузить ее позже\n",
        "    # Можно добавить обработку этой ошибки, если нужно\n",
        "    conversion_errors += 1 # Считаем это ошибкой конвертации\n",
        "\n",
        "print(f\"\\n'knowledge_map' created with {len(knowledge_map)} entries for Cell 5.5.\")\n",
        "\n",
        "# --- Очистка ---\n",
        "# (Без изменений)\n",
        "if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "     torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\nMemory cleanup attempted.\")\n",
        "print(\"DB connection remains open for Cell 5.5/6.\")\n",
        "\n",
        "# --- Завершение Ячейки 5 ---\n",
        "end_cell5_time = time.time()\n",
        "print(f\"--- Cell 5 Finished in {end_cell5_time - start_cell5_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "id": "bjqLbM14jyYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5.5: Save Intermediate Data for Cell 6 ===\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Running Cell 5.5: Saving Intermediate Data ---\")\n",
        "\n",
        "# --- Проверка наличия необходимых переменных из предыдущих ячеек ---\n",
        "if 'knowledge_map' not in locals() or not isinstance(knowledge_map, dict):\n",
        "    raise NameError(\"Variable 'knowledge_map' not found or invalid. Ensure Cell 5 ran successfully.\")\n",
        "if 'model' not in locals() or model is None:\n",
        "    # Нам нужен как минимум конфиг модели для num_layers\n",
        "    raise NameError(\"Variable 'model' (or model.config) not found. Ensure Cell 4 ran successfully.\")\n",
        "if 'model_NAME' not in locals() or not model_NAME:\n",
        "     raise NameError(\"Variable 'model_NAME' not defined. Check Cell 1.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "     raise NameError(\"Variable 'DB_PATH' not defined or invalid. Check Cell 1.\")\n",
        "\n",
        "# --- Данные для сохранения ---\n",
        "# Сохраняем только конфиг, а не всю модель, для экономии места\n",
        "cell6_input_data = {\n",
        "    'knowledge_map': knowledge_map,\n",
        "    'model_config': model.config, # Сохраняем конфиг\n",
        "    'model_name': model_NAME,\n",
        "    'db_path': str(DB_PATH.resolve()) # Сохраняем путь к БД как строку\n",
        "}\n",
        "\n",
        "# --- Имя файла и сохранение ---\n",
        "intermediate_filename = f\"{model_NAME}_cell6_input_data.pkl\"\n",
        "intermediate_filepath = DB_PATH / intermediate_filename\n",
        "\n",
        "try:\n",
        "    # Убедимся, что директория DB_PATH существует\n",
        "    DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving intermediate data to: {intermediate_filepath}\")\n",
        "    with open(intermediate_filepath, 'wb') as f:\n",
        "        pickle.dump(cell6_input_data, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Intermediate data saved successfully.\")\n",
        "    print(f\"  Knowledge map entries: {len(knowledge_map)}\")\n",
        "    print(f\"  Model Config Type: {type(model.config)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR saving intermediate data: {e} !!!---\")\n",
        "    # Можно добавить raise e, если критично прервать выполнение\n",
        "else:\n",
        "    print(\"--- Cell 5.5 Finished ---\")\n",
        "\n",
        "# --- Очистка памяти от модели (если она больше не нужна до перезапуска) ---\n",
        "# Раскомментируй, если хочешь освободить память после сохранения промежуточных данных\n",
        "# import gc\n",
        "# if 'model' in locals(): del model\n",
        "# if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "# print(\"Cleaned up model from memory (optional).\")"
      ],
      "metadata": {
        "id": "j_G7u5Qyj6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/db/g500"
      ],
      "metadata": {
        "id": "OJnlx5dVWmh-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qooJcIfBiMNj"
      },
      "outputs": [],
      "source": [
        "# === Исправленный Код для Ячейки 6 (Высокоуровневые OP + Эталонные выходы) ===\n",
        "# Создает процессоры Veector с использованием новых OP-кодов\n",
        "# и одновременно сохраняет эталонные выходы HF модели (float32)\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Необходимые библиотеки ---\n",
        "try:\n",
        "    import torch\n",
        "    from torch import nn\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Missing essential libraries (torch, transformers): {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Импорты проекта Veector ---\n",
        "try:\n",
        "    from core import Veector, CORE_VERSION\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING, get_tensor_hash,\n",
        "        TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_ATTENTION,\n",
        "        TAG_FUNC_FFN, TAG_FUNC_LINEAR, TAG_COMP_LAYERNORM, TAG_MODEL_DEEPSEEK,\n",
        "        tag_layer, GROUP_IDX_QWEN_PROCESSOR, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LM_HEAD,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    from operations import OPERATIONS_VERSION\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "\n",
        "    print(f\"Using Core: {CORE_VERSION}, Tensors: {TENSORS_VERSION}, Ops: {OPERATIONS_VERSION}, DB: {VEECTORDB_VERSION}\")\n",
        "    if CORE_VERSION < \"0.7.10\": print(\"WARN: Expected core v0.7.10+ for high-level op registration.\")\n",
        "    if OPERATIONS_VERSION < \"0.8.9\": print(\"WARN: Expected operations v0.8.9+\")\n",
        "    if TENSORS_VERSION < \"0.7.6\": raise ImportError(\"Tensors version too old\")\n",
        "    if VEECTORDB_VERSION < \"0.9.7\": raise ImportError(\"VeectorDB version too old\")\n",
        "\n",
        "    print(\"Veector components imported successfully.\")\n",
        "\n",
        "    # --- Локальное определение OP кодов (включая новые) ---\n",
        "    OP_SUM=[0,0,0]; OP_SUBTRACT=[0,0,1]; OP_ADD=[0,0,2]; OP_MULTIPLY=[0,1,0]\n",
        "    OP_DIVIDE=[0,1,1]; OP_SQRT=[0,2,0]; OP_POWER=[0,2,1]; OP_ABS=[0,3,0]\n",
        "    OP_MOD=[0,5,0]; OP_FLOOR=[0,6,0]; OP_CEIL=[0,6,1]; OP_SIN=[1,0,0]\n",
        "    OP_COS=[1,0,1]; OP_TAN=[1,1,0]; OP_COT=[1,1,1]; OP_ASIN=[1,2,0]\n",
        "    OP_ACOS=[1,2,1]; OP_ATAN=[1,3,0]; OP_GREATER=[2,0,0]; OP_EQUAL=[2,0,1]\n",
        "    OP_AND=[2,1,0]; OP_OR=[2,1,1]; OP_NOT=[2,2,0]; OP_XOR=[2,3,0]\n",
        "    OP_NAND=[2,4,0]; OP_NOR=[2,4,1]; OP_IF=[3,0,0]; OP_LOOP_MULT=[4,0,0]\n",
        "    OP_CHOICE=[7,0,0]; OP_RAND_UNIFORM=[5,1,0]; OP_RAND_NORMAL=[5,1,1]\n",
        "    OP_MEDIAN=[5,2,0]; OP_PRINT=[8,0,0]; OP_IDENTITY=[9,0,0]\n",
        "    OP_TRIGGER_REASON=[10,0,0]; OP_DFS=[15,0,0]; OP_MEAN=[16,0,0]\n",
        "    OP_STDDEV=[16,1,0]; OP_RELU=[18,0,0]; OP_SIGMOID=[18,1,0]\n",
        "    OP_SOFTMAX=[18,2,0]; OP_LEAKY_RELU=[18,3,0]; OP_SILU=[18,4,0]\n",
        "    OP_GELU=[40,5,0]; OP_EXP_SMOOTHING=[19,0,0]; OP_NORMALIZE_01=[20,0,0]\n",
        "    OP_INTERPOLATE=[20,1,0]; OP_LAYER_NORM=[40,1,0]; OP_BATCH_NORM=[40,4,0]\n",
        "    OP_DROPOUT=[40,3,0]; OP_GET_Q_ROT=[40,7,1]; OP_GET_K_ROT=[40,7,2]\n",
        "    OP_MATRIX_MULTIPLY=[30,0,0]; OP_DETERMINANT=[30,1,0]; OP_EIGENVALUES=[30,2,0]\n",
        "    OP_CONVOLUTION=[30,3,0]; OP_TRANSPOSE=[30,4,0]; OP_INVERSE=[30,5,0]\n",
        "    OP_TRACE=[30,6,0]; OP_ATTENTION_MULTIHEAD=[40,2,0]; OP_EMBEDDING_LOOKUP=[40,6,0]\n",
        "    OP_APPLY_ROPE=[40,7,0]; OP_RESHAPE_HEADS=[40,9,0]; OP_REPEAT_KV_HEADS=[40,9,1]\n",
        "    OP_SCALED_DOT_PROD_ATTN=[40,9,2]; OP_MERGE_HEADS=[40,9,3]; OP_ADD_BIAS=[0,0,3]\n",
        "    OP_UPDATE_KV_CACHE = [40, 10, 0]; OP_CREATE_CAUSAL_MASK = [40, 10, 1]\n",
        "    OP_RESIDUAL_ADD=OP_ADD; OP_LINEAR=OP_MATRIX_MULTIPLY; OP_FINAL_NORM=OP_LAYER_NORM\n",
        "    OP_LINEAR_HEAD=OP_LINEAR; OP_QUANTUM_HADAMARD=[50,0,0]; OP_QUANTUM_PAULI_X=[50,0,1]\n",
        "    OP_QUANTUM_CNOT=[50,1,0]; OP_QUANTUM_MEASURE=[50,2,0]; OP_QUANTUM_SUPERPOS=[50,3,0]\n",
        "    OP_QUANTUM_ENTANGLE=[50,4,0]; META_OP_CATEGORY=99; OP_STORE=[99,0,0]\n",
        "    OP_LOAD=[99,0,1]; OP_LOAD_INITIAL_INPUT=[99,0,3]; OP_DEBUG_CONTEXT=[99,1,0]\n",
        "    OP_MAKE_TUPLE = [99, 2, 0]\n",
        "    # Novye vysokourovnevye OP Kody\n",
        "    OP_QWEN2_RMSNORM = [300, 0, 0]\n",
        "    OP_QWEN2_ATTENTION = [300, 1, 0]\n",
        "    OP_QWEN2_MLP = [300, 2, 0]\n",
        "    # --- Konec OP kodov ---\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Failed to import Veector components: {e}\")\n",
        "    raise\n",
        "except Exception as e_other:\n",
        "    print(f\"FATAL ERROR during Veector imports: {e_other}\")\n",
        "    raise\n",
        "\n",
        "# --- Konfiguracija ---\n",
        "DB_PATH = Path(\"./data/db\") # Put' k BD Veector\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # Identifikator ili lokal'nyj put'\n",
        "TOKENIZER_SOURCE = MODEL_SOURCE # Gde iskat' tokenizator\n",
        "REFERENCE_OUTPUT_FILENAME = \"hf_reference_outputs_fp32.pkl\" # Imja fajla dlja etalonnyh vyhodov\n",
        "PROMPT_FOR_REFERENCE = \"Hello, how are you?\" # Standartnyj prompt dlja generacii etalonov\n",
        "\n",
        "# --- Zagruzka vspomogatel'nyh dannyh ---\n",
        "print(\"\\\\n--- Loading Helper Data (Knowledge Map, Name ID Map) ---\")\n",
        "knowledge_map = None\n",
        "name_id_map = None\n",
        "map_model_name = MODEL_SOURCE.split('/')[-1]\n",
        "knowledge_map_filepath = DB_PATH / f\"{map_model_name}_knowledge_map.pkl\"\n",
        "name_map_filepath = DB_PATH / f\"{map_model_name}_name_id_map.pkl\"\n",
        "\n",
        "try:\n",
        "    with open(knowledge_map_filepath, 'rb') as f:\n",
        "        knowledge_map = pickle.load(f)\n",
        "    print(f\"Loaded knowledge map ({len(knowledge_map)} entries) from {knowledge_map_filepath}\")\n",
        "    if name_map_filepath.is_file():\n",
        "        with open(name_map_filepath, 'rb') as f: name_id_map = pickle.load(f) # Zagruzhaem, esli est'\n",
        "        print(f\"Loaded name ID map from {name_map_filepath}\")\n",
        "    else: print(f\"Warning: Name ID map file not found at {name_map_filepath}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"FATAL ERROR: Required map file not found: {e}. Run previous notebook cells.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR loading map files: {e}\")\n",
        "    raise\n",
        "if not knowledge_map: raise ValueError(\"Knowledge map is empty or failed to load.\")\n",
        "\n",
        "def find_knowledge_id(hf_param_name: str) -> Optional[str]:\n",
        "    \"\"\"Ispol'zuet zagruzhennuju kartu znanij.\"\"\"\n",
        "    return knowledge_map.get(hf_param_name)\n",
        "\n",
        "# --- Zagruzka Etalonnoj Modeli (FP32) i Tokenizatora ---\n",
        "print(\"\\\\n--- Loading Reference HF Model (Float32) and Tokenizer ---\")\n",
        "tokenizer = None\n",
        "model_fp32 = None\n",
        "model_config = None\n",
        "num_layers = 0\n",
        "num_attention_heads = 0\n",
        "num_key_value_heads = 0\n",
        "hidden_size = 0\n",
        "head_dim = 0\n",
        "rms_norm_eps = 1e-6 # Default, mozhno poluchit' iz konfiga\n",
        "\n",
        "try:\n",
        "    print(f\"Loading Tokenizer from: {TOKENIZER_SOURCE}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True)\n",
        "    print(f\"Tokenizer class: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "    print(f\"Loading HF Model {MODEL_SOURCE} with float32...\")\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    model_fp32.eval()\n",
        "    print(f\"HF Model loaded to device: {model_fp32.device}\")\n",
        "\n",
        "    model_config = model_fp32.config\n",
        "    num_layers = model_config.num_hidden_layers\n",
        "    num_attention_heads = model_config.num_attention_heads\n",
        "    num_key_value_heads = getattr(model_config, 'num_key_value_heads', num_attention_heads)\n",
        "    hidden_size = model_config.hidden_size\n",
        "    if num_attention_heads > 0: head_dim = hidden_size // num_attention_heads\n",
        "    else: raise ValueError(\"num_attention_heads is zero\")\n",
        "    rms_norm_eps = model_config.rms_norm_eps # Poluchaem epsilon iz konfiga\n",
        "    print(f\"Model Config: L={num_layers}, H={num_attention_heads}, KVH={num_key_value_heads}, HDim={head_dim}, Epsilon={rms_norm_eps}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR loading HF model/tokenizer: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# --- Podgotovka Vhodnyh Dannyh dlja Etalona ---\n",
        "print(\"\\\\n--- Preparing Input IDs for Reference Run ---\")\n",
        "input_ids_torch = None\n",
        "try:\n",
        "    messages = [{\"role\": \"user\", \"content\": PROMPT_FOR_REFERENCE}]\n",
        "    input_ids_np = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"np\")\n",
        "    if input_ids_np.ndim == 1: input_ids_np = np.expand_dims(input_ids_np, axis=0)\n",
        "    input_ids_torch = torch.tensor(input_ids_np).to(model_fp32.device)\n",
        "    print(f\"Reference Input IDs shape: {input_ids_torch.shape}\")\n",
        "    print(f\"Reference Decoded Input: '{tokenizer.decode(input_ids_torch[0].cpu().numpy())}'\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR preparing reference input: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Funkcija-Hook i Registracija ---\n",
        "hf_reference_outputs = {}\n",
        "hook_handles = []\n",
        "def get_hook(name):\n",
        "    def hook_fn(module, input, output):\n",
        "        actual_output = output[0] if isinstance(output, tuple) else output\n",
        "        print(f\"  [HOOK] Captured output for: {name}\")\n",
        "        hf_reference_outputs[name] = actual_output.detach().cpu().numpy().astype(np.float32)\n",
        "    return hook_fn\n",
        "\n",
        "print(\"\\\\n--- Registering Hooks for Reference Run ---\")\n",
        "try:\n",
        "    hook_handles.append(model_fp32.model.embed_tokens.register_forward_hook(get_hook(\"embed_tokens\")))\n",
        "    for i in range(num_layers):\n",
        "        hook_handles.append(model_fp32.model.layers[i].register_forward_hook(get_hook(f\"layer_{i}_output\")))\n",
        "        hook_handles.append(model_fp32.model.layers[i].input_layernorm.register_forward_hook(get_hook(f\"L{i}_input_layernorm\")))\n",
        "        hook_handles.append(model_fp32.model.layers[i].self_attn.register_forward_hook(get_hook(f\"L{i}_self_attn\")))\n",
        "        hook_handles.append(model_fp32.model.layers[i].post_attention_layernorm.register_forward_hook(get_hook(f\"L{i}_post_attn_layernorm\")))\n",
        "        hook_handles.append(model_fp32.model.layers[i].mlp.register_forward_hook(get_hook(f\"L{i}_mlp\")))\n",
        "    hook_handles.append(model_fp32.model.norm.register_forward_hook(get_hook(\"final_norm\")))\n",
        "    hook_handles.append(model_fp32.lm_head.register_forward_hook(get_hook(\"lm_head\")))\n",
        "    print(f\"Registered {len(hook_handles)} hooks.\")\n",
        "except Exception as e: print(f\"FATAL ERROR registering hooks: {e}\"); raise\n",
        "\n",
        "# --- Prjamoj prohod HF modeli dlja sbora etalonnyh znachenij ---\n",
        "print(\"\\\\n--- Running HF Model Forward Pass (Float32) to Capture Reference Outputs ---\")\n",
        "try:\n",
        "    with torch.no_grad(): hf_model_output = model_fp32(input_ids_torch, use_cache=False)\n",
        "    print(\"HF forward pass complete.\")\n",
        "except Exception as e: print(f\"FATAL ERROR during HF forward pass: {e}\"); traceback.print_exc()\n",
        "finally:\n",
        "    for handle in hook_handles: handle.remove()\n",
        "    print(\"Hooks removed.\")\n",
        "    del model_fp32; gc.collect();\n",
        "    if 'torch' in locals() and hasattr(torch, 'cuda'): torch.cuda.empty_cache()\n",
        "    print(\"Cleaned up reference model from memory.\")\n",
        "if not hf_reference_outputs: print(\"FATAL ERROR: No reference outputs were captured. Cannot proceed.\"); exit()\n",
        "\n",
        "# --- Inicializacija Veector dlja sohranenija processorov ---\n",
        "print(\"\\\\n--- Initializing Veector for Saving Processors ---\")\n",
        "vec = None\n",
        "try:\n",
        "    vec = Veector(db_dir=DB_PATH)\n",
        "    print(f\"Veector core v{CORE_VERSION} initialized for saving.\")\n",
        "except Exception as e: print(f\"FATAL: Veector initialization failed: {e}\"); raise\n",
        "\n",
        "# --- Opredelenie i Sohranenie Processorov Veector (s Vysokourovnevymi OP) ---\n",
        "print(\"\\\\n--- Defining and Saving Veector Processor Tensors (using High-Level OPs) ---\")\n",
        "processor_errors = 0\n",
        "processor_map: Dict[str, str] = {}\n",
        "\n",
        "def create_and_save_processor(name: str, coord: TensorCoordinate, tags: List[int], interface: Dict, ops_sequences: Dict):\n",
        "    global processor_errors, processor_map, vec\n",
        "    proc_id = None\n",
        "    try:\n",
        "        print(f\"  Defining Processor: {name} at {coord}\")\n",
        "        tensor_structure = vec.create_tensor(coord=coord, tensor_type=\"processor\", tags=tags, interface=interface, ops_sequences=ops_sequences, status=\"active\", name_id=-1)\n",
        "        if not validate_tensor(tensor_structure): raise ValueError(f\"Invalid list structure created for {name}\")\n",
        "        proc_id = vec.save_tensor(tensor_structure)\n",
        "        if proc_id:\n",
        "            map_key = \"\"\n",
        "            if \"Embedding\" in name: map_key = \"embedding\"\n",
        "            elif \"Final Norm\" in name: map_key = \"final_norm\"\n",
        "            elif \"LM Head\" in name: map_key = \"lm_head\"\n",
        "            elif \"Attention Processor L\" in name:\n",
        "              try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"attn_{layer_idx}\"\n",
        "              except: pass\n",
        "            elif \"FFN Processor L\" in name:\n",
        "              try: layer_idx = int(name.split(\"L\")[-1]); map_key = f\"ffn_{layer_idx}\"\n",
        "              except: pass\n",
        "            if map_key: processor_map[map_key] = proc_id; print(f\"    SUCCESS: Saved {name} with ID: {proc_id} (Key: {map_key})\")\n",
        "            else: print(f\"    WARN: Saved {name} with ID: {proc_id}, but could not determine map key.\")\n",
        "        else: processor_errors += 1; print(f\"    ERROR saving {name}\")\n",
        "    except Exception as e: print(f\"    ERROR during definition/saving of {name}: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "    return proc_id\n",
        "\n",
        "# --- Parametry dlja processorov ---\n",
        "processor_group_idx = GROUP_IDX_QWEN_PROCESSOR # 500\n",
        "model_tag = TAG_MODEL_DEEPSEEK # 12\n",
        "prec_tag_weights = TAG_PREC_FLOAT16\n",
        "\n",
        "# --- 1. Embedding Processor (Ostavljaem nizkourovnevyj) ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=0)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, model_tag]\n",
        "    param_name = \"embedding_matrix\"\n",
        "    kn_tags = [TAG_COMP_EMBEDDING, model_tag, TAG_COMP_WEIGHTS, TAG_PREC_INT8]\n",
        "    kid = find_knowledge_id(\"model.embed_tokens.weight\")\n",
        "    if not kid: raise ValueError(\"Embedding knowledge tensor ID not found in map.\")\n",
        "    interface = { \"inputs\": [{\"name\":\"token_ids\", \"dtype\":\"int64\"}], \"outputs\": [{\"name\":\"hidden_states\", \"dtype\":\"float16\"}], \"knowledge_needed\": [{\"param_name\": param_name, \"tags\": kn_tags, \"knowledge_id\": kid}] }\n",
        "    ops_sequences = {'default': [[OP_EMBEDDING_LOOKUP, {\"embedding_matrix\": param_name}]]}\n",
        "    create_and_save_processor(\"Embedding Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining Embedding Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "# --- 2. Sloi Transformera (Ispol'zuem novye OP) ---\n",
        "print(f\"\\\\n--- Defining Transformer Layer Processors (0 to {num_layers-1}) using High-Level OPs ---\")\n",
        "# >>> IZMENENO: Udalena oshibochnaja stroka 'current_input = last_hidden_state' <<<\n",
        "for layer_idx in range(num_layers):\n",
        "    layer_tag = tag_layer(layer_idx)\n",
        "    print(f\"  Processing Layer {layer_idx}...\")\n",
        "\n",
        "    # --- 2.A Attention Processor ---\n",
        "    try:\n",
        "        coord_attn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=0)\n",
        "        tags_attn = [TAG_TYPE_PROCESSOR, TAG_FUNC_ATTENTION, layer_tag, model_tag]\n",
        "        kn_defs_attn = [\n",
        "            {\"p\":\"norm_weight_input\", \"t\":[TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.input_layernorm.weight\"},\n",
        "            {\"p\":\"q_weights\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"},\n",
        "            {\"p\":\"q_bias\",      \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.bias\", \"opt\": True},\n",
        "            {\"p\":\"k_weights\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"},\n",
        "            {\"p\":\"k_bias\",      \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.bias\", \"opt\": True},\n",
        "            {\"p\":\"v_weights\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"},\n",
        "            {\"p\":\"v_bias\",      \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.bias\", \"opt\": True},\n",
        "            {\"p\":\"o_weights\",   \"t\":[TAG_COMP_ATTN_O, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"},\n",
        "        ]\n",
        "        knowledge_needs_attn = []\n",
        "        missing_essential = False\n",
        "        for kdef in kn_defs_attn:\n",
        "            kid = find_knowledge_id(kdef[\"f\"])\n",
        "            is_opt = kdef.get(\"opt\", False)\n",
        "            if kid: knowledge_needs_attn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "            elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for Attn L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "        if not missing_essential:\n",
        "            interface_attn = {\n",
        "                \"inputs\": [ {\"name\": \"hidden_state_in\"}, {\"name\": \"residual_input\"}, {\"name\": \"position_ids\"}, {\"name\": \"past_key\", \"optional\": True}, {\"name\": \"past_value\", \"optional\": True}, {\"name\": \"start_pos\", \"dtype\": \"int\", \"optional\": True}, {\"name\": \"total_seq_len\", \"dtype\": \"int\", \"optional\": True} ],\n",
        "                \"outputs\": [{\"name\": \"attn_block_output\"}], # Vyhod Attn + Pervyj Residual\n",
        "                \"knowledge_needed\": knowledge_needs_attn\n",
        "            }\n",
        "            ops_sequences_attn = {'default': [\n",
        "                [OP_STORE, 'residual_attn'],\n",
        "                [OP_QWEN2_RMSNORM, {\"norm_weight\": \"norm_weight_input\", \"eps\": rms_norm_eps}],\n",
        "                # Predpolagaem, chto OP_QWEN2_ATTENTION prinimaet normirovannyj vhod i vozvrashhaet vyhod O-projekcii\n",
        "                # Obnovlenie kesha proishodit vnutri ili cherez step_context (nuzhno utochnit' pri realizacii operacii)\n",
        "                [OP_QWEN2_ATTENTION, {\n",
        "                    \"q_weights\": \"q_weights\", \"k_weights\": \"k_weights\", \"v_weights\": \"v_weights\", \"o_weights\": \"o_weights\",\n",
        "                    \"q_bias\": \"q_bias\", \"k_bias\": \"k_bias\", \"v_bias\": \"v_bias\",\n",
        "                    \"position_ids\": \"position_ids\", \"past_key\": \"past_key\", \"past_value\": \"past_value\",\n",
        "                    \"start_pos\": \"start_pos\", \"total_seq_len\": \"total_seq_len\",\n",
        "                    \"num_heads\": num_attention_heads, \"num_kv_heads\": num_key_value_heads, \"head_dim\": head_dim,\n",
        "                    \"layer_idx\": layer_idx\n",
        "                }],\n",
        "                [OP_ADD, {\"input_a\": \"residual_attn\", \"input_b\": \"_\"}] # Pervyj Residual Add\n",
        "            ]}\n",
        "            create_and_save_processor(f\"Attention Processor L{layer_idx}\", coord_attn, tags_attn, interface_attn, ops_sequences_attn)\n",
        "        else: processor_errors += 1\n",
        "    except Exception as e: print(f\"Error defining Attn L{layer_idx}: {e}\"); processor_errors += 1\n",
        "\n",
        "    # --- 2.B FFN Processor ---\n",
        "    try:\n",
        "        coord_ffn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=1)\n",
        "        tags_ffn = [TAG_TYPE_PROCESSOR, TAG_FUNC_FFN, layer_tag, model_tag]\n",
        "        kn_defs_ffn = [\n",
        "            {\"p\": \"norm_weight_post_attn\", \"t\": [TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\": f\"model.layers.{layer_idx}.post_attention_layernorm.weight\"},\n",
        "            {\"p\": \"gate_weights\", \"t\": [TAG_COMP_FFN_GATE, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.gate_proj.weight\"},\n",
        "            {\"p\": \"up_weights\",   \"t\": [TAG_COMP_FFN_UP, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],    \"f\": f\"model.layers.{layer_idx}.mlp.up_proj.weight\"},\n",
        "            {\"p\": \"down_weights\", \"t\": [TAG_COMP_FFN_DOWN, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.down_proj.weight\"},\n",
        "        ]\n",
        "        knowledge_needs_ffn = []\n",
        "        missing_essential = False\n",
        "        for kdef in kn_defs_ffn:\n",
        "            kid = find_knowledge_id(kdef[\"f\"])\n",
        "            is_opt = kdef.get(\"opt\", False)\n",
        "            if kid: knowledge_needs_ffn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"knowledge_id\": kid, \"optional\": is_opt})\n",
        "            elif not is_opt: missing_essential = True; print(f\"ERROR: Missing essential knowledge for FFN L{layer_idx}: {kdef['p']} ({kdef['f']})\")\n",
        "\n",
        "        if not missing_essential:\n",
        "            interface_ffn = {\n",
        "                \"inputs\": [{\"name\":\"attn_block_output\"}, {\"name\":\"residual_input\"}], # residual_input - eto vyhod Attn bloka dlja vtorogo residual\n",
        "                \"outputs\": [{\"name\":\"layer_output\"}],\n",
        "                \"knowledge_needed\": knowledge_needs_ffn\n",
        "            }\n",
        "            ops_sequences_ffn = {'default': [\n",
        "                [OP_STORE, 'residual_ffn'], # Sohranjaem vyhod Attn bloka dlja vtorogo residual\n",
        "                [OP_QWEN2_RMSNORM, {\"norm_weight\": \"norm_weight_post_attn\", \"eps\": rms_norm_eps}],\n",
        "                # Predpolagaem, chto OP_QWEN2_MLP prinimaet normirovannyj vhod i vozvrashhaet vyhod Down projekcii\n",
        "                [OP_QWEN2_MLP, {\n",
        "                    \"gate_weights\": \"gate_weights\", \"up_weights\": \"up_weights\", \"down_weights\": \"down_weights\"\n",
        "                }],\n",
        "                [OP_ADD, {\"input_a\": \"residual_ffn\", \"input_b\": \"_\"}] # Vtoroj Residual Add\n",
        "            ]}\n",
        "            create_and_save_processor(f\"FFN Processor L{layer_idx}\", coord_ffn, tags_ffn, interface_ffn, ops_sequences_ffn)\n",
        "        else: processor_errors += 1\n",
        "    except Exception as e: print(f\"Error defining FFN L{layer_idx}: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- 3. Final Norm Processor ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=1)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_COMP_LAYERNORM, model_tag]\n",
        "    kn_tags = [TAG_COMP_LAYERNORM, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights]\n",
        "    pattern = \"model.norm.weight\"\n",
        "    kid = find_knowledge_id(pattern)\n",
        "    if not kid: raise ValueError(\"Final Norm knowledge tensor ID not found in map.\")\n",
        "    knowledge_needs = [{\"param_name\": \"norm_weight\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "    interface = {\"inputs\": [{\"name\":\"final_hidden_state\"}], \"outputs\": [{\"name\":\"final_normed_state\"}], \"knowledge_needed\": knowledge_needs}\n",
        "    ops_sequences = {'default': [[OP_QWEN2_RMSNORM, {\"norm_weight\": \"norm_weight\", \"eps\": rms_norm_eps}]]}\n",
        "    create_and_save_processor(\"Final Norm Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining Final Norm Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- 4. LM Head Processor (Ostavljaem nizkourovnevyj) ---\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=2)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_LINEAR, model_tag]\n",
        "    kn_tags = [TAG_COMP_LM_HEAD, model_tag, TAG_COMP_WEIGHTS, TAG_PREC_INT8]\n",
        "    pattern = \"lm_head.weight\"\n",
        "    kid = find_knowledge_id(pattern)\n",
        "    if not kid: raise ValueError(\"LM Head knowledge tensor ID not found in map.\")\n",
        "    knowledge_needs = [{\"param_name\": \"lm_head_weights\", \"tags\": kn_tags, \"knowledge_id\": kid}]\n",
        "    interface = {\"inputs\": [{\"name\":\"final_normed_state\"}], \"outputs\": [{\"name\":\"logits\"}], \"knowledge_needed\": knowledge_needs}\n",
        "    ops_sequences = {'default': [[OP_LINEAR_HEAD, {\"weights\": \"lm_head_weights\"}]]}\n",
        "    create_and_save_processor(\"LM Head Processor\", coord, tags, interface, ops_sequences)\n",
        "except Exception as e: print(f\"Error defining LM Head Processor: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- Sohranenie Karty Processorov i Etalonnyh Vyhodov ---\n",
        "print(f\"\\\\n--- Finalizing Cell 6 ({processor_errors} errors during processor creation) ---\")\n",
        "\n",
        "processor_map_filepath = DB_PATH / f\"{map_model_name}_proc_map.pkl\"\n",
        "try:\n",
        "    if processor_errors == 0:\n",
        "        expected_proc_count = 3 + 2 * num_layers\n",
        "        if len(processor_map) == expected_proc_count:\n",
        "             with open(processor_map_filepath, 'wb') as f: pickle.dump(processor_map, f)\n",
        "             print(f\"Processor map saved to {processor_map_filepath} ({len(processor_map)} entries)\")\n",
        "        else:\n",
        "             print(f\"WARN: Processor map has incorrect entry count ({len(processor_map)} vs {expected_proc_count}). NOT SAVED.\")\n",
        "             processor_errors += 1\n",
        "    else: print(f\"Processor map NOT saved due to {processor_errors} errors.\")\n",
        "except Exception as e: print(f\"Error saving processor map: {e}\"); processor_errors += 1\n",
        "\n",
        "ref_output_path = Path(REFERENCE_OUTPUT_FILENAME)\n",
        "try:\n",
        "    if hf_reference_outputs:\n",
        "        with open(ref_output_path, 'wb') as f: pickle.dump(hf_reference_outputs, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Reference HF outputs saved to {ref_output_path} ({len(hf_reference_outputs)} entries)\")\n",
        "    else: print(f\"WARN: No reference HF outputs were captured, file '{ref_output_path}' not saved.\"); processor_errors += 1\n",
        "except Exception as e: print(f\"Error saving reference outputs: {e}\"); processor_errors += 1\n",
        "\n",
        "# --- Ochistka ---\n",
        "if 'vec' in locals() and vec and hasattr(vec, 'db') and vec.db:\n",
        "    print(\"\\\\nClosing Veector DB connection...\")\n",
        "    vec.db.close()\n",
        "gc.collect()\n",
        "print(\"\\\\nMemory cleanup attempted.\")\n",
        "\n",
        "if processor_errors == 0: print(f\"\\\\n--- Cell 6 Finished Successfully ---\")\n",
        "else: print(f\"\\\\n--- Cell 6 Finished with {processor_errors} ERRORS ---\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Skript sravnenija Veector(fp16) s sohranennymi vyhodami HF v10 ===\n",
        "# Cel': Sravnit' promezhutochnye vyhody Veector (fp16) s zaranee sohranennymi etalonnymi HF (fp32)\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "# --- Neobhodimye biblioteki ---\n",
        "try:\n",
        "    import torch # Nuzhen dlja tipov dannyh v nekotoryh mestah\n",
        "    from transformers import AutoTokenizer, AutoConfig # Gruzim tol'ko Tokenizer i Config\n",
        "    print(\"Torch and Transformers imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Missing essential libraries (torch, transformers): {e}\")\n",
        "    print(\"Please install them: pip install torch transformers accelerate\")\n",
        "    exit()\n",
        "\n",
        "# --- Importy vashego proekta Veector ---\n",
        "try:\n",
        "    # Ubedites', chto put' k vashim fajlam pravil'nyj\n",
        "    # import sys\n",
        "    # sys.path.append('/content/src') # Primer\n",
        "\n",
        "    from core import Veector, CORE_VERSION\n",
        "    from tensors import TensorCoordinate, TENSORS_VERSION, GROUP_IDX_QWEN_KNOWLEDGE\n",
        "    from operations import OPERATIONS_VERSION\n",
        "    from veectordb import VeectorDB, VEECTORDB_VERSION\n",
        "\n",
        "    print(f\"Using Core: {CORE_VERSION}, Tensors: {TENSORS_VERSION}, Ops: {OPERATIONS_VERSION}, DB: {VEECTORDB_VERSION}\")\n",
        "    if CORE_VERSION < \"0.7.9\": print(\"WARN: Expected core v0.7.9+ for knowledge group fix logging.\")\n",
        "    if OPERATIONS_VERSION < \"0.8.9\": print(\"WARN: Expected operations v0.8.9+ for SDPA stability fix.\")\n",
        "\n",
        "    print(\"Veector components imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Failed to import Veector components: {e}\")\n",
        "    print(\"Ensure core.py, tensors.py, operations.py, veectordb.py are accessible.\")\n",
        "    exit()\n",
        "except Exception as e_other:\n",
        "    print(f\"FATAL ERROR during Veector imports: {e_other}\")\n",
        "    exit()\n",
        "\n",
        "# --- Konfiguracija ---\n",
        "DB_PATH = Path(\"./data/db\") # Put' k vashej BD Veector v Colab\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # Ispol'zuetsja dlja tokenizatora i konfiga\n",
        "TOKENIZER_SOURCE = MODEL_SOURCE\n",
        "REFERENCE_OUTPUT_FILENAME = \"hf_reference_outputs_fp32.pkl\" # Fajl s etalonnymi vyhodami\n",
        "\n",
        "PROMPT = \"Hello, how are you?\" # Tot zhe prompt, chto ispol'zovalsja pri sozdanii etalonov\n",
        "NEST_LEVEL = 1 # Veector budet rabotat' v float16\n",
        "COMPARISON_TOLERANCE_ATOL = 5e-3 # Absoljutnyj dopusk\n",
        "COMPARISON_TOLERANCE_RTOL = 1e-3 # Otnositel'nyj dopusk\n",
        "KNOWLEDGE_GROUP_ID = GROUP_IDX_QWEN_KNOWLEDGE # = 100\n",
        "FALLBACK_MAX_SEQ_LEN = 2048\n",
        "\n",
        "# --- Zagruzka Tokenizatora, Konfiga Modeli, Veector ---\n",
        "print(\"\\\\n--- Loading Tokenizer, Config, and Veector ---\")\n",
        "tokenizer = None\n",
        "vec = None\n",
        "processor_map = None\n",
        "model_config = None\n",
        "num_layers = 0\n",
        "num_kv_heads = 0\n",
        "head_dim = 0\n",
        "max_seq_len = FALLBACK_MAX_SEQ_LEN\n",
        "\n",
        "try:\n",
        "    print(f\"Loading Tokenizer from: {TOKENIZER_SOURCE}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True)\n",
        "    print(f\"Tokenizer class: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "    # Gruzim konfig otdel'no\n",
        "    print(f\"Loading Config from: {MODEL_SOURCE}\")\n",
        "    model_config = AutoConfig.from_pretrained(MODEL_SOURCE, trust_remote_code=True)\n",
        "    num_layers = model_config.num_hidden_layers\n",
        "    num_kv_heads = getattr(model_config, 'num_key_value_heads', model_config.num_attention_heads)\n",
        "    head_dim = model_config.hidden_size // model_config.num_attention_heads\n",
        "    max_seq_len = getattr(model_config, 'max_position_embeddings', FALLBACK_MAX_SEQ_LEN)\n",
        "    print(f\"Config loaded: L={num_layers}, KVH={num_kv_heads}, HDim={head_dim}, MaxSeqLen={max_seq_len}\")\n",
        "\n",
        "    print(f\"Initializing Veector from DB: {DB_PATH.resolve()}\")\n",
        "    vec = Veector(db_dir=DB_PATH)\n",
        "    map_model_name = MODEL_SOURCE.split('/')[-1]\n",
        "    proc_map_file = DB_PATH / f\"{map_model_name}_proc_map.pkl\"\n",
        "    with open(proc_map_file, 'rb') as f:\n",
        "        processor_map = pickle.load(f)\n",
        "    print(f\"Veector initialized, loaded processor map ({len(processor_map)} entries).\")\n",
        "\n",
        "    # --- DIAGNOSTIC CHECK: Try loading Embedding Processor directly ---\n",
        "    try:\n",
        "        embedding_proc_id = processor_map.get(\"embedding\")\n",
        "        if embedding_proc_id:\n",
        "            print(f\"Attempting to load Embedding Processor (ID: {embedding_proc_id}) directly...\")\n",
        "            embedding_structure = vec.load_tensor(embedding_proc_id, load_knowledge=False)\n",
        "            if embedding_structure and vec.validate_tensor(embedding_structure):\n",
        "                print(\"  SUCCESS: Embedding processor structure loaded and validated OK.\")\n",
        "                # Mozhno vyvesti chast' struktury dlja proverki\n",
        "                # from tensors import get_tensor_interface, get_processor_ops_sequences\n",
        "                # print(\"    Interface:\", get_tensor_interface(embedding_structure))\n",
        "                # print(\"    Ops Sequence:\", get_processor_ops_sequences(embedding_structure))\n",
        "            elif embedding_structure:\n",
        "                print(\"  ERROR: Embedding processor structure loaded BUT FAILED validation.\")\n",
        "            else:\n",
        "                print(f\"  ERROR: Failed to load tensor structure for ID: {embedding_proc_id}\")\n",
        "        else:\n",
        "            print(\"  ERROR: 'embedding' key not found in processor_map.\")\n",
        "    except Exception as diag_e:\n",
        "        print(f\"  ERROR during diagnostic check: {diag_e}\")\n",
        "    # --- END DIAGNOSTIC CHECK ---\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during loading: {e}\")\n",
        "    traceback.print_exc()\n",
        "    exit()\n",
        "\n",
        "# --- Zagruzka Etalonnyh Vyhodov ---\n",
        "print(f\"\\\\n--- Loading Reference HF Outputs from {REFERENCE_OUTPUT_FILENAME} ---\")\n",
        "hf_outputs = None\n",
        "try:\n",
        "    ref_output_path = Path(REFERENCE_OUTPUT_FILENAME)\n",
        "    if not ref_output_path.is_file():\n",
        "        raise FileNotFoundError(f\"Reference output file not found: {ref_output_path.resolve()}. Please run the script to generate reference outputs first.\")\n",
        "    with open(ref_output_path, 'rb') as f:\n",
        "        hf_outputs = pickle.load(f)\n",
        "    if not isinstance(hf_outputs, dict):\n",
        "        raise TypeError(\"Loaded reference data is not a dictionary.\")\n",
        "    print(f\"Successfully loaded {len(hf_outputs)} reference outputs.\")\n",
        "    # print(f\"Available keys: {list(hf_outputs.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR loading reference outputs: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Podgotovka vhodnyh dannyh ---\n",
        "print(\"\\\\n--- Preparing Input IDs ---\")\n",
        "prompt_input_ids_np = None\n",
        "input_seq_len = 0\n",
        "try:\n",
        "    messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
        "    print(\"Applying chat template...\")\n",
        "    prompt_input_ids_np = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    if prompt_input_ids_np.ndim == 1:\n",
        "        prompt_input_ids_np = np.expand_dims(prompt_input_ids_np, axis=0)\n",
        "\n",
        "    input_seq_len = prompt_input_ids_np.shape[1]\n",
        "\n",
        "    print(f\"Input IDs shape: {prompt_input_ids_np.shape}\")\n",
        "    print(f\"Input Sequence Length: {input_seq_len}\")\n",
        "    print(f\"Decoded Input: '{tokenizer.decode(prompt_input_ids_np[0])}'\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR preparing input: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Vypolnenie Veector i izvlechenie rezul'tatov ---\n",
        "print(\"\\\\n--- Running Veector Processors (float16) ---\")\n",
        "veector_outputs = {}\n",
        "last_hidden_state = None\n",
        "error_occurred = False\n",
        "\n",
        "batch_size = prompt_input_ids_np.shape[0]\n",
        "cache_dtype = np.float16\n",
        "initial_cache_shape = (batch_size, num_kv_heads, max_seq_len, head_dim)\n",
        "print(f\"Initializing zero KV cache for Veector with shape: {initial_cache_shape}\")\n",
        "initial_past_key = np.zeros(initial_cache_shape, dtype=cache_dtype)\n",
        "initial_past_value = np.zeros(initial_cache_shape, dtype=cache_dtype)\n",
        "current_kv_cache_list = [(initial_past_key.copy(), initial_past_value.copy()) for _ in range(num_layers)]\n",
        "\n",
        "try:\n",
        "    # 1. Embedding\n",
        "    print(\"  Running Veector Embedding...\")\n",
        "    embed_context = {\n",
        "        \"input_data\": prompt_input_ids_np,\n",
        "        \"required_nest\": NEST_LEVEL, # = 1 (float16)\n",
        "        \"target_knowledge_group\": KNOWLEDGE_GROUP_ID\n",
        "    }\n",
        "    embed_result = vec.compute(processor_map[\"embedding\"], context=embed_context)\n",
        "    if embed_result.get(\"status\") == \"completed\":\n",
        "        last_hidden_state = embed_result.get(\"data\")\n",
        "        veector_outputs[\"embed_tokens\"] = last_hidden_state\n",
        "        print(\"    Embedding OK.\")\n",
        "    else:\n",
        "        print(f\"    ERROR in Embedding: {embed_result.get('provenance', {}).get('error')}\")\n",
        "        error_occurred = True\n",
        "\n",
        "    # 2. Sloi (posledovatel'no Attn + FFN)\n",
        "    current_input = last_hidden_state\n",
        "    if not error_occurred:\n",
        "        for i in range(num_layers):\n",
        "            print(f\"  Running Veector Layer {i}...\")\n",
        "            if current_input is None:\n",
        "                 print(f\"    ERROR: Input for Layer {i} is None.\")\n",
        "                 error_occurred = True\n",
        "                 break\n",
        "\n",
        "            attn_proc_id = processor_map[f\"attn_{i}\"]\n",
        "            current_position_ids = np.arange(0, input_seq_len, dtype=np.int64).reshape(1, input_seq_len)\n",
        "            attn_context = {\n",
        "                \"input_data\": current_input,\n",
        "                \"residual_input\": current_input,\n",
        "                \"required_nest\": NEST_LEVEL,\n",
        "                \"target_knowledge_group\": KNOWLEDGE_GROUP_ID,\n",
        "                \"position_ids\": current_position_ids,\n",
        "                \"total_seq_len\": input_seq_len,\n",
        "                \"past_key\": current_kv_cache_list[i][0],\n",
        "                \"past_value\": current_kv_cache_list[i][1],\n",
        "                \"start_pos\": 0\n",
        "            }\n",
        "            attn_result = vec.compute(attn_proc_id, context=attn_context)\n",
        "\n",
        "            if attn_result.get(\"status\") == \"completed\":\n",
        "                attn_hidden_state = attn_result.get(\"data\")\n",
        "                if attn_hidden_state is None:\n",
        "                    print(f\"    ERROR: Attn L{i} returned None in 'data' field.\")\n",
        "                    error_occurred = True; break\n",
        "\n",
        "                result_step_context = attn_result.get(\"step_context\", {})\n",
        "                returned_k = result_step_context.get('k_cache_out')\n",
        "                returned_v = result_step_context.get('v_cache_out')\n",
        "                if returned_k is None or returned_v is None:\n",
        "                     print(f\"    WARN: K/V cache not found in step_context for Attn L{i}.\")\n",
        "\n",
        "                print(f\"    Attn L{i} OK.\")\n",
        "\n",
        "                ffn_proc_id = processor_map[f\"ffn_{i}\"]\n",
        "                ffn_context = {\n",
        "                    \"input_data\": attn_hidden_state,\n",
        "                    \"residual_input\": attn_hidden_state,\n",
        "                    \"required_nest\": NEST_LEVEL,\n",
        "                    \"target_knowledge_group\": KNOWLEDGE_GROUP_ID\n",
        "                }\n",
        "                ffn_result = vec.compute(ffn_proc_id, context=ffn_context)\n",
        "\n",
        "                if ffn_result.get(\"status\") == \"completed\":\n",
        "                    layer_output = ffn_result.get(\"data\")\n",
        "                    # Proverka na None pered sohraneniem\n",
        "                    if layer_output is None:\n",
        "                        print(f\"    ERROR: FFN L{i} returned None in 'data' field.\")\n",
        "                        error_occurred = True; break\n",
        "                    veector_outputs[f\"layer_{i}_output\"] = layer_output\n",
        "                    current_input = layer_output\n",
        "                    print(f\"    FFN L{i} OK.\")\n",
        "                else:\n",
        "                    print(f\"    ERROR in FFN L{i}: {ffn_result.get('provenance', {}).get('error')}\")\n",
        "                    error_occurred = True; break\n",
        "            else:\n",
        "                print(f\"    ERROR in Attn L{i}: {attn_result.get('provenance', {}).get('error')}\")\n",
        "                error_occurred = True; break\n",
        "\n",
        "    # 3. Final Norm\n",
        "    if not error_occurred and current_input is not None:\n",
        "        print(\"  Running Veector Final Norm...\")\n",
        "        norm_context = {\n",
        "            \"input_data\": current_input,\n",
        "            \"required_nest\": NEST_LEVEL,\n",
        "            \"target_knowledge_group\": KNOWLEDGE_GROUP_ID\n",
        "        }\n",
        "        norm_result = vec.compute(processor_map[\"final_norm\"], context=norm_context)\n",
        "        if norm_result.get(\"status\") == \"completed\":\n",
        "            final_norm_output = norm_result.get(\"data\")\n",
        "            if final_norm_output is None:\n",
        "                 print(f\"    ERROR: Final Norm returned None in 'data' field.\")\n",
        "                 error_occurred = True\n",
        "            else:\n",
        "                 veector_outputs[\"final_norm\"] = final_norm_output\n",
        "                 last_hidden_state = final_norm_output\n",
        "                 print(\"    Final Norm OK.\")\n",
        "        else:\n",
        "            print(f\"    ERROR in Final Norm: {norm_result.get('provenance', {}).get('error')}\")\n",
        "            error_occurred = True\n",
        "\n",
        "    # 4. LM Head\n",
        "    if not error_occurred and last_hidden_state is not None:\n",
        "        print(\"  Running Veector LM Head...\")\n",
        "        lm_head_context = {\n",
        "            \"input_data\": last_hidden_state,\n",
        "            \"required_nest\": NEST_LEVEL,\n",
        "            \"target_knowledge_group\": KNOWLEDGE_GROUP_ID\n",
        "        }\n",
        "        lm_head_result = vec.compute(processor_map[\"lm_head\"], context=lm_head_context)\n",
        "        if lm_head_result.get(\"status\") == \"completed\":\n",
        "            lm_head_output = lm_head_result.get(\"data\")\n",
        "            if lm_head_output is None:\n",
        "                 print(f\"    ERROR: LM Head returned None in 'data' field.\")\n",
        "                 error_occurred = True\n",
        "            else:\n",
        "                 veector_outputs[\"lm_head\"] = lm_head_output\n",
        "                 print(\"    LM Head OK.\")\n",
        "        else:\n",
        "            print(f\"    ERROR in LM Head: {lm_head_result.get('provenance', {}).get('error')}\")\n",
        "            error_occurred = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during Veector execution: {e}\")\n",
        "    traceback.print_exc()\n",
        "    error_occurred = True\n",
        "finally:\n",
        "    if vec and hasattr(vec, 'db') and vec.db:\n",
        "        try:\n",
        "            vec.db.close()\n",
        "            print(\"Veector DB connection closed.\")\n",
        "        except Exception as db_close_e:\n",
        "            print(f\"Error closing Veector DB: {db_close_e}\")\n",
        "\n",
        "# --- Sravnenie Rezul'tatov ---\n",
        "print(\"\\\\n--- Comparing Outputs (Veector fp16 vs HF fp32) ---\")\n",
        "first_difference_found = False\n",
        "\n",
        "comparison_keys = [\"embed_tokens\"]\n",
        "for i in range(num_layers):\n",
        "    comparison_keys.append(f\"layer_{i}_output\")\n",
        "comparison_keys.append(\"final_norm\")\n",
        "comparison_keys.append(\"lm_head\")\n",
        "\n",
        "if not error_occurred and hf_outputs:\n",
        "    for key in comparison_keys:\n",
        "        print(f\"Comparing: {key}\")\n",
        "        hf_out = hf_outputs.get(key)\n",
        "        vec_out = veector_outputs.get(key)\n",
        "\n",
        "        if hf_out is None or vec_out is None:\n",
        "            print(f\"  ERROR: Output missing for {key} (HF: {'OK' if hf_out is not None else 'MISSING'}, Veector: {'OK' if vec_out is not None else 'MISSING'})\")\n",
        "            if key not in hf_outputs: print(f\"    Key '{key}' not found in loaded reference file '{REFERENCE_OUTPUT_FILENAME}'.\")\n",
        "            if key not in veector_outputs: print(f\"    Key '{key}' not found in Veector outputs.\")\n",
        "            first_difference_found = True\n",
        "            break\n",
        "\n",
        "        print(f\"  HF Shape (fp32): {hf_out.shape}, dtype: {hf_out.dtype}\")\n",
        "        print(f\"  Veector Shape (fp16): {vec_out.shape}, dtype: {vec_out.dtype}\")\n",
        "\n",
        "        if hf_out.shape != vec_out.shape:\n",
        "            print(f\"  ERROR: Shape mismatch for {key}!\")\n",
        "            first_difference_found = True\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            hf_out_f32 = hf_out # Uzhe float32\n",
        "            vec_out_f32 = vec_out.astype(np.float32) # Privodim Veector k float32\n",
        "\n",
        "            are_close = np.allclose(\n",
        "                hf_out_f32,\n",
        "                vec_out_f32,\n",
        "                atol=COMPARISON_TOLERANCE_ATOL,\n",
        "                rtol=COMPARISON_TOLERANCE_RTOL\n",
        "            )\n",
        "            print(f\"  Result: {'CLOSE' if are_close else '!!! DIFFERENT !!!'}\")\n",
        "\n",
        "            if not are_close:\n",
        "                diff = np.abs(hf_out_f32 - vec_out_f32)\n",
        "                max_diff = np.max(diff)\n",
        "                mean_diff = np.mean(diff)\n",
        "                print(f\"    Max Abs Difference:  {max_diff:.6f}\")\n",
        "                print(f\"    Mean Abs Difference: {mean_diff:.6f}\")\n",
        "                print(f\"    HF Sample (fp32):      {hf_out.flatten()[:5]}\")\n",
        "                print(f\"    Veector Sample (fp16): {vec_out.flatten()[:5]}\")\n",
        "                first_difference_found = True\n",
        "                break\n",
        "\n",
        "        except Exception as cmp_e:\n",
        "            print(f\"  ERROR during comparison for {key}: {cmp_e}\")\n",
        "            first_difference_found = True\n",
        "            break\n",
        "elif not hf_outputs:\n",
        "     print(\"Comparison skipped because reference HF outputs were not loaded.\")\n",
        "else:\n",
        "    print(\"Comparison skipped due to errors during Veector execution.\")\n",
        "\n",
        "if not first_difference_found and not error_occurred and hf_outputs:\n",
        "    print(\"\\\\nSUCCESS: All compared outputs are close!\")\n",
        "elif not error_occurred:\n",
        "    print(\"\\\\nFAILURE: Differences found. Check the output above for the first mismatch.\")\n",
        "\n",
        "print(f\"\\\\n--- Comparison Script Finished ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z-GMdM34yDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архивация и скачивание\n",
        "import shutil\n",
        "shutil.make_archive(\"model_DeepSeek-r1-distill-1.5b\", \"zip\", \"data\")\n",
        "zip_name = \"model_DeepSeek-r1-distill-1.5b.zip\""
      ],
      "metadata": {
        "id": "PQxNzSYRmGM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выгрузка на Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "destination_path = f\"/content/drive/My Drive/models/\"\n",
        "shutil.copy(zip_name, destination_path)\n",
        "print(f\"🟢 [LOG] ✅ Архив загружен на Google Drive: {destination_path}\")"
      ],
      "metadata": {
        "id": "fQ5P4_x-mMcr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}