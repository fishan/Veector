{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeRExI/IZD+SG7IMuhofL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishan/Veector/blob/base/Veector_split_DeepSeek_R1_Distill_Qwen_1_5b_int8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 0: Install Dependencies ===\n",
        "!pip install numpy psutil torch transformers accelerate bitsandbytes ipfshttpclient qiskit qiskit-aer requests huggingface_hub -q\n",
        "print(\"Dependencies installed/checked.\")"
      ],
      "metadata": {
        "id": "aat2VxAOEcHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1958cd7b-a78e-4407-a2af-aff5552cab78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed/checked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Imports (Corrected and Simplified - FINAL) ===\n",
        "\n",
        "# --- Standard Imports ---\n",
        "import numpy as np\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "import random\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "from google.colab import drive, files, userdata # Keep Colab imports\n",
        "from huggingface_hub import login             # Keep HF import\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Keep Transformers imports\n",
        "\n",
        "print(\"Standard/External imports loaded.\")\n",
        "\n",
        "# --- Optional Imports ---\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"Warning: PyTorch not found. GPU features may be limited.\")\n",
        "\n",
        "try:\n",
        "    import ipfshttpclient\n",
        "    IPFS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IPFS_AVAILABLE = False\n",
        "    # print(\"Warning: ipfshttpclient not found. IPFS features disabled.\")\n",
        "\n",
        "try:\n",
        "    from qiskit import QuantumCircuit\n",
        "    from qiskit.providers.aer import Aer\n",
        "    from qiskit import execute\n",
        "    QISKIT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    QISKIT_AVAILABLE = False\n",
        "    # print(\"Warning: Qiskit not found. Quantum operations disabled.\")\n",
        "\n",
        "print(\"Optional imports checked.\")\n",
        "\n",
        "# --- Veector Project Imports (Single Correct Block) ---\n",
        "# Ensure core.py, tensors.py (v0.5.1+), veectordb.py (v0.7.1+),\n",
        "# operations.py, memory.py are uploaded and accessible.\n",
        "PROJECT_IMPORTS_OK = False\n",
        "try:\n",
        "    # Import core classes/functions needed by THIS script (converter/inference)\n",
        "    from core import Veector\n",
        "    from veectordb import VeectorDB # Needed if we re-initialize DB here? Usually not.\n",
        "    from tensors import (\n",
        "        TensorCoordinate, create_tensor, # Needed for creating tensors\n",
        "        # Import ALL necessary TAG and GROUP constants for use in this script\n",
        "        TAG_CAT_TYPE, TAG_CAT_COMPONENT, TAG_CAT_PRECISION, TAG_CAT_MODEL_FAMILY,\n",
        "        TAG_CAT_LAYER_IDX, TAG_CAT_FUNCTION, TAG_CAT_DATA_SEMANTIC, TAG_CAT_USER,\n",
        "        TAG_TYPE_PROCESSOR, TAG_TYPE_KNOWLEDGE, TAG_TYPE_CONVERTER, TAG_TYPE_STATE,\n",
        "        TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_EMBEDDING, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_ATTN_QKV,\n",
        "        TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, TAG_COMP_LAYERNORM,\n",
        "        TAG_COMP_LM_HEAD, TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16,\n",
        "        TAG_PREC_INT8, TAG_PREC_INT4, TAG_MODEL_QWEN2, TAG_MODEL_LLAMA3,\n",
        "        TAG_MODEL_DEEPSEEK, TAG_FUNC_LINEAR, TAG_FUNC_ATTENTION, TAG_FUNC_FFN,\n",
        "        TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_CAST_DTYPE, TAG_FUNC_RESHAPE,\n",
        "        TAG_SEMANTIC_HIDDEN_STATE, TAG_SEMANTIC_LOGITS, TAG_SEMANTIC_TOKEN_IDS,\n",
        "        TAG_SEMANTIC_KV_CACHE, tag_layer,\n",
        "        GROUP_IDX_QWEN_KNOWLEDGE, GROUP_IDX_QWEN_PROCESSOR\n",
        "    )\n",
        "    # Only import from operations/memory if DIRECTLY used in THIS script, otherwise core.py handles it\n",
        "    # from operations import * # Generally not needed here\n",
        "    # from memory import Memory # Generally not needed here\n",
        "\n",
        "    print(\"Veector project components imported successfully for this script.\")\n",
        "    PROJECT_IMPORTS_OK = True\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"---!!! FATAL ERROR (ImportError) !!! ---\")\n",
        "    print(f\"Specific error: {e}\")\n",
        "    print(f\"Could not import required name from core.py or tensors.py.\")\n",
        "    print(f\"Ensure files are UP-TO-DATE (tensors v0.5.1+, core v0.5.2+), CORRECT, and ACCESSIBLE.\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    # Optionally define dummies if needed for notebook structure\n",
        "except Exception as other_e:\n",
        "    print(f\"---!!! FATAL ERROR (Other Exception during Import) !!! ---\")\n",
        "    print(f\"Specific error: {other_e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"Check imported files for syntax errors.\")\n",
        "    print(f\"----------------------------------------------------------\")\n",
        "\n",
        "# Removed the redundant import check block ('Checking imports...')"
      ],
      "metadata": {
        "id": "1wWvJFeLElIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8280f0bf-6c3b-41ca-d942-8815b4196e4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard/External imports loaded.\n",
            "Optional imports checked.\n",
            "  Imported VeectorDB (v0.9.7)\n",
            "  Imported tensors (v0.7.6)\n",
            "  Imported operations (v0.8.3)\n",
            "  Imported Memory (v0.1.0)\n",
            "Core components imported successfully.\n",
            "---!!! FATAL ERROR (ImportError) !!! ---\n",
            "Specific error: cannot import name 'TAG_CAT_TYPE' from 'tensors' (/content/tensors.py)\n",
            "Could not import required name from core.py or tensors.py.\n",
            "Ensure files are UP-TO-DATE (tensors v0.5.1+, core v0.5.2+), CORRECT, and ACCESSIBLE.\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Очистка директории для чистоты эксперимента\n",
        "!rm -rf data/\n",
        "output_dir = \"data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "CKjQiacFcv6A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Аутентификация с Hugging Face\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Добавь HF_TOKEN в секреты Colab!\")\n",
        "login(hf_token)\n",
        "print(\"Аутентификация прошла успешно\")\n",
        "\n",
        "# Подключение Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive подключён\")\n",
        "\n",
        "HF_MODEL_NAME = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# Определяем ОДИН основной путь к БД (например, в data/db/)\n",
        "DB_PATH = Path(\"./data/db/\")\n",
        "DB_PATH.mkdir(parents=True, exist_ok=True) # Создаем data/db, если ее нет\n",
        "print(f\"Using Main Veector DB Path: {DB_PATH.resolve()}\")\n",
        "\n",
        "# Set data type (bfloat16 might not be fully supported everywhere, float16 is safer)\n",
        "TORCH_DTYPE = torch.float16 # Use float16 for wider compatibility\n",
        "\n",
        "print(f\"Model to convert: {HF_MODEL_NAME}\")\n",
        "print(f\"Target Veector DB: {DB_PATH}\")\n",
        "print(f\"Target dtype: {TORCH_DTYPE}\")"
      ],
      "metadata": {
        "id": "8ZysrakRHEV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da34d58a-a6ed-49c0-98bc-412a649d727d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Аутентификация прошла успешно\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive подключён\n",
            "Using Main Veector DB Path: /content/data/db\n",
            "Model to convert: DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Target Veector DB: data/db\n",
            "Target dtype: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Tag Ontology and Mappings Definition (Sync with tensors.py v0.7.0) ===\n",
        "\n",
        "import torch # Ensure torch is imported for dtype checking if needed later\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union # Import typing for hints\n",
        "\n",
        "# --- Version (for tracking changes in this cell) ---\n",
        "CONVERTER_CELL2_VERSION = \"Synced with tensors.py v0.7.0\"\n",
        "print(f\"--- Running Converter Cell 2 v{CONVERTER_CELL2_VERSION} ---\")\n",
        "\n",
        "# --- Type Hint for Metadata Tuple (from tensors.py) ---\n",
        "# Needed if any functions within Colab cells might use this type hint\n",
        "MetadataTuple = Tuple[\n",
        "    List[Union[float, int]],         # [0] data_description\n",
        "    List[int],                       # [1] coord\n",
        "    List[int],                       # [2] shape\n",
        "    List[int],                       # [3] tags\n",
        "    Optional[Dict],                  # [4] ops_sequences\n",
        "    Optional[Dict],                  # [5] interface\n",
        "    Optional[List],                  # [6] filters\n",
        "    Optional[List],                  # [7] exit_gates\n",
        "    List[int],                       # [8] lifecycle\n",
        "    Optional[List[str]]              # [9] parents\n",
        "]\n",
        "\n",
        "# --- Simplified Tag Ontology (Flat Integers with Ranges - from tensors.py v0.7.0) ---\n",
        "# 1-9: Tensor Type\n",
        "TAG_TYPE_PROCESSOR = 1\n",
        "TAG_TYPE_KNOWLEDGE = 2\n",
        "TAG_TYPE_CONVERTER = 3\n",
        "TAG_TYPE_STATE = 4\n",
        "# 10-19: Model Family\n",
        "TAG_MODEL_QWEN2 = 10\n",
        "TAG_MODEL_LLAMA3 = 11\n",
        "TAG_MODEL_DEEPSEEK = 12\n",
        "# 20-29: Precision\n",
        "TAG_PREC_FLOAT32 = 20\n",
        "TAG_PREC_FLOAT16 = 21\n",
        "TAG_PREC_BFLOAT16 = 22\n",
        "TAG_PREC_INT8 = 23\n",
        "TAG_PREC_INT4 = 24\n",
        "# 30-49: Component Type\n",
        "TAG_COMP_WEIGHTS = 30\n",
        "TAG_COMP_BIAS = 31\n",
        "TAG_COMP_EMBEDDING = 32\n",
        "TAG_COMP_ATTN_Q = 33\n",
        "TAG_COMP_ATTN_K = 34\n",
        "TAG_COMP_ATTN_V = 35\n",
        "TAG_COMP_ATTN_O = 36\n",
        "TAG_COMP_ATTN_QKV = 37\n",
        "TAG_COMP_FFN_GATE = 38\n",
        "TAG_COMP_FFN_UP = 39\n",
        "TAG_COMP_FFN_DOWN = 40\n",
        "TAG_COMP_LAYERNORM = 41\n",
        "TAG_COMP_LM_HEAD = 42\n",
        "# 50-59: Function\n",
        "TAG_FUNC_LINEAR = 50\n",
        "TAG_FUNC_ATTENTION = 51\n",
        "TAG_FUNC_FFN = 52\n",
        "TAG_FUNC_EMBED_LOOKUP = 53\n",
        "TAG_FUNC_CAST_DTYPE = 54\n",
        "TAG_FUNC_RESHAPE = 55\n",
        "# 60-69: Data Semantic Type\n",
        "TAG_SEMANTIC_HIDDEN_STATE = 60\n",
        "TAG_SEMANTIC_LOGITS = 61\n",
        "TAG_SEMANTIC_TOKEN_IDS = 62\n",
        "TAG_SEMANTIC_KV_CACHE = 63\n",
        "# 100-999: Layer Index\n",
        "LAYER_IDX_TAG_OFFSET = 100\n",
        "\n",
        "def tag_layer(idx: int) -> int:\n",
        "    \"\"\"Generates a layer tag using an offset.\"\"\"\n",
        "    if not isinstance(idx, int): raise TypeError(f\"Layer index must be an integer, got {type(idx)}\")\n",
        "    if idx < 0: raise ValueError(f\"Invalid layer index for tagging: {idx}. Must be non-negative.\")\n",
        "    return LAYER_IDX_TAG_OFFSET + idx\n",
        "# 1000+: User Defined Tags\n",
        "USER_TAG_OFFSET = 1000\n",
        "# --- End of Tags ---\n",
        "print(\"Simplified tag ontology (flat integers) defined.\")\n",
        "\n",
        "# --- Group ID Constants (from tensors.py v0.7.0) ---\n",
        "GROUP_IDX_QWEN_KNOWLEDGE = 100\n",
        "GROUP_IDX_QWEN_PROCESSOR = 500\n",
        "GROUP_IDX_LLAMA_KNOWLEDGE = 101\n",
        "GROUP_IDX_LLAMA_PROCESSOR = 501\n",
        "GROUP_IDX_DEEPSEEK_KNOWLEDGE = 102 # Added constant\n",
        "# GROUP_IDX_DEEPSEEK_PROCESSOR = 502 # Optional\n",
        "GROUP_IDX_GENERIC_PROCESSOR = 50\n",
        "print(f\"Group Indices defined: QwenK={GROUP_IDX_QWEN_KNOWLEDGE}, QwenP={GROUP_IDX_QWEN_PROCESSOR}, DeepSeekK={GROUP_IDX_DEEPSEEK_KNOWLEDGE}\")\n",
        "\n",
        "\n",
        "# --- Mappings (from tensors.py v0.7.0) ---\n",
        "# 1. DATA_TYPE_MAPPING\n",
        "DATA_TYPE_MAPPING = {\n",
        "    \"knowledge\": 1,\n",
        "    \"processor\": 2,\n",
        "    \"converter\": 3,\n",
        "    \"state\": 4,\n",
        "}\n",
        "REVERSE_DATA_TYPE_MAPPING = {\n",
        "    1: \"knowledge\",\n",
        "    2: \"processor\",\n",
        "    3: \"converter\",\n",
        "    4: \"state\",\n",
        "}\n",
        "print(f\"DATA_TYPE_MAPPING defined: {DATA_TYPE_MAPPING}\")\n",
        "\n",
        "# 2. DTYPE_MAPPING\n",
        "DTYPE_MAPPING = {\n",
        "    # Standard Names\n",
        "    'float32': 1, 'float16': 2, 'bfloat16': 3, 'int8': 4, 'int4': 5,\n",
        "    'int32': 6, 'int64': 7, 'bool': 8, 'complex64': 9, 'complex128': 10,\n",
        "    # Numpy Types\n",
        "    np.float32: 1, np.float16: 2, np.int8: 4, np.int32: 6, np.int64: 7,\n",
        "    np.bool_: 8, np.complex64: 9, np.complex128: 10,\n",
        "    # PyTorch Types (as strings and potentially objects if torch loaded)\n",
        "    'torch.float32': 1, 'torch.float16': 2, 'torch.bfloat16': 3, 'torch.int8': 4,\n",
        "    'torch.int32': 6, 'torch.int64': 7, 'torch.bool': 8,\n",
        "    'torch.complex64': 9, 'torch.complex128': 10,\n",
        "}\n",
        "# Add torch objects if torch is available\n",
        "if 'torch' in globals():\n",
        "    DTYPE_MAPPING[torch.float32] = 1\n",
        "    DTYPE_MAPPING[torch.float16] = 2\n",
        "    DTYPE_MAPPING[torch.bfloat16] = 3\n",
        "    DTYPE_MAPPING[torch.int8] = 4\n",
        "    DTYPE_MAPPING[torch.int32] = 6\n",
        "    DTYPE_MAPPING[torch.int64] = 7\n",
        "    DTYPE_MAPPING[torch.bool] = 8\n",
        "    DTYPE_MAPPING[torch.complex64] = 9\n",
        "    DTYPE_MAPPING[torch.complex128] = 10\n",
        "\n",
        "REVERSE_DTYPE_MAPPING = {\n",
        "    1: 'float32', 2: 'float16', 3: 'bfloat16', 4: 'int8', 5: 'int4',\n",
        "    6: 'int32', 7: 'int64', 8: 'bool', 9: 'complex64', 10: 'complex128',\n",
        "}\n",
        "print(f\"DTYPE_MAPPING defined.\")\n",
        "\n",
        "# 3. STATUS_MAPPING\n",
        "STATUS_MAPPING = {\n",
        "    \"active\": 1,\n",
        "    \"archived\": 0\n",
        "}\n",
        "REVERSE_STATUS_MAPPING = {\n",
        "    1: \"active\",\n",
        "    0: \"archived\"\n",
        "}\n",
        "print(f\"STATUS_MAPPING defined: {STATUS_MAPPING}\")\n",
        "\n",
        "# --- Metadata Encoding Configuration (from tensors.py v0.7.0) ---\n",
        "METADATA_STRUCTURE_VERSION = 1.1\n",
        "print(f\"Metadata Structure Version: {METADATA_STRUCTURE_VERSION}\")\n",
        "\n",
        "print(\"Tag ontology, Group IDs, Mappings, and Config defined for Cell 2.\")"
      ],
      "metadata": {
        "id": "_lp77v6xHH4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c3778c-a47f-4325-b7f1-ced46fc46e24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Converter Cell 2 vSynced with tensors.py v0.7.0 ---\n",
            "Simplified tag ontology (flat integers) defined.\n",
            "Group Indices defined: QwenK=100, QwenP=500, DeepSeekK=102\n",
            "DATA_TYPE_MAPPING defined: {'knowledge': 1, 'processor': 2, 'converter': 3, 'state': 4}\n",
            "DTYPE_MAPPING defined.\n",
            "STATUS_MAPPING defined: {'active': 1, 'archived': 0}\n",
            "Metadata Structure Version: 1.1\n",
            "Tag ontology, Group IDs, Mappings, and Config defined for Cell 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG0B83NH5N7h",
        "outputId": "9050f9dd-cc0f-4cc8-c737-4336dfc41e86"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  67K Apr  4 08:06 core.py\n",
            "-rw-r--r-- 1 root root 2.9K Apr  4 04:43 memory.py\n",
            "-rw-r--r-- 1 root root  22K Apr  4 07:45 operations.py\n",
            "-rw-r--r-- 1 root root  23K Apr  4 07:49 tensors.py\n",
            "-rw-r--r-- 1 root root  39K Apr  4 07:47 veectordb.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Initialize Veector (SINGLE Instance) ===\n",
        "from core import Veector # Импортируем класс Veector из core.py\n",
        "try:\n",
        "    # Используем этот путь при инициализации\n",
        "    vec = Veector(db_dir=DB_PATH, ipfs_enabled=False)\n",
        "    print(f\"Veector core initialized using DB at: {DB_PATH.resolve()}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Veector initialization failed: {e}\")\n",
        "    raise RuntimeError(\"Veector Core failed to initialize\") from e"
      ],
      "metadata": {
        "id": "L5XPciufHLD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ac9d9d-60bb-4c28-ebf8-bf4d0caafd2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Veector Core v0.7.2 ---\n",
            "    Requires: tensors v0.7.6+, veectordb v0.9.7+, operations v0.8.3+\n",
            "    IPFS: False, Address: /ip4/127.0.0.1/tcp/5001\n",
            "--- Initializing VeectorDB v0.9.7 (requires tensors v0.7.6+) ---\n",
            "VeectorDB v0.9.7 initialized at /content/data/db. Index entries: 0.\n",
            "VeectorDB initialized.\n",
            "Cache initialized: Size=1000, Strategy=LRU\n",
            "Initialized 74 core operations.\n",
            "Veector core initialized using DB at: /content/data/db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Load Hugging Face Model ===\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(f\"deepseek-ai/{HF_MODEL_NAME}\", torch_dtype=TORCH_DTYPE, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"deepseek-ai/{HF_MODEL_NAME}\", trust_remote_code=True)\n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"Successfully loaded HF model: {HF_MODEL_NAME}\")\n",
        "    print(f\"Model config: {model.config}\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Failed to load HF model '{HF_MODEL_NAME}': {e}\")\n",
        "    # Stop execution\n",
        "    raise RuntimeError(f\"Hugging Face model loading failed\") from e\n",
        "\n",
        "# Clean up GPU memory if possible after loading\n",
        "if TORCH_AVAILABLE and torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Model loaded and memory potentially cleaned.\")"
      ],
      "metadata": {
        "id": "q_gNOVs4HNfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbfd19f-543f-44cf-8198-9d0082c584e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded HF model: DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Model config: Qwen2Config {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "Model loaded and memory potentially cleaned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test inference"
      ],
      "metadata": {
        "id": "b2UYc_vZNmIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubedis', chto peremennye 'model' i 'tokenizer' zagruzheny iz predydushhih jacheek\n",
        "# model = AutoModelForCausalLM.from_pretrained(...)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(...)\n",
        "if 'model' not in locals() or 'tokenizer' not in locals():\n",
        "    raise NameError(\"Model or tokenizer not loaded\")\n",
        "model.eval() # Perevodim model' v rezhim inference\n",
        "# Esli est' GPU, perenosim model' tuda\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')"
      ],
      "metadata": {
        "id": "ntGTsJJiGILl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hello!\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "# Ispol'zuem standartnyj metod dlja korrektnogo formatirovanija\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True, # Dobavljaet markery nachala otveta assistenta\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\" # Vozvrashhaem PyTorch tenzory\n",
        ")\n",
        "# Perenosim na GPU, esli ispol'zuem\n",
        "if torch.cuda.is_available():\n",
        "    input_ids = input_ids.to('cuda')"
      ],
      "metadata": {
        "id": "OZZcLAtGGOqC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametry generacii (mozhno poigrat'sja)\n",
        "max_new_tokens = 500\n",
        "temperature = 0.6\n",
        "top_p = 0.95\n",
        "eos_token_id = tokenizer.eos_token_id # Berem EOS ID iz tokenizatora\n",
        "\n",
        "print(f\"\\n--- Starting generation with model.generate() ---\")\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Parameters: temp={temperature}, top_p={top_p}, max_new={max_new_tokens}\")\n",
        "\n",
        "# Sam process generacii\n",
        "# model.generate ispol'zuet KV-cache i optimizacii PyTorch\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    do_sample=True, # Vkljuchaem semplirovanie\n",
        "    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_token_id # Vazhno dlja nekotoryh modelej\n",
        "    # eos_token_id=eos_token_id # Mozhno javno ukazat', no obychno on beretsja iz konfiga\n",
        ")\n",
        "print(\"--- Generation finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VBDarXjGSeN",
        "outputId": "9bdca8ec-11a4-487e-c238-b2abdecda303"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting generation with model.generate() ---\n",
            "Prompt: 'Hello!'\n",
            "Parameters: temp=0.6, top_p=0.95, max_new=500\n",
            "--- Generation finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dekodiruem tol'ko sgenerirovannuju chast'\n",
        "# generated_ids[0] soderzhit ID vhodnogo prompta + sgenerirovannye ID\n",
        "output_ids = generated_ids[0][input_ids.shape[1]:]\n",
        "generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Generated Text (Original Model) ---\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFl28VZsGXzn",
        "outputId": "a2b7e262-ab7b-4ef3-f6f4-95ee0f29a361"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Text (Original Model) ---\n",
            "Alright, the user just said \"Hello!\" to me. I should respond in a friendly and approachable way. Maybe I can greet them back and ask how I can assist. Keeping it simple and open-ended should encourage them to share what they need help with.\n",
            "</think>\n",
            "\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting knowledge tensors"
      ],
      "metadata": {
        "id": "QUdNlwi0Nq5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Convert Parameters to Knowledge Tensors (Transposed Weights) ===\n",
        "\n",
        "import gc\n",
        "import pickle\n",
        "import time\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Импорты из проекта (убедись, что версии >= 0.6.12 и >= 0.7.6) ---\n",
        "try:\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor_tuple, validate_tensor, DTYPE_MAPPING,\n",
        "        TAG_TYPE_KNOWLEDGE, TAG_MODEL_DEEPSEEK, TAG_COMP_WEIGHTS, TAG_COMP_BIAS,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_LM_HEAD, TAG_COMP_LAYERNORM, TAG_COMP_ATTN_Q,\n",
        "        TAG_COMP_ATTN_K, TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE,\n",
        "        TAG_COMP_FFN_UP, TAG_COMP_FFN_DOWN, tag_layer, GROUP_IDX_QWEN_KNOWLEDGE,\n",
        "        TAG_PREC_FLOAT32, TAG_PREC_FLOAT16, TAG_PREC_BFLOAT16, TAG_PREC_INT8\n",
        "    )\n",
        "    if TENSORS_VERSION < \"0.7.6\":\n",
        "        raise ImportError(f\"Requires tensors v0.7.6+, found v{TENSORS_VERSION}\")\n",
        "    from core import Veector, CORE_VERSION\n",
        "    if CORE_VERSION < \"0.6.12\":\n",
        "        raise ImportError(f\"Requires core v0.6.12+, found v{CORE_VERSION}\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Import failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Версия Ячейки ---\n",
        "CONVERTER_CELL5_VERSION = \"Hybrid v0.7.6 + Quant + Transpose v2\"\n",
        "# --- Конец Версии ---\n",
        "\n",
        "print(f\"--- Running Converter Cell 5 v{CONVERTER_CELL5_VERSION} ---\")\n",
        "start_cell5_time = time.time()\n",
        "\n",
        "# --- Проверка необходимых переменных ---\n",
        "if 'vec' not in locals() or vec is None:\n",
        "    raise NameError(\"'vec' object not defined.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "    raise NameError(\"DB_PATH not defined or invalid.\")\n",
        "if 'model' not in locals() or model is None:\n",
        "    raise NameError(\"HF 'model' not loaded.\")\n",
        "if 'HF_MODEL_NAME' not in locals() or not HF_MODEL_NAME:\n",
        "    raise NameError(\"HF_MODEL_NAME not defined.\")\n",
        "\n",
        "# --- Переинициализация DB (если необходимо) ---\n",
        "if not hasattr(vec, 'db') or vec.db is None:\n",
        "    try:\n",
        "        print(\"Attempting DB re-init for Cell 5...\")\n",
        "        # Импортируем только если нужно, чтобы избежать ненужных импортов вверху\n",
        "        from veectordb import VeectorDB\n",
        "        vec.db = VeectorDB(db_dir=DB_PATH)\n",
        "        print(\"DB connection re-established.\")\n",
        "    except Exception as db_reinit_e:\n",
        "        raise AttributeError(f\"DB re-init failed: {db_reinit_e}\")\n",
        "else:\n",
        "    print(\"'vec' object found and DB connection seems active.\")\n",
        "\n",
        "# --- Инициализация ---\n",
        "ORIGINAL_NAME_TO_ID_MAP: Dict[str, int] = {}\n",
        "ID_TO_ORIGINAL_NAME_MAP: Dict[int, str] = {}\n",
        "NEXT_NAME_ID: int = 0\n",
        "print(\"Initialized Name <-> ID mapping dictionaries.\")\n",
        "\n",
        "knowledge_map: Dict[str, str] = {} # Карта Имя -> ID Знания\n",
        "param_count: int = 0\n",
        "conversion_errors: int = 0\n",
        "\n",
        "# --- Вспомогательная функция для ID ---\n",
        "def get_or_create_name_id(name: Optional[str]) -> int:\n",
        "    \"\"\"Assigns and returns a unique ID for a parameter name.\"\"\"\n",
        "    global NEXT_NAME_ID, ORIGINAL_NAME_TO_ID_MAP, ID_TO_ORIGINAL_NAME_MAP\n",
        "    if not name:\n",
        "        return -1\n",
        "    if name in ORIGINAL_NAME_TO_ID_MAP:\n",
        "        return ORIGINAL_NAME_TO_ID_MAP[name]\n",
        "    current_id = NEXT_NAME_ID\n",
        "    ORIGINAL_NAME_TO_ID_MAP[name] = current_id\n",
        "    ID_TO_ORIGINAL_NAME_MAP[current_id] = name\n",
        "    NEXT_NAME_ID += 1\n",
        "    return current_id\n",
        "\n",
        "# --- Параметры конвертации ---\n",
        "default_precision_tag = TAG_PREC_FLOAT16\n",
        "default_torch_dtype = torch.float16\n",
        "if 'TORCH_DTYPE' in locals(): # Определено в Cell 1\n",
        "    default_torch_dtype = TORCH_DTYPE\n",
        "    if TORCH_DTYPE == torch.float16: default_precision_tag = TAG_PREC_FLOAT16\n",
        "    elif TORCH_DTYPE == torch.bfloat16: default_precision_tag = TAG_PREC_BFLOAT16\n",
        "    elif TORCH_DTYPE == torch.float32: default_precision_tag = TAG_PREC_FLOAT32\n",
        "    elif TORCH_DTYPE == torch.int8: default_precision_tag = TAG_PREC_INT8\n",
        "\n",
        "knowledge_group_idx = GROUP_IDX_QWEN_KNOWLEDGE # 100\n",
        "model_tag = TAG_MODEL_DEEPSEEK # 12\n",
        "\n",
        "print(f\"\\n--- Creating Knowledge Tensors (Group: {knowledge_group_idx}) ---\")\n",
        "print(f\"    Model Tag: {model_tag}\")\n",
        "print(f\"    Default Precision Tag: {default_precision_tag}\")\n",
        "print(f\"    Quantizing Embed/LMHead to INT8. Transposing Linear Weights.\")\n",
        "\n",
        "# --- Основной цикл конвертации ---\n",
        "total_params = sum(1 for _ in model.named_parameters())\n",
        "print(f\"Found {total_params} parameters to process.\")\n",
        "\n",
        "for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "    loop_start_time = time.time()\n",
        "    print(f\"\\nProcessing Param {idx+1}/{total_params}: {name}\")\n",
        "    print(f\"  Original Shape: {param.shape} | Dtype: {param.dtype}\")\n",
        "\n",
        "    # Инициализация переменных цикла\n",
        "    param_data_fp32: Optional[np.ndarray] = None\n",
        "    knowledge_data_to_pass: Optional[np.ndarray] = None\n",
        "    tags: List[int] = []\n",
        "    metadata_extra_to_pass: Optional[Dict] = None\n",
        "    dtype_to_pass: Any = None\n",
        "    final_tags: List[int] = []\n",
        "    knowledge_coord: Optional[TensorCoordinate] = None\n",
        "    name_id: int = -1\n",
        "    create_result: Optional[List] = None\n",
        "    knowledge_id: Optional[str] = None\n",
        "    requires_transpose: bool = False\n",
        "\n",
        "    try:\n",
        "        # Шаг 1-3: Получение данных, ID, Тегов, Координат\n",
        "        param_data_fp32 = param.data.cpu().to(torch.float32).numpy()\n",
        "        name_id = get_or_create_name_id(name)\n",
        "        tags = [TAG_TYPE_KNOWLEDGE, model_tag]\n",
        "        layer_idx = -1\n",
        "        group_idx = knowledge_group_idx\n",
        "        coord_x = 0\n",
        "        current_nest = 1 # По умолчанию Nest=1 для знаний\n",
        "        is_weight = name.endswith(\".weight\")\n",
        "        is_bias = name.endswith(\".bias\")\n",
        "\n",
        "        if is_weight: tags.append(TAG_COMP_WEIGHTS)\n",
        "        elif is_bias: tags.append(TAG_COMP_BIAS)\n",
        "\n",
        "        # Определение компонента, X координа и флага транспонирования\n",
        "        if \"model.embed_tokens.weight\" in name:\n",
        "             tags.append(TAG_COMP_EMBEDDING); coord_x = 0\n",
        "        elif \"lm_head.weight\" in name:\n",
        "             tags.append(TAG_COMP_LM_HEAD); coord_x = 1; requires_transpose = True\n",
        "        elif \"model.norm.weight\" in name:\n",
        "             layer_idx = model.config.num_hidden_layers; tags.append(TAG_COMP_LAYERNORM); coord_x = 0\n",
        "        elif \".layers.\" in name:\n",
        "            try:\n",
        "                layer_part = name.split('.layers.')[1]\n",
        "                layer_idx = int(layer_part.split('.')[0])\n",
        "                if layer_idx >= 0: tags.append(tag_layer(layer_idx))\n",
        "                else: raise ValueError(f\"Invalid L idx: {layer_idx}\")\n",
        "\n",
        "                component_tag_layer = None\n",
        "                if \"self_attn\" in name:\n",
        "                    if \"q_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 10; requires_transpose = True\n",
        "                    elif \"q_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_Q; coord_x = 11\n",
        "                    elif \"k_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 20; requires_transpose = True\n",
        "                    elif \"k_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_K; coord_x = 21\n",
        "                    elif \"v_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 30; requires_transpose = True\n",
        "                    elif \"v_proj.bias\" in name: component_tag_layer = TAG_COMP_ATTN_V; coord_x = 31\n",
        "                    elif \"o_proj.weight\" in name: component_tag_layer = TAG_COMP_ATTN_O; coord_x = 40; requires_transpose = True\n",
        "                elif \"mlp\" in name:\n",
        "                    if \"gate_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_GATE; coord_x = 50; requires_transpose = True\n",
        "                    elif \"up_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_UP; coord_x = 60; requires_transpose = True\n",
        "                    elif \"down_proj.weight\" in name: component_tag_layer = TAG_COMP_FFN_DOWN; coord_x = 70; requires_transpose = True\n",
        "                elif \"input_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 1\n",
        "                elif \"post_attention_layernorm.weight\" in name: component_tag_layer = TAG_COMP_LAYERNORM; coord_x = 2\n",
        "\n",
        "                if component_tag_layer: tags.append(component_tag_layer)\n",
        "                elif not is_weight and not is_bias: print(f\"  WARN: Unrecognized comp in L{layer_idx}: {name}\"); coord_x = 99\n",
        "            except Exception as parse_e:\n",
        "                print(f\"  Error parsing layer for {name}: {parse_e}\"); conversion_errors += 1; continue\n",
        "        else:\n",
        "            print(f\"  WARN: Param unmatched: {name}\"); layer_idx = -1; coord_x = 999\n",
        "\n",
        "        knowledge_coord = TensorCoordinate(layer=layer_idx, group=group_idx, nest=current_nest, x=coord_x)\n",
        "\n",
        "        # Шаг 4: Квантование / Приведение типов / Транспонирование\n",
        "        quantization_scale = None\n",
        "        current_precision_tag = default_precision_tag\n",
        "        data_before_save = None\n",
        "\n",
        "        if name == \"model.embed_tokens.weight\" or name == \"lm_head.weight\":\n",
        "            if np.issubdtype(param_data_fp32.dtype, np.floating):\n",
        "                try:\n",
        "                    abs_max = np.max(np.abs(param_data_fp32)); scale = 1.0\n",
        "                    if abs_max >= 1e-9: scale = abs_max / 127.0\n",
        "                    scale = max(scale, 1e-9) # Prevent division by zero\n",
        "                    quantized_data = np.round(param_data_fp32 / scale).astype(np.int8)\n",
        "                    data_before_save = quantized_data; dtype_to_pass = np.int8\n",
        "                    quantization_scale = float(scale); current_precision_tag = TAG_PREC_INT8\n",
        "                    metadata_extra_to_pass = {\"quantization_scale\": quantization_scale}\n",
        "                    # Транспонируем только LM Head ПОСЛЕ квантования\n",
        "                    if name == \"lm_head.weight\": # requires_transpose is True here\n",
        "                        print(\"  Transposing quantized LM Head weights...\")\n",
        "                        data_before_save = data_before_save.T\n",
        "                except Exception as quant_e:\n",
        "                     print(f\"  ERROR quantizing {name}: {quant_e}\"); conversion_errors += 1; continue\n",
        "            else: # Не float - не квантуем\n",
        "                 data_before_save = param_data_fp32; dtype_to_pass = data_before_save.dtype; current_precision_tag = DTYPE_MAPPING.get(dtype_to_pass, default_precision_tag); metadata_extra_to_pass = None\n",
        "                 if requires_transpose: # Все равно транспонируем, если нужно\n",
        "                      print(f\"  Transposing non-quantized {name}...\")\n",
        "                      data_before_save = data_before_save.T\n",
        "        else: # Не embedding и не lm_head\n",
        "            try:\n",
        "                target_np_dtype = default_torch_dtype.numpy_dtype if hasattr(default_torch_dtype, 'numpy_dtype') else np.float16\n",
        "                data_before_save = param_data_fp32.astype(target_np_dtype)\n",
        "                dtype_to_pass = data_before_save.dtype; current_precision_tag = default_precision_tag\n",
        "                metadata_extra_to_pass = None\n",
        "                # Транспонируем если нужно\n",
        "                if requires_transpose:\n",
        "                    print(f\"  Transposing {name} weights...\")\n",
        "                    data_before_save = data_before_save.T\n",
        "            except Exception as cast_e:\n",
        "                 print(f\"  ERROR casting/transposing {name}: {cast_e}\"); conversion_errors += 1; continue\n",
        "\n",
        "        # Финальные данные для сохранения\n",
        "        knowledge_data_to_pass = data_before_save\n",
        "        final_shape_to_save = knowledge_data_to_pass.shape if knowledge_data_to_pass is not None else None\n",
        "\n",
        "        # Шаг 5: Финализация тегов\n",
        "        final_tags = list(tags)\n",
        "        if current_precision_tag != default_precision_tag and default_precision_tag in final_tags:\n",
        "            final_tags.remove(default_precision_tag)\n",
        "        if current_precision_tag:\n",
        "            final_tags.append(current_precision_tag)\n",
        "        final_tags = sorted(list(set(final_tags)))\n",
        "\n",
        "        print(f\"  Final Tags: {final_tags}\"); print(f\"  Coordinate: {knowledge_coord}\")\n",
        "        print(f\"  Data to save: dtype={dtype_to_pass}, shape={final_shape_to_save}\") # Используем final_shape_to_save\n",
        "        if metadata_extra_to_pass: print(f\"  Extra Metadata: {metadata_extra_to_pass}\")\n",
        "\n",
        "        # Шаг 6: Создание Тензора\n",
        "        create_result = vec.create_tensor(\n",
        "             coord=knowledge_coord,\n",
        "             tensor_type=\"knowledge\",\n",
        "             knowledge_data=knowledge_data_to_pass, # Передаем возможно транспонированные данные\n",
        "             tags=final_tags,\n",
        "             dtype=dtype_to_pass,\n",
        "             shape=final_shape_to_save, # Передаем правильную форму\n",
        "             name_id=name_id,\n",
        "             metadata_extra=metadata_extra_to_pass,\n",
        "             status=\"active\"\n",
        "         )\n",
        "\n",
        "        # Шаг 8: Сохранение Тензора\n",
        "        knowledge_id = vec.save_tensor(create_result) # Передаем список\n",
        "\n",
        "        if knowledge_id:\n",
        "            knowledge_map[name] = knowledge_id\n",
        "            param_count += 1\n",
        "        else:\n",
        "            conversion_errors += 1\n",
        "            print(f\"  ERROR saving tensor for {name}\")\n",
        "\n",
        "    except Exception as create_save_e:\n",
        "        print(f\"  ERROR during create/save for {name}: {create_save_e}\")\n",
        "        traceback.print_exc(); conversion_errors += 1\n",
        "    finally:\n",
        "        if param_data_fp32 is not None:\n",
        "            del param_data_fp32 # Освобождаем память\n",
        "        loop_end_time = time.time()\n",
        "        # print(f\"  Param {idx+1} time: {loop_end_time - loop_start_time:.2f}s\") # Сократим лог\n",
        "\n",
        "# --- Конец Цикла ---\n",
        "\n",
        "print(f\"\\n--- Finished saving {param_count} knowledge tensors to {vec.db.db_root_path if vec.db else 'N/A'} ---\")\n",
        "if conversion_errors > 0:\n",
        "    print(f\"!!! WARNING: {conversion_errors} errors occurred during knowledge conversion !!!\")\n",
        "\n",
        "# --- Сохранение Name ID Map ---\n",
        "name_map_file = DB_PATH / f\"{HF_MODEL_NAME}_name_id_map.pkl\"\n",
        "try:\n",
        "    map_data_to_save = {\n",
        "        \"name_to_id\": ORIGINAL_NAME_TO_ID_MAP,\n",
        "        \"id_to_name\": ID_TO_ORIGINAL_NAME_MAP,\n",
        "        \"next_id\": NEXT_NAME_ID\n",
        "    }\n",
        "    with open(name_map_file, 'wb') as f:\n",
        "        pickle.dump(map_data_to_save, f)\n",
        "    print(f\"\\nName <-> ID map saved to {name_map_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving name ID map: {e}\")\n",
        "\n",
        "# --- Сохранение Knowledge Map (для Cell 5.5) ---\n",
        "# Имя файла определяется в Cell 4.5, но мы его здесь переопределим для надежности\n",
        "knowledge_map_filename = f\"{HF_MODEL_NAME}_knowledge_map.pkl\"\n",
        "knowledge_map_filepath = DB_PATH / knowledge_map_filename\n",
        "try:\n",
        "    print(f\"\\n--- Saving Knowledge Map (for Cell 5.5) ---\")\n",
        "    with open(knowledge_map_filepath, 'wb') as f:\n",
        "        pickle.dump(knowledge_map, f)\n",
        "    print(f\"  Knowledge map saved to {knowledge_map_filepath}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Error saving knowledge map: {e}\")\n",
        "    # Важно: если карта не сохранилась, Cell 6 не сможет загрузить ее позже\n",
        "    # Можно добавить обработку этой ошибки, если нужно\n",
        "    conversion_errors += 1 # Считаем это ошибкой конвертации\n",
        "\n",
        "print(f\"\\n'knowledge_map' created with {len(knowledge_map)} entries for Cell 5.5.\")\n",
        "\n",
        "# --- Очистка ---\n",
        "# (Без изменений)\n",
        "if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "     torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\nMemory cleanup attempted.\")\n",
        "print(\"DB connection remains open for Cell 5.5/6.\")\n",
        "\n",
        "# --- Завершение Ячейки 5 ---\n",
        "end_cell5_time = time.time()\n",
        "print(f\"--- Cell 5 Finished in {end_cell5_time - start_cell5_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "id": "YIvG3cjvHQdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93a631f-f81d-449e-e734-b431c33804cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Converter Cell 5 vHybrid v0.7.6 + Quant + Transpose v2 ---\n",
            "'vec' object found and DB connection seems active.\n",
            "Initialized Name <-> ID mapping dictionaries.\n",
            "\n",
            "--- Creating Knowledge Tensors (Group: 100) ---\n",
            "    Model Tag: 12\n",
            "    Default Precision Tag: 21\n",
            "    Quantizing Embed/LMHead to INT8. Transposing Linear Weights.\n",
            "Found 339 parameters to process.\n",
            "\n",
            "Processing Param 1/339: model.embed_tokens.weight\n",
            "  Original Shape: torch.Size([151936, 1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 23, 30, 32]\n",
            "  Coordinate: L-1_G100_N1_X0_Y0_Z0\n",
            "  Data to save: dtype=<class 'numpy.int8'>, shape=(151936, 1536)\n",
            "  Extra Metadata: {'quantization_scale': 0.0025990402791649103}\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fb4ef375e208da470d6841a89c441ca29e016873a80c8519660c22cfa227be8d -> Type: knowledge, Status: active, Coords: L-1_G100_N1_X0_Y0_Z0\n",
            "\n",
            "Processing Param 2/339: model.layers.0.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 100]\n",
            "  Coordinate: L0_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3e405f866732b9da27a6bc1c1ff7b0977a66bc51b7e58a476f9f559855c46dc1 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 3/339: model.layers.0.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 100]\n",
            "  Coordinate: L0_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7e35759fe8c401a67215f0914efcf0130a3523aec2106b85ccc604a9204ba17d -> Type: knowledge, Status: active, Coords: L0_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 4/339: model.layers.0.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 100]\n",
            "  Coordinate: L0_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5edca5c7dcc96773e1d3801d80799891cc6a8913aff06dd5b15effc5c3b5f148 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 5/339: model.layers.0.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 100]\n",
            "  Coordinate: L0_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f665d55801290b3994e246a3b7eb684ea39772978e429a63deaca87446a769c7 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 6/339: model.layers.0.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 100]\n",
            "  Coordinate: L0_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2ec166a92d4bc6ca2e846cf6d974217905a3dad140d9797416ff410fc59576b7 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 7/339: model.layers.0.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 100]\n",
            "  Coordinate: L0_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7b164dbf6c9c4a19e943cf32d71ddef66ce228b1a4ad68c8a6c5a59a81293c40 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 8/339: model.layers.0.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 100]\n",
            "  Coordinate: L0_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3cf993f52edc3bddd09e0183bf4de68a3aa403c7b2ff8e288bf305d5d8cbf60c -> Type: knowledge, Status: active, Coords: L0_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 9/339: model.layers.0.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 100]\n",
            "  Coordinate: L0_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 05de8e3da032bf5299ce45c46e57f53f0ddeafb1eae34b15765ecd4878dbbb0b -> Type: knowledge, Status: active, Coords: L0_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 10/339: model.layers.0.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 100]\n",
            "  Coordinate: L0_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 77cf62d00caed9a389e7a74108ea2f4ffb44207143eef727d024c49a22344651 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 11/339: model.layers.0.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.0.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 100]\n",
            "  Coordinate: L0_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 141f6bea8cf5033f148e56b413579102f3880fb56909fcac4ca2c0bdc88358b5 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 12/339: model.layers.0.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 100]\n",
            "  Coordinate: L0_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c553c0b4339a04b5502cdfe4e350523abc6e1b4e5baa9cde57207020587f1c85 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 13/339: model.layers.0.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 100]\n",
            "  Coordinate: L0_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID cd230a37dfd83f0c96cea293f6cd319a8cfe3c96cea595a6d319cd6764f5c106 -> Type: knowledge, Status: active, Coords: L0_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 14/339: model.layers.1.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 101]\n",
            "  Coordinate: L1_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 34cf3cff4f3ed321092bb3bedc1acebbe644268c02658c0bd2ba7a9ae731e2cf -> Type: knowledge, Status: active, Coords: L1_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 15/339: model.layers.1.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 101]\n",
            "  Coordinate: L1_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 721d20194c6975e4fedfaf5c8eec4e2ed9932b3fffe14b48515d6bd368861c5b -> Type: knowledge, Status: active, Coords: L1_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 16/339: model.layers.1.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 101]\n",
            "  Coordinate: L1_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID e1d5a36834ffe48e413fee530ca9a1ea3b5d62c81fc8373e9047ee427712c9c3 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 17/339: model.layers.1.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 101]\n",
            "  Coordinate: L1_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 14539935d170211be75dc0ca279874bba720d7a7b0a1021088f394b06e4d1ced -> Type: knowledge, Status: active, Coords: L1_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 18/339: model.layers.1.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 101]\n",
            "  Coordinate: L1_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID df2c130357b1405feaf0503218343455a71b0456858f11df1902e37aeb384dc6 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 19/339: model.layers.1.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 101]\n",
            "  Coordinate: L1_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d90267334f3da61f0447620058b504bf427c18e607f1bdbbdd4c67b16dea6229 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 20/339: model.layers.1.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 101]\n",
            "  Coordinate: L1_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID dd1d797d4589aa6a95e51c25eb29c2d6520cb4f0ad2579f8c4288e13c7f5a3ee -> Type: knowledge, Status: active, Coords: L1_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 21/339: model.layers.1.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 101]\n",
            "  Coordinate: L1_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 83d7f4960d5e8c3fde5e7ad7d91b9dc45fd2de56b734dc1eaf28128c250e5166 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 22/339: model.layers.1.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 101]\n",
            "  Coordinate: L1_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f8b01033a6913c65e318cc68ba1b56dc5ca796266f52d6a7862c24eb135906c8 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 23/339: model.layers.1.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.1.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 101]\n",
            "  Coordinate: L1_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fbfe8df4037f0ef70faa408cb70ce21430c4763e7f1ded0f32c9a14baa039f18 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 24/339: model.layers.1.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 101]\n",
            "  Coordinate: L1_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bbff97175ca2975716a3f5d50c78a81fba62ba8c5c81cb424128002834c62dfc -> Type: knowledge, Status: active, Coords: L1_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 25/339: model.layers.1.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 101]\n",
            "  Coordinate: L1_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4a02d59fbcb402711b176564d5765eadb110ba373b7b5ae19e71a1582b497251 -> Type: knowledge, Status: active, Coords: L1_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 26/339: model.layers.2.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 102]\n",
            "  Coordinate: L2_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID dd297ed92e75ef4d6bf7439940eded91d88d73ad48c49488d8392c47f4499272 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 27/339: model.layers.2.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 102]\n",
            "  Coordinate: L2_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3a46f30ad386c0929a49bf6a8147a0a1e450b1e69c31225d11743006b9b937a9 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 28/339: model.layers.2.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 102]\n",
            "  Coordinate: L2_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3f578f555468beba1ad600dd4ffef99ec1ca8e041c77c956e80c2b5c9842ffde -> Type: knowledge, Status: active, Coords: L2_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 29/339: model.layers.2.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 102]\n",
            "  Coordinate: L2_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID baaa77d5c1b9b0ec46af45eb9b5f62ed2edaac8512eb88493de189c3fe7440f2 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 30/339: model.layers.2.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 102]\n",
            "  Coordinate: L2_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c1ba15d5fda025b733770e94969165079716910e45f5cb009fce2740e168f4c3 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 31/339: model.layers.2.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 102]\n",
            "  Coordinate: L2_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fa5b22838380f681b0dc172416de9b963ff0e2e6cf618755855219fe058fba5b -> Type: knowledge, Status: active, Coords: L2_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 32/339: model.layers.2.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 102]\n",
            "  Coordinate: L2_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3ff44c996e581c4ecf713c3b2d77be222a5c33817b2a89fdaac795fb7bb0b09a -> Type: knowledge, Status: active, Coords: L2_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 33/339: model.layers.2.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 102]\n",
            "  Coordinate: L2_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f00ed0a8f3b8e0e9d9129255803f39a2e9969baed6ab3f5f974f43f03c1024b7 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 34/339: model.layers.2.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 102]\n",
            "  Coordinate: L2_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9e7789eda53a02bb3886e51ad397c2ad66f9947b8bcd82f5be39ef9969a9279a -> Type: knowledge, Status: active, Coords: L2_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 35/339: model.layers.2.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.2.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 102]\n",
            "  Coordinate: L2_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d92f20872862183ea83689f4b8017951ce88e23808406adf4c409cf1e8289968 -> Type: knowledge, Status: active, Coords: L2_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 36/339: model.layers.2.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 102]\n",
            "  Coordinate: L2_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 541fe4ee89457b22f0f9551b3dbe55b5a7d8f54e13fc64914bf6d0bdf6fa548f -> Type: knowledge, Status: active, Coords: L2_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 37/339: model.layers.2.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 102]\n",
            "  Coordinate: L2_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8a4dc84bbc43002d949c8001f42f519b1a87aea33236a23f1aa6ac6b1440413a -> Type: knowledge, Status: active, Coords: L2_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 38/339: model.layers.3.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 103]\n",
            "  Coordinate: L3_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1488319d58f45262334887b07a18399928de71c9a63d71669f86605c65541f07 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 39/339: model.layers.3.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 103]\n",
            "  Coordinate: L3_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0ed5449a1456992cfd7aa236edcc388179c7be93301d4919ffc5e23b48f90468 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 40/339: model.layers.3.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 103]\n",
            "  Coordinate: L3_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0c4f7f9720bc467a6de8276a35ed795b722aae69cd0909e8682aad672de8cff9 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 41/339: model.layers.3.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 103]\n",
            "  Coordinate: L3_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0cf7ecdd0478bde9af68d55d3a1facd1238a7e274844bddc4585d759e9df470a -> Type: knowledge, Status: active, Coords: L3_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 42/339: model.layers.3.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 103]\n",
            "  Coordinate: L3_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID aa688843d0cd1c38fa43d9c50f4ca70db4dcf33f30cba0a73f5cb895d6f1a13a -> Type: knowledge, Status: active, Coords: L3_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 43/339: model.layers.3.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 103]\n",
            "  Coordinate: L3_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 36744042e889a567a26cc7288cebfe0156d8ff47c1ff4b1754de5e6a43f60401 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 44/339: model.layers.3.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 103]\n",
            "  Coordinate: L3_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2b2df3e8aeff9f78b630a6e50156d85940134e25144dc49ded59746f038ae90c -> Type: knowledge, Status: active, Coords: L3_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 45/339: model.layers.3.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 103]\n",
            "  Coordinate: L3_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6151c9a57b66364e729bca460d9dabb82fc7765ce634f9461ec7dab357ae1a01 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 46/339: model.layers.3.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 103]\n",
            "  Coordinate: L3_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2ce92c695117987a96d0ac64388831089fe787b451e1aa8f7e52aa526feb172d -> Type: knowledge, Status: active, Coords: L3_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 47/339: model.layers.3.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.3.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 103]\n",
            "  Coordinate: L3_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 18654c398db57a6493877ebf29239fe614a86f46bdc456fdb160daaaab233ed1 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 48/339: model.layers.3.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 103]\n",
            "  Coordinate: L3_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bd6cc29ca5f6240f7492b8b65a6855f7fe3b922a7cb24fc70f31c6534039fc5f -> Type: knowledge, Status: active, Coords: L3_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 49/339: model.layers.3.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 103]\n",
            "  Coordinate: L3_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3c1600afe8834edee11e263cbb3f0536d8aa59f2a39e146acdf51f7d426004a8 -> Type: knowledge, Status: active, Coords: L3_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 50/339: model.layers.4.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 104]\n",
            "  Coordinate: L4_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 12a9ab0bf0ed3583d0c0e80d8411a8ba45ee1f6069d18ebb8e46b76dd1ca8703 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 51/339: model.layers.4.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 104]\n",
            "  Coordinate: L4_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 96800ce89cfbe72dc9f2b2e33c4cbdc6aad8b1f0ecb923406ab9320d076ff2df -> Type: knowledge, Status: active, Coords: L4_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 52/339: model.layers.4.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 104]\n",
            "  Coordinate: L4_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f49c0f7d7fa28fb9fd06524172608715b1be4d59e791a3b00e65bafb76bbc602 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 53/339: model.layers.4.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 104]\n",
            "  Coordinate: L4_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 73fd244841ffa514f73461ba767f5ce5b76aaa3cc6ec90b5666b0d3361410534 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 54/339: model.layers.4.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 104]\n",
            "  Coordinate: L4_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID cf7dee640a23c4111cceb4bf2120bf836fb115e7f53cfd995f3db2fa76b19929 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 55/339: model.layers.4.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 104]\n",
            "  Coordinate: L4_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 682430340a40689d74800db472cfe38f09b146da3740766928010c35aae403a5 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 56/339: model.layers.4.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 104]\n",
            "  Coordinate: L4_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 83e6143d5572891a6ef8ce4ab8dbbc8547eeb21cf38ed5436679147205db137a -> Type: knowledge, Status: active, Coords: L4_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 57/339: model.layers.4.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 104]\n",
            "  Coordinate: L4_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID abe6c5e7c71208a1a45ceeeb8bdac8e54f682026b4f5156c2edf9ac4f34b7733 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 58/339: model.layers.4.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 104]\n",
            "  Coordinate: L4_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a3784f8f2d0e7d54ebc4bd9925b9a7b509b6d2fca6ed5e9e93f553d479b4dac3 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 59/339: model.layers.4.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.4.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 104]\n",
            "  Coordinate: L4_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9bd78c166ab12862e8396baa9f3882b68d0f661f54d06b97067413873b68446e -> Type: knowledge, Status: active, Coords: L4_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 60/339: model.layers.4.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 104]\n",
            "  Coordinate: L4_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 74e37154991ec927be6005e475ecfa87ee7db62fbe49d3e3dfea4f1716a6f856 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 61/339: model.layers.4.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 104]\n",
            "  Coordinate: L4_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 12de42abb62097b45ef9d1bc78df702ba0045584b0f83c84adce1191002b87d7 -> Type: knowledge, Status: active, Coords: L4_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 62/339: model.layers.5.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 105]\n",
            "  Coordinate: L5_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b5ee7c83a17ad080131b0b0aba984e0cbd11970d6636d5292e60a59c9112506e -> Type: knowledge, Status: active, Coords: L5_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 63/339: model.layers.5.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 105]\n",
            "  Coordinate: L5_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ac85a262d45b63de0ac426c26c480bbe0f87b52884bb6e5e3fe5e3c3d4f368d3 -> Type: knowledge, Status: active, Coords: L5_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 64/339: model.layers.5.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 105]\n",
            "  Coordinate: L5_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d698c59525010c277ceb09e10190b530c509d5b5c37aaf738d66387c257a1f60 -> Type: knowledge, Status: active, Coords: L5_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 65/339: model.layers.5.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 105]\n",
            "  Coordinate: L5_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 627bea51ff98a7bc5cdd08aafa39589a111ae14267ff61dd473acc330b9df9ed -> Type: knowledge, Status: active, Coords: L5_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 66/339: model.layers.5.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 105]\n",
            "  Coordinate: L5_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c260dd7a862ba3bcdf71c50be7f53cbb82cc816a55ab1a3aefec0dfcf6b203bf -> Type: knowledge, Status: active, Coords: L5_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 67/339: model.layers.5.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 105]\n",
            "  Coordinate: L5_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bb662346abdb0a06382eb4b4330ffa3d7b077281ff538bd957032f07553733d0 -> Type: knowledge, Status: active, Coords: L5_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 68/339: model.layers.5.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 105]\n",
            "  Coordinate: L5_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7dad1f1e749a408713451264ab96429c6ec37afd97a36718affc0e6270fe914e -> Type: knowledge, Status: active, Coords: L5_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 69/339: model.layers.5.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 105]\n",
            "  Coordinate: L5_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c03afdd51c88d84b1e2e01800a6515ba536ddf59194265b3911a3224e21743eb -> Type: knowledge, Status: active, Coords: L5_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 70/339: model.layers.5.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 105]\n",
            "  Coordinate: L5_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 98348979f5f05cd513e6c9a7cd6089a65a87559097b7e5f2608c9a8d1563b4fa -> Type: knowledge, Status: active, Coords: L5_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 71/339: model.layers.5.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.5.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 105]\n",
            "  Coordinate: L5_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8d265afa2cb9c9dcdae90273e883ab4bb76f0624dea3420fc653e0195e202c8e -> Type: knowledge, Status: active, Coords: L5_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 72/339: model.layers.5.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 105]\n",
            "  Coordinate: L5_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2527e9007f7ab369c0b48c22d70cdfc239f0d84eb6276c5980ae2f2267e940a7 -> Type: knowledge, Status: active, Coords: L5_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 73/339: model.layers.5.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 105]\n",
            "  Coordinate: L5_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f8e4dc26078d2b05510dfba16f3ff3d7cecb6f0e38d9f96e96216f63c7214d01 -> Type: knowledge, Status: active, Coords: L5_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 74/339: model.layers.6.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 106]\n",
            "  Coordinate: L6_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 269c58f3ac3174c9841cdffdaf6166450ea127a981388980bc3dae04cde32b89 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 75/339: model.layers.6.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 106]\n",
            "  Coordinate: L6_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b845e3288ea5bf68f9aed73273c7758f5175422c2dd05ebab1f2dff20dafdfa1 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 76/339: model.layers.6.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 106]\n",
            "  Coordinate: L6_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID efd8e433053fd7536d124ead722d99bd6aecef826462d2fbaf2dcbf4218378dc -> Type: knowledge, Status: active, Coords: L6_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 77/339: model.layers.6.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 106]\n",
            "  Coordinate: L6_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9026e70bc3a576bd175f3df88cf6ee27375cb5d31840db2dc85fe57559e3d09a -> Type: knowledge, Status: active, Coords: L6_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 78/339: model.layers.6.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 106]\n",
            "  Coordinate: L6_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c1f57f3de2c80ed3a619937a5d6d8b6ae6b27169ce52d71475a23520a2976589 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 79/339: model.layers.6.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 106]\n",
            "  Coordinate: L6_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 36cfd2bcb073c3b26edf2b280f9b15306414d9375abb5b495d830a0f66823562 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 80/339: model.layers.6.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 106]\n",
            "  Coordinate: L6_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 293d73adf70afff76538bac4cb730e451262c823d4cc2b59548beda2e9937e07 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 81/339: model.layers.6.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 106]\n",
            "  Coordinate: L6_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 92c8ddc834ffb3cec11b3f222977cd7294bfc9b8e5ea3fc7e8ba25e1029f9405 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 82/339: model.layers.6.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 106]\n",
            "  Coordinate: L6_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8830f35fc98e2cc6ed53c792f1316688b0090be59ccbb778d1977ba47e6dd9e6 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 83/339: model.layers.6.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.6.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 106]\n",
            "  Coordinate: L6_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 248c19b80e6d4e6045ab87a071ae48c29fb1c1622aebf33cf1a5d19a7408b4f8 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 84/339: model.layers.6.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 106]\n",
            "  Coordinate: L6_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b2b0e7f4390c964f363d7751347627a68ea05598e17b68c8e5aad7e9d8deea35 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 85/339: model.layers.6.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 106]\n",
            "  Coordinate: L6_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d375fa37c556d0297d897c3987b56453ae7d88f9c65db6696d842402f103d5c7 -> Type: knowledge, Status: active, Coords: L6_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 86/339: model.layers.7.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 107]\n",
            "  Coordinate: L7_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID be67b509ef4b3a193851b20f3d39433be7c8cd56d520d4c92c82b27ae6db0fd1 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 87/339: model.layers.7.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 107]\n",
            "  Coordinate: L7_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f1d751208ebb54cb28661dab6da011a6402e2bb1ad76dd7025589ee64d48e8b8 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 88/339: model.layers.7.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 107]\n",
            "  Coordinate: L7_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 342be9efd5ed341a145815f055df7ee2c190187a104386b315dbf123c96d3222 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 89/339: model.layers.7.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 107]\n",
            "  Coordinate: L7_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bbf47b3c7b752127e31682300067e3abd281253ea685407447028cdbfda7713b -> Type: knowledge, Status: active, Coords: L7_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 90/339: model.layers.7.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 107]\n",
            "  Coordinate: L7_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8dcc4fa0cf3b04cedc77298b262536cfb6c6887aea251cf0b1e3d91f45f17393 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 91/339: model.layers.7.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 107]\n",
            "  Coordinate: L7_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1264c4fc7bf11b71d7d16ab1b15d2b0c0f86f6b61837e4ae7b4e4a3f56c2d607 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 92/339: model.layers.7.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 107]\n",
            "  Coordinate: L7_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 61122861bdbd6b92ff7c8dc18bdc2517b2550dfb61584d4be1b8fb74690ce1e1 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 93/339: model.layers.7.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 107]\n",
            "  Coordinate: L7_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d5392898259ff405c74525801bf4f51bc16e7c1c2b330bc785319b12f446e984 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 94/339: model.layers.7.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 107]\n",
            "  Coordinate: L7_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f416bb15413b389d5a9bd37bf0b8ba4aa556714cd8fa38f9102d0046b34e1c59 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 95/339: model.layers.7.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.7.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 107]\n",
            "  Coordinate: L7_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6c5c29c74e31fed0e3c083c3571c1ce151006e69acb4e2d73e82db8c335ab0e4 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 96/339: model.layers.7.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 107]\n",
            "  Coordinate: L7_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f00e4af7ab9477608a9058a1a0798daf3b2f26cc515b6a034dd8adbdabdc9778 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 97/339: model.layers.7.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 107]\n",
            "  Coordinate: L7_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fcdd503cd24425bed03048a6440f944b71d53eae96c9bb91384f7375deb5a3b2 -> Type: knowledge, Status: active, Coords: L7_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 98/339: model.layers.8.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 108]\n",
            "  Coordinate: L8_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ff06ff7ba01d075ca6181cc1f4cb259ff995cc0e5651572386e402e469ff09ab -> Type: knowledge, Status: active, Coords: L8_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 99/339: model.layers.8.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 108]\n",
            "  Coordinate: L8_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 934c4036aadbe92fd9dc3599a694027c87dec31c234c3e5339f7a4869c27f268 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 100/339: model.layers.8.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 108]\n",
            "  Coordinate: L8_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 132e00468c0d735ef9e3fe8ebc69df00f4f2b01649261f09ca6e94745c04e5ff -> Type: knowledge, Status: active, Coords: L8_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 101/339: model.layers.8.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 108]\n",
            "  Coordinate: L8_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 496bcaffa6e33f603d77a645160f60de6eaedd134cd434ffde3dee786b2866f3 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 102/339: model.layers.8.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 108]\n",
            "  Coordinate: L8_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0d4226fa652ef0087211753f6ef8c03e65bf32665cfd4d3a9a9f5dcb36a77cd0 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 103/339: model.layers.8.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 108]\n",
            "  Coordinate: L8_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 96ed3fdbd95cc0a1b1b061f2830c17cdbe5651c595a5aac9cecf69549dbc62b8 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 104/339: model.layers.8.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 108]\n",
            "  Coordinate: L8_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7780190a851f229031ebe854dda9303cb393a5f80442cef2a0240abc59c2a2f4 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 105/339: model.layers.8.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 108]\n",
            "  Coordinate: L8_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 090643ca8528bf394132a236ce6848b89272d9800acf01763c7f975571b16f9f -> Type: knowledge, Status: active, Coords: L8_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 106/339: model.layers.8.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 108]\n",
            "  Coordinate: L8_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 785761151afa01d935e98ee0181cb1747de2e3ebdd0107984336b75a4060a101 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 107/339: model.layers.8.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.8.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 108]\n",
            "  Coordinate: L8_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4f564a5d3cca31af4138c877aa835f283697035f706351d1aabb8392c2dfbda4 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 108/339: model.layers.8.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 108]\n",
            "  Coordinate: L8_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6a73f8e9e8ba9c2a3507499e0ee11c17d218ef82e9484ea847bc9fb75b05be7e -> Type: knowledge, Status: active, Coords: L8_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 109/339: model.layers.8.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 108]\n",
            "  Coordinate: L8_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2772987ff4679508cce68f6564918b9e8998d406f559c4d2d2bf40aa9d925d25 -> Type: knowledge, Status: active, Coords: L8_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 110/339: model.layers.9.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 109]\n",
            "  Coordinate: L9_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 93294a68d8fc4bb876da133e8aab8f5d0534cf5a9771ce34416e34460dcd39fd -> Type: knowledge, Status: active, Coords: L9_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 111/339: model.layers.9.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 109]\n",
            "  Coordinate: L9_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3f7e4b0f2737b52b0dcad9af4bc6a85981b5d93312d96eec2d531f5b1edf350e -> Type: knowledge, Status: active, Coords: L9_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 112/339: model.layers.9.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 109]\n",
            "  Coordinate: L9_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 671a2e595c77dbf50022185e1848f7e7f84d0a8c6106d16f76cf8a53ec8a156e -> Type: knowledge, Status: active, Coords: L9_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 113/339: model.layers.9.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 109]\n",
            "  Coordinate: L9_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 92cb85724d63ea0d270d5b76d4b4fcac2b2aa20cc3af5c0ad13399a3d993620e -> Type: knowledge, Status: active, Coords: L9_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 114/339: model.layers.9.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 109]\n",
            "  Coordinate: L9_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9b0cca902aac82ed2426738dcb5a0c8c048e94b21b3e28831eb3d36771ad3984 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 115/339: model.layers.9.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 109]\n",
            "  Coordinate: L9_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2c87b01662e8108e2280278ff59556cf4a20fcd33c46230d160fca8aa999fbb7 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 116/339: model.layers.9.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 109]\n",
            "  Coordinate: L9_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8c5fe6171efe1a2d5a970ed40a2507bc0e26eaec540b3372ba93e2526bb1ee22 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 117/339: model.layers.9.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 109]\n",
            "  Coordinate: L9_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3a1c2675029aba21d851707d354c120109674cb743131cae0374d17130be57c9 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 118/339: model.layers.9.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 109]\n",
            "  Coordinate: L9_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0a7484389822251150b23da70bd08602cb41623cd3e17e5e65133be4e1fd8176 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 119/339: model.layers.9.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.9.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 109]\n",
            "  Coordinate: L9_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b54160e6a3e217779eec3963eb3b6c93dbd743828067778d05ba2f7a2e62e41b -> Type: knowledge, Status: active, Coords: L9_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 120/339: model.layers.9.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 109]\n",
            "  Coordinate: L9_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4563f2b79a4f3aed9d6cc5dcc7aea276f667a0ed8b0e2626aec18b4367ccaea8 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 121/339: model.layers.9.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 109]\n",
            "  Coordinate: L9_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 290bd8b3fa76e81c97445ebb229809b7501d812036348a99998539dcd14beaf9 -> Type: knowledge, Status: active, Coords: L9_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 122/339: model.layers.10.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 110]\n",
            "  Coordinate: L10_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8e081ece0eb0f1f6dc7781cf411b6f632f39dd90845ff116829393f4d30edb1d -> Type: knowledge, Status: active, Coords: L10_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 123/339: model.layers.10.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 110]\n",
            "  Coordinate: L10_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d3a90d5c3d43378d58627f399242ed9c76606c32c2f082fb024af111ce34d5f1 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 124/339: model.layers.10.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 110]\n",
            "  Coordinate: L10_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ef6907613fa3b011e8bb9d8aa3aff0acb0b3f1b7839b2b6045ecd73dbb3d6791 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 125/339: model.layers.10.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 110]\n",
            "  Coordinate: L10_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 989ea2508b4f9b59081faaf441d4aab276f8a228bcb98a704eea3d8089feadf2 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 126/339: model.layers.10.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 110]\n",
            "  Coordinate: L10_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 94a395777d7c8e7daf08e94e5244c01b7aa59280163000037ec7d461a87f2b4a -> Type: knowledge, Status: active, Coords: L10_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 127/339: model.layers.10.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 110]\n",
            "  Coordinate: L10_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 606819499e1198af26b6bdcb872f46ba95ee6173c8a153759f2b4de816ae6cea -> Type: knowledge, Status: active, Coords: L10_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 128/339: model.layers.10.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 110]\n",
            "  Coordinate: L10_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2dac7429133b282fad8ee8082edee6ff475876801cd944be7a7adcbd640de619 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 129/339: model.layers.10.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 110]\n",
            "  Coordinate: L10_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 73488167f8798804c8a4eb791aec5969ebf7bb7fe407d3ada68c5b69bcea7290 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 130/339: model.layers.10.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 110]\n",
            "  Coordinate: L10_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0797457a7de2fad94d9669c663acb2a18c1b69184a5bddb44e2adc9295d339ca -> Type: knowledge, Status: active, Coords: L10_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 131/339: model.layers.10.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.10.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 110]\n",
            "  Coordinate: L10_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2f74097806fa730368cf443b90ee484b8d5490549719490abae0f1a370ad7aa8 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 132/339: model.layers.10.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 110]\n",
            "  Coordinate: L10_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6ab0d1a6e09483690b8a7982fb0c4f969928c31055584675efc0778398c24d0d -> Type: knowledge, Status: active, Coords: L10_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 133/339: model.layers.10.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 110]\n",
            "  Coordinate: L10_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID df58149ed980255b07f373a15c633791c371c570662cef7cb515a76ae882f865 -> Type: knowledge, Status: active, Coords: L10_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 134/339: model.layers.11.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 111]\n",
            "  Coordinate: L11_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d4aa57a188935798050c4bba2bae54a1b3664067bec3f523b179d2114ac1c242 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 135/339: model.layers.11.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 111]\n",
            "  Coordinate: L11_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 704fb1a4320fdcad8fe6243f43e223821d772c093ea622b7c8ae4dfdf8d78410 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 136/339: model.layers.11.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 111]\n",
            "  Coordinate: L11_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7c9f91f2fcd620d6f4c6ebc341cda7c2d3d4900c636ac9133017844ea991c4c5 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 137/339: model.layers.11.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 111]\n",
            "  Coordinate: L11_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2b223655f021152d6048967b8e0ae1f4be78cfe01d242dc4129d26c0084b6fc9 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 138/339: model.layers.11.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 111]\n",
            "  Coordinate: L11_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b48df84293d483bad92dace86eb16de5bb7d48dc87b1a153800d56bcbc64c32e -> Type: knowledge, Status: active, Coords: L11_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 139/339: model.layers.11.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 111]\n",
            "  Coordinate: L11_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 687449da42d630315b7cbaf4dfacc92f0e9fc8df8d27d974cd601eadfeac0418 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 140/339: model.layers.11.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 111]\n",
            "  Coordinate: L11_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5b4ff0bacec1a4149c2e3931ada104a340db12d0493cd9188a8610fbf65a054e -> Type: knowledge, Status: active, Coords: L11_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 141/339: model.layers.11.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 111]\n",
            "  Coordinate: L11_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 74e796990c1e1a6d83860c946380d452b184f2b7182f518696cb63bdb2c096f3 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 142/339: model.layers.11.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 111]\n",
            "  Coordinate: L11_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bd9166882a86613985d879c55f58138a00a99bfa223781ea9d74481696b74c04 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 143/339: model.layers.11.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.11.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 111]\n",
            "  Coordinate: L11_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4a6630447314f63970b586e8f399738fcb4d0add53c60a375067b2fb0f0ac317 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 144/339: model.layers.11.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 111]\n",
            "  Coordinate: L11_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 15442ced4a4d99ff65790eef1843b51526e823ca3ddc5ba34369b2dbb6fbf4f2 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 145/339: model.layers.11.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 111]\n",
            "  Coordinate: L11_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7d0bd1480b3b059a98e342dd8a18328b339f6f1d7a25b79a83508758e8c774b6 -> Type: knowledge, Status: active, Coords: L11_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 146/339: model.layers.12.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 112]\n",
            "  Coordinate: L12_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c95e24dcc370da7f1ebcb669b0ad76c35a27886bd672b23911894183c15470fb -> Type: knowledge, Status: active, Coords: L12_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 147/339: model.layers.12.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 112]\n",
            "  Coordinate: L12_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4d66a6ab368339ad741dea4e2ee7abb35933833b6db7bc2eef569a38f7793ab6 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 148/339: model.layers.12.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 112]\n",
            "  Coordinate: L12_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 71135c29a4797024d27acc772337cb0babc8fba0404b21ea9c53552feb30b325 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 149/339: model.layers.12.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 112]\n",
            "  Coordinate: L12_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3170263f0621f840ac278d2e43648425ab611dcc83df5493e3b106d3beb8eff6 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 150/339: model.layers.12.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 112]\n",
            "  Coordinate: L12_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5c85186557ee9579befd743c7186b206347d632ac1ff2dc5546e73d2a1497b11 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 151/339: model.layers.12.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 112]\n",
            "  Coordinate: L12_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0346867335b677a8dbad5c5b879da6847f9383b037ac28c71cc9bb87be656bd2 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 152/339: model.layers.12.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 112]\n",
            "  Coordinate: L12_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ad1f79be5a2051eee525a8868e44762ce36a10055631a770ce5e574b3c5bc670 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 153/339: model.layers.12.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 112]\n",
            "  Coordinate: L12_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8be270ec133822c63f52af4220b0d25b6788da564960101ebc10854c34911848 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 154/339: model.layers.12.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 112]\n",
            "  Coordinate: L12_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0e5585c5a637a0514143d15b696d24e3bd304d8be70d71b3317ececa2588cc94 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 155/339: model.layers.12.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.12.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 112]\n",
            "  Coordinate: L12_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4a65ac8cad0a74377a05f9705fb69cdbbb58a315b62f0baa465264a6e5c55f38 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 156/339: model.layers.12.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 112]\n",
            "  Coordinate: L12_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 32e8699c039907d1b87936154e2ae8707c0ff184c7cbee3a8791a14c75531499 -> Type: knowledge, Status: active, Coords: L12_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 157/339: model.layers.12.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 112]\n",
            "  Coordinate: L12_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 77069f179b859ea8c32e2ac1e2ec1a84a8ca0d342be775df38841250fc44bcfe -> Type: knowledge, Status: active, Coords: L12_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 158/339: model.layers.13.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 113]\n",
            "  Coordinate: L13_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3b67552fee90740d0266299ef2bdab2dddb13f76bfa92b91a8a36a6c2ed7ab93 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 159/339: model.layers.13.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 113]\n",
            "  Coordinate: L13_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ab58553c8deb3fd3e453cac3b2c3232d9ca2e2dcd6b0b637e9789e9578399bf5 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 160/339: model.layers.13.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 113]\n",
            "  Coordinate: L13_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0d3648e388344a7cff3b4c9723e8b8c748cda526c18f57aa8acc40a261676c4b -> Type: knowledge, Status: active, Coords: L13_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 161/339: model.layers.13.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 113]\n",
            "  Coordinate: L13_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 973eb85f9a338c52b206e6257ebe06d50d298270cebe2f6ecbacf21445485bd6 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 162/339: model.layers.13.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 113]\n",
            "  Coordinate: L13_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6ba3a187caa6b64ac698dd32e02ada05d31f50ce04784aa3121b472ceccf9cd7 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 163/339: model.layers.13.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 113]\n",
            "  Coordinate: L13_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 93736eb264be6947276c60408fc6572358b6003b3e112940b5239a03be0ed032 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 164/339: model.layers.13.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 113]\n",
            "  Coordinate: L13_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 65bb297ce47d94479823e6f25adb934eeda5d7f7e8e35c88c86c5ba56ef43ebd -> Type: knowledge, Status: active, Coords: L13_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 165/339: model.layers.13.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 113]\n",
            "  Coordinate: L13_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c4e5812cce0cb3d5e84ba633dfa8cd06cdd4d5d2972bba046793dd2bc4f7c593 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 166/339: model.layers.13.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 113]\n",
            "  Coordinate: L13_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3100301befff01815ad53928e92c4d3c0784908c9fc95523f4d63ae1cd2c5f8c -> Type: knowledge, Status: active, Coords: L13_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 167/339: model.layers.13.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.13.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 113]\n",
            "  Coordinate: L13_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID dbdbc9c03be50564349ae683344c176ff10a8d88b61d27a1dc37557d401763f3 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 168/339: model.layers.13.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 113]\n",
            "  Coordinate: L13_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8732c8c803775ece273e0a9e59adca00d7b2386d7377d8850c03efe06e17b2e1 -> Type: knowledge, Status: active, Coords: L13_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 169/339: model.layers.13.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 113]\n",
            "  Coordinate: L13_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b5227aeb2ca0304f36ae28089e5b518f51b4f84e09d92108113dca814af5e36b -> Type: knowledge, Status: active, Coords: L13_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 170/339: model.layers.14.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 114]\n",
            "  Coordinate: L14_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ca06358412f773564beb9bea8c64c82d12f44d45c045c716e75d2b815f006336 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 171/339: model.layers.14.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 114]\n",
            "  Coordinate: L14_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 854baf0337e7afd23c1147cb50cb0f3fc0532093e47e40345e09458067da7d3a -> Type: knowledge, Status: active, Coords: L14_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 172/339: model.layers.14.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 114]\n",
            "  Coordinate: L14_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c2ce0045acf3546ee251f5b7b2fd9f20416325d426dc8d377f8361f328fc5220 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 173/339: model.layers.14.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 114]\n",
            "  Coordinate: L14_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bc5acc1ceb1fea1dd2b29b21ba00e20ee7481e5c527016eb2560daf699816487 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 174/339: model.layers.14.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 114]\n",
            "  Coordinate: L14_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 13e701bbb4eff606e247640f2834e10b1771e3553af15d4e682689606d7d6e1e -> Type: knowledge, Status: active, Coords: L14_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 175/339: model.layers.14.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 114]\n",
            "  Coordinate: L14_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 94852cadf72e7a700974cf938aea7d99020d1c8d2ba05312fe3e9aacc7fd38d8 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 176/339: model.layers.14.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 114]\n",
            "  Coordinate: L14_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b6b5a521cae2ceac4e8886d0608356a3d687d94c9e713a4a58a1a75f883c58e8 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 177/339: model.layers.14.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 114]\n",
            "  Coordinate: L14_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f6c01a18f5767442fa0807a9a8df06b6bb9af67a96a9435d46874b909f72aeb2 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 178/339: model.layers.14.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 114]\n",
            "  Coordinate: L14_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 16374c08f229967165e0894dd04d77d41f1da02f57372877b2b525e770f9ac21 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 179/339: model.layers.14.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.14.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 114]\n",
            "  Coordinate: L14_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9631d642d05004fe21f79b6d08704c8d43ae38d11def901f44f3775db632cfd1 -> Type: knowledge, Status: active, Coords: L14_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 180/339: model.layers.14.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 114]\n",
            "  Coordinate: L14_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d0c839fd8df8b8b53a62953247ea6d0c1757bd83d766c0cda2f241a56d45a45f -> Type: knowledge, Status: active, Coords: L14_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 181/339: model.layers.14.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 114]\n",
            "  Coordinate: L14_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5e408c4ef642ce51b287127a54c1a260548e28b66aaed9ae2f130f28fcd22e7f -> Type: knowledge, Status: active, Coords: L14_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 182/339: model.layers.15.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 115]\n",
            "  Coordinate: L15_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4674e4d8f3f2fe6fa41202be9b5c82eb6800a0334801a852044982041d407d34 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 183/339: model.layers.15.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 115]\n",
            "  Coordinate: L15_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 80bfabd90b7f123ee23f5dded60eb767ea9267c2fd9102a20720891603d97f12 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 184/339: model.layers.15.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 115]\n",
            "  Coordinate: L15_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID aaad72c95118f60bc9e6a3cfff5ac557e151af99666a74e4138303195e6e9b98 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 185/339: model.layers.15.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 115]\n",
            "  Coordinate: L15_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 31768f6301105d19b9a72a24d3212c3ae3dd9416f1d17d5cc984c9a8dc0bbfce -> Type: knowledge, Status: active, Coords: L15_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 186/339: model.layers.15.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 115]\n",
            "  Coordinate: L15_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 310d3ce0df6d37ccb3e91b09d26d65df61446798ef217b7cf6d20aecf7aae75f -> Type: knowledge, Status: active, Coords: L15_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 187/339: model.layers.15.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 115]\n",
            "  Coordinate: L15_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2a1275601ec3a67897a20e562b3ffe10d485cddf53fdb9244fc7b3c01c65de91 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 188/339: model.layers.15.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 115]\n",
            "  Coordinate: L15_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 638da2ed1d97e9201f87df19667f78d8ee9252d13eb1b706350f4b2af2fa46da -> Type: knowledge, Status: active, Coords: L15_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 189/339: model.layers.15.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 115]\n",
            "  Coordinate: L15_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8fdbc3eb8412cb84d98a617c73c482248421a4fe518dca65d4743a1ad75c7e08 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 190/339: model.layers.15.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 115]\n",
            "  Coordinate: L15_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c027eb01fb2fc787079904a76514dae9f7980690e077ce037242ef0c8045cf05 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 191/339: model.layers.15.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.15.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 115]\n",
            "  Coordinate: L15_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d6ab50a785f61c9d836e8b11bdf74ea4d46419129f614d5137cf86745ec0e2c6 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 192/339: model.layers.15.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 115]\n",
            "  Coordinate: L15_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f229dee5b97a23222822b80d09bb49d841add3f7059df361cadf3be35040cf0f -> Type: knowledge, Status: active, Coords: L15_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 193/339: model.layers.15.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 115]\n",
            "  Coordinate: L15_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f734aa9777af51e6ff7f858ff5cd57609f0fd28fa481a0b99aa1df885d95a529 -> Type: knowledge, Status: active, Coords: L15_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 194/339: model.layers.16.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 116]\n",
            "  Coordinate: L16_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c7b2434e4ad67ebf7d23e23b740133d69b4459d3fe41e5b299761fd8cfa987b4 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 195/339: model.layers.16.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 116]\n",
            "  Coordinate: L16_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c5b21e90ec9f1a944b842bf2146173a574ba1e3fa7be64ceb58caa68cdc3c063 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 196/339: model.layers.16.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 116]\n",
            "  Coordinate: L16_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a354c6822424392b3260f1622012b116888e0a165e56616ec97c199894ced80f -> Type: knowledge, Status: active, Coords: L16_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 197/339: model.layers.16.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 116]\n",
            "  Coordinate: L16_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d4d1c5815d545205804d4f95b7a029c499ce9d6de47958b1c0cccf76841632ba -> Type: knowledge, Status: active, Coords: L16_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 198/339: model.layers.16.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 116]\n",
            "  Coordinate: L16_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7e406f5c8e2f6608a1301335d87031974288f452dc05f7299c8561c017f2876b -> Type: knowledge, Status: active, Coords: L16_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 199/339: model.layers.16.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 116]\n",
            "  Coordinate: L16_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a342acd09b9c9716dfdb201a91942e57ee05491bac1aa93f001caeb2777ffd64 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 200/339: model.layers.16.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 116]\n",
            "  Coordinate: L16_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5d2c7fa0218614cc19654bb943bf796bb578655bdaed9ef24b1bc0e1b9dc3f4c -> Type: knowledge, Status: active, Coords: L16_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 201/339: model.layers.16.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 116]\n",
            "  Coordinate: L16_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9580abac28a5134dcd4c2a921cd99b1cc308a1eb588703e9855da10c4dd3e450 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 202/339: model.layers.16.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 116]\n",
            "  Coordinate: L16_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0e4644ad92743837e30df161c7b97af3aec15e44fb331303b79e2743b03147b0 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 203/339: model.layers.16.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.16.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 116]\n",
            "  Coordinate: L16_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0e40489aced92bca99cdba7a4923e5687bdf97d0adccd082a6e7f0ba9ed6e052 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 204/339: model.layers.16.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 116]\n",
            "  Coordinate: L16_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ade29977dad3cbd34ad783e80e2487193fc94080039949ba26f7027619c16a60 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 205/339: model.layers.16.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 116]\n",
            "  Coordinate: L16_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d52b060cd6b96f5c521b2e8392fdffa5c0c3337da014e6dfc081b5d255dd4f19 -> Type: knowledge, Status: active, Coords: L16_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 206/339: model.layers.17.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 117]\n",
            "  Coordinate: L17_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 89ab249ad05b0d4fa07490e1b166a4b8ecddd22060ee0aa7c8bef0dd8b950419 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 207/339: model.layers.17.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 117]\n",
            "  Coordinate: L17_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ed1d1c2bbba8b446846745721328ecc8cee4eab20a1d8e0e69906f7ec480ff01 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 208/339: model.layers.17.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 117]\n",
            "  Coordinate: L17_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 522019552ea605ee1c18b8271ab5b0297bb8bce1cea63a2c2bb3d58a5c012041 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 209/339: model.layers.17.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 117]\n",
            "  Coordinate: L17_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 28434af68f9912b22a39818dc88845424f921ea5399bb47a06e5299e5c402e1d -> Type: knowledge, Status: active, Coords: L17_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 210/339: model.layers.17.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 117]\n",
            "  Coordinate: L17_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c44ae8afcd1598123c9c4e5caa7fd5ef33a7ae6e9977c62aacf4bae1b590a3e6 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 211/339: model.layers.17.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 117]\n",
            "  Coordinate: L17_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9b0e8cb1cef9bf1d20855b4189ebc73384ff0901ea1225e72b4ff79cab1d4b5d -> Type: knowledge, Status: active, Coords: L17_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 212/339: model.layers.17.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 117]\n",
            "  Coordinate: L17_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fa221b3e0a855eb0a2f62de4b2dd116f6b6ddbdf9ecffb7189938ac2b37e0bda -> Type: knowledge, Status: active, Coords: L17_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 213/339: model.layers.17.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 117]\n",
            "  Coordinate: L17_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a9f63f513ae2c8893b995075cfad7bc7182284c0107a76a39aad6a178a108c40 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 214/339: model.layers.17.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 117]\n",
            "  Coordinate: L17_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8160eb4615ef8d81dde376fca4fda209be74ae371482d0a923a94195a256eb9a -> Type: knowledge, Status: active, Coords: L17_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 215/339: model.layers.17.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.17.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 117]\n",
            "  Coordinate: L17_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 40a81dea80603e81a71ad1f7b9e3fd068cc8000b938ab3449047bf83783e485f -> Type: knowledge, Status: active, Coords: L17_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 216/339: model.layers.17.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 117]\n",
            "  Coordinate: L17_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8ba7c37da3ac28ec214692913a21b42f6d34c05074b880e06cf2d7a5cb3defbf -> Type: knowledge, Status: active, Coords: L17_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 217/339: model.layers.17.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 117]\n",
            "  Coordinate: L17_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b1059dc5f7fa9ea19ec22c2597b5957e846fb510173de6d41250c5b5a66dc8a8 -> Type: knowledge, Status: active, Coords: L17_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 218/339: model.layers.18.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 118]\n",
            "  Coordinate: L18_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2e765139774a1a9acc6d3db2217a0b04ea97ff1a4a33bffa4985e7c3ddf694ac -> Type: knowledge, Status: active, Coords: L18_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 219/339: model.layers.18.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 118]\n",
            "  Coordinate: L18_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8e66ad28e8295b0a1dd6e325b608167b60bb021ab88dab896221dd7b43d6ab4d -> Type: knowledge, Status: active, Coords: L18_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 220/339: model.layers.18.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 118]\n",
            "  Coordinate: L18_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1586182d303c2229dd1485c25f43f68da229e940b3c5a54a9f2f95dbd2c48875 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 221/339: model.layers.18.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 118]\n",
            "  Coordinate: L18_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3d75dd95f3947010a41766b51b1a08d51d855049c73b6b3d4efef424ec9cf8a7 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 222/339: model.layers.18.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 118]\n",
            "  Coordinate: L18_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 18df17c15a760125488faee050b9281458dfd845adc45a1ae5f9f53da5990ea2 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 223/339: model.layers.18.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 118]\n",
            "  Coordinate: L18_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 760a9f0eb6598f2bbd57431d65b10e3c7d783c7a0b5184a553df6aea870a1493 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 224/339: model.layers.18.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 118]\n",
            "  Coordinate: L18_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4d79dfa1b61003bdd493720a60995cc8e36290614952411f71680f41f7627dd6 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 225/339: model.layers.18.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 118]\n",
            "  Coordinate: L18_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2c5854d2d026717cd1b8855b248dd6ffb9ba8183dd27b2ef486f3d78174ecf16 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 226/339: model.layers.18.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 118]\n",
            "  Coordinate: L18_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 659ad6222b1e7443b7ee62f989f814e0f0f2f7b7beceb0519741f5e4db99adc4 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 227/339: model.layers.18.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.18.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 118]\n",
            "  Coordinate: L18_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 237050d0fd8652b3ba165ce7f7d85944739a8eec2bda3e95c2a21208136a842e -> Type: knowledge, Status: active, Coords: L18_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 228/339: model.layers.18.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 118]\n",
            "  Coordinate: L18_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 82213ed9b6ca5a16794158fa9344faf77b5ea206d54159dc976ca426b71c34d2 -> Type: knowledge, Status: active, Coords: L18_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 229/339: model.layers.18.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 118]\n",
            "  Coordinate: L18_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID edb17e1c86f0ba9d460ff4732834a3ad3424cec708ea69443686fc692aeb589b -> Type: knowledge, Status: active, Coords: L18_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 230/339: model.layers.19.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 119]\n",
            "  Coordinate: L19_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4b1c84089fec223ddd8e66dfc3910c055bc568788a8bc1c0fa1b2fc57d6d1350 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 231/339: model.layers.19.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 119]\n",
            "  Coordinate: L19_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 104cd268c92466bed192f9998e4f254a6966b6edbaca3f0313c9af6f7af3da5a -> Type: knowledge, Status: active, Coords: L19_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 232/339: model.layers.19.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 119]\n",
            "  Coordinate: L19_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1af667de8c67b3df3165f39303c9ca7c90ea2f51dc0844e82baa9bb34b225b7e -> Type: knowledge, Status: active, Coords: L19_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 233/339: model.layers.19.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 119]\n",
            "  Coordinate: L19_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5b1b6ffff32e75cdf847e9a49dc480e5cc474d252ac2d9de20e8604976567f50 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 234/339: model.layers.19.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 119]\n",
            "  Coordinate: L19_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2717e7919c0e79bcb466b1bde6922bfafb90dc8d939a65078e7b7a461ec972fe -> Type: knowledge, Status: active, Coords: L19_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 235/339: model.layers.19.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 119]\n",
            "  Coordinate: L19_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1ce54736e2dc64f58adcd9ca4f7c7d32c51a01b6687c37d75947d29e5d281b56 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 236/339: model.layers.19.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 119]\n",
            "  Coordinate: L19_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0c1ebbc791d3cae44cb5268bd6b687c520c636de8077c68aa782abffbba8f6d0 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 237/339: model.layers.19.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 119]\n",
            "  Coordinate: L19_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 556888b8282b6a0dce1d5c3105ea9a10353b847d7190a34d59f9cdec1076e4af -> Type: knowledge, Status: active, Coords: L19_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 238/339: model.layers.19.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 119]\n",
            "  Coordinate: L19_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5aeb205d4462b2f0b183ef6c19050249f811b3db7dd0c3880557d2a738115051 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 239/339: model.layers.19.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.19.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 119]\n",
            "  Coordinate: L19_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 227094ee29bb0284c7bbbe714b4dd49b3bc40b3aeb178dac0ef14b5b76558bca -> Type: knowledge, Status: active, Coords: L19_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 240/339: model.layers.19.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 119]\n",
            "  Coordinate: L19_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a5372bcde1a89fd1dc908c82931fb3271ef7487bd88b93e00d0dd8ad027de716 -> Type: knowledge, Status: active, Coords: L19_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 241/339: model.layers.19.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 119]\n",
            "  Coordinate: L19_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4a6023d3866de73eb5363c49a00c0714cd5501469b9548a7c17ead94b410a6bb -> Type: knowledge, Status: active, Coords: L19_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 242/339: model.layers.20.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 120]\n",
            "  Coordinate: L20_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f5ebb32ae81e893561afac54a7e1f65e39f9fc7535700d919859fb76c4b254ed -> Type: knowledge, Status: active, Coords: L20_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 243/339: model.layers.20.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 120]\n",
            "  Coordinate: L20_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2cfa28d9b2a86d4df727180ee7b0bd2679a62f4bd9cdf02b844b0427046d8e32 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 244/339: model.layers.20.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 120]\n",
            "  Coordinate: L20_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7be40d0060de5736935960fc60b3417c17a4dfed5a5492850614590c87648394 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 245/339: model.layers.20.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 120]\n",
            "  Coordinate: L20_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d92ee5cee083f19dc46127a8157a21f367063b7e1967938ee21a4181d9785042 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 246/339: model.layers.20.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 120]\n",
            "  Coordinate: L20_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a979cb2a113e376fa64a3c4862164a40e94a541061c0de127f726b0da6a1a003 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 247/339: model.layers.20.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 120]\n",
            "  Coordinate: L20_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3e7cb198b800decf61df6e4a94a1f7cf0d65a20864f0253dbcd56bb2cd49f118 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 248/339: model.layers.20.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 120]\n",
            "  Coordinate: L20_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b3cdeab6072335d1c89a54af21d4214f4bbb9308da469ff68fe3e056ecb2917b -> Type: knowledge, Status: active, Coords: L20_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 249/339: model.layers.20.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 120]\n",
            "  Coordinate: L20_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8c2b546cae082a6ba14d085319ea93ab9814d19362ce1a48c103c399bfd2bfa0 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 250/339: model.layers.20.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 120]\n",
            "  Coordinate: L20_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d6ab0299c7fd75359126ebe31e7fb88fb4048ddd403b694fb5b8813a1b5d5373 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 251/339: model.layers.20.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.20.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 120]\n",
            "  Coordinate: L20_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7c9e76a7a2330887de5f0f28ee09c2109dcec193d5538cb27a73a881f6a21f98 -> Type: knowledge, Status: active, Coords: L20_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 252/339: model.layers.20.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 120]\n",
            "  Coordinate: L20_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 73a71c532d35f97a5605a17853e4476c3ddb8d5f7b67bc2c8b61cd0aab94d2ca -> Type: knowledge, Status: active, Coords: L20_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 253/339: model.layers.20.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 120]\n",
            "  Coordinate: L20_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 072432d22f486a5da624bd34098c50b0d2402ce594b492ea5e9cd1437d9a340e -> Type: knowledge, Status: active, Coords: L20_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 254/339: model.layers.21.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 121]\n",
            "  Coordinate: L21_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8d8929e762902cacbd4cb2ddfff860db02b57d645272a800fcc0b09534810380 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 255/339: model.layers.21.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 121]\n",
            "  Coordinate: L21_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 032f1e488fc83784a48716e06ce6310fd16effaeca834f42235065a2f17b42c7 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 256/339: model.layers.21.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 121]\n",
            "  Coordinate: L21_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b725f58b6c10435e5361d5d01691f0d673b930d3e60adec5e66855de908eb13a -> Type: knowledge, Status: active, Coords: L21_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 257/339: model.layers.21.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 121]\n",
            "  Coordinate: L21_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID e34e99f638b5f946ec78ace9feb0ba6542daf3cfb24047be67127abdf30e4268 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 258/339: model.layers.21.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 121]\n",
            "  Coordinate: L21_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8a925343f497f91cbe94f24d83a8aec319508483e6be2316371e70671a79c6cd -> Type: knowledge, Status: active, Coords: L21_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 259/339: model.layers.21.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 121]\n",
            "  Coordinate: L21_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c5dad0b6de0b9dd26353166b09a9c41a973774c09c3c9e31e806ddc3e30459ef -> Type: knowledge, Status: active, Coords: L21_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 260/339: model.layers.21.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 121]\n",
            "  Coordinate: L21_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2257f54c85eabe819ec08caa55ec7197f5bdefd4f2713c347f7547a31bde89a8 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 261/339: model.layers.21.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 121]\n",
            "  Coordinate: L21_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a77dc325c4aa92c556b5b26e06f894d2c0fdf1207c753ddf06d1e7b21e7cb73b -> Type: knowledge, Status: active, Coords: L21_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 262/339: model.layers.21.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 121]\n",
            "  Coordinate: L21_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2f79dc027f263c97de39c0cddb7d4f8e4ce7102171f1ccd65b85fc1496a31889 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 263/339: model.layers.21.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.21.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 121]\n",
            "  Coordinate: L21_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ace256a69ee3d5adf501c9d66620d98171817100aa433e82be11bce214f0e6d8 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 264/339: model.layers.21.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 121]\n",
            "  Coordinate: L21_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7f70002fe0136fdbb716c35d66570b115e1d074c48af292c128f4ff974573eb2 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 265/339: model.layers.21.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 121]\n",
            "  Coordinate: L21_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b525599c3583a3105cc9440cfba69fcec7a0dab0781897e1b328b70ed853d0e2 -> Type: knowledge, Status: active, Coords: L21_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 266/339: model.layers.22.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 122]\n",
            "  Coordinate: L22_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c0cd00f030b4512831e41e7df806b5ece8a007624cf1e1ec5de5df9c5a825347 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 267/339: model.layers.22.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 122]\n",
            "  Coordinate: L22_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d560075c526659479a7e2990e16dd899f8bca094e26abc1a970d3ca814b02811 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 268/339: model.layers.22.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 122]\n",
            "  Coordinate: L22_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0c4194691d0a7e41c76ac52fa942350b44e69fb1f74fbc0223e3c28fdf97f1d4 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 269/339: model.layers.22.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 122]\n",
            "  Coordinate: L22_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bf6127fd1f36480c116f90642aa8744f88d78577de8d2b69d972c1eb90ec1c00 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 270/339: model.layers.22.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 122]\n",
            "  Coordinate: L22_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fd8b23acc76d34c783cfe2300b6f403284faa41fc29981d700c57c2996fcf6a9 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 271/339: model.layers.22.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 122]\n",
            "  Coordinate: L22_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 771844942dac9827b76e697ec0a43a114a8effb48a37fe294c120db25abca42a -> Type: knowledge, Status: active, Coords: L22_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 272/339: model.layers.22.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 122]\n",
            "  Coordinate: L22_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5dc1806f6c638b8d4022ea32f22c3df86ac93b449f4b14b88fefcc9976aa7226 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 273/339: model.layers.22.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 122]\n",
            "  Coordinate: L22_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 03651fd35f8058abb877d31215cd9ba8482056956a4e1d1cf3a4d69d60bf5f33 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 274/339: model.layers.22.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 122]\n",
            "  Coordinate: L22_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3c85ee4467669aad64e39556050ee8674fd790c350820f541e1a07d51a64b3fb -> Type: knowledge, Status: active, Coords: L22_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 275/339: model.layers.22.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.22.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 122]\n",
            "  Coordinate: L22_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 48143ba6c4acce1a373a714e9734751f9a715b17e1acc3dabacce3c4fc675f68 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 276/339: model.layers.22.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 122]\n",
            "  Coordinate: L22_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2adca8b54bffaf38ad69d6a6ec6cd59b8572864019665962ad369573fae9924e -> Type: knowledge, Status: active, Coords: L22_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 277/339: model.layers.22.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 122]\n",
            "  Coordinate: L22_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7c59ea0007bffa526a0591030b086e019844b1da14fb1b14a31ad48490ea0e37 -> Type: knowledge, Status: active, Coords: L22_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 278/339: model.layers.23.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 123]\n",
            "  Coordinate: L23_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 73f9fced7a14521d153d7cb47733ea9e48f8439a7bab87fa175783f99065db24 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 279/339: model.layers.23.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 123]\n",
            "  Coordinate: L23_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f0087347121fffa67f245672d2e57c2b870dff20f51004feeb56c522bcd8ea6a -> Type: knowledge, Status: active, Coords: L23_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 280/339: model.layers.23.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 123]\n",
            "  Coordinate: L23_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 78e617b2fed877a2cce4aa49af6cbfe76aedc612e05ba2f7f9ab0af375eabe9b -> Type: knowledge, Status: active, Coords: L23_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 281/339: model.layers.23.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 123]\n",
            "  Coordinate: L23_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fa833d4189829155735f7721423fd40caaaa05e0e4465e102a3f521153427b9e -> Type: knowledge, Status: active, Coords: L23_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 282/339: model.layers.23.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 123]\n",
            "  Coordinate: L23_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b07fb2bb5be4a8d7a799330b44709b7f46f828b08e3b4029b4fba2a8dca73dec -> Type: knowledge, Status: active, Coords: L23_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 283/339: model.layers.23.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 123]\n",
            "  Coordinate: L23_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8a05f2c2be465d7b84437d4e9f214f78eb1cac9a28165aebbf8bfb1d9194cb1c -> Type: knowledge, Status: active, Coords: L23_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 284/339: model.layers.23.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 123]\n",
            "  Coordinate: L23_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d96cf0dc14dedc45c266144d6f90ceab10249963e44a628439981bee7785d5e7 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 285/339: model.layers.23.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 123]\n",
            "  Coordinate: L23_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 78d098423747c93210743f5bdf7e6a754206164ba196bfcca14aaa09786c2c1e -> Type: knowledge, Status: active, Coords: L23_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 286/339: model.layers.23.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 123]\n",
            "  Coordinate: L23_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 81391b5e1f342e70a698ad5f95b63a15f7db239f7794eb8d93f2e1e54666a7a8 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 287/339: model.layers.23.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.23.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 123]\n",
            "  Coordinate: L23_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 94e2c32969c07ea0652683563c2841947c549eea57b54224c4a284a8f891db30 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 288/339: model.layers.23.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 123]\n",
            "  Coordinate: L23_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d78c71b679865caabb44db7e4fa4bb5f35c2fc398f2ce9be84fda7fd212ceaa4 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 289/339: model.layers.23.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 123]\n",
            "  Coordinate: L23_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 46daa735985ff6cf65f0f23dd2651a5117655ff2654defa04f311a4bd9d3abf2 -> Type: knowledge, Status: active, Coords: L23_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 290/339: model.layers.24.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 124]\n",
            "  Coordinate: L24_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7c4499da69aa970278f55948f95e4403be5bc2bed19760e7ee5e04ebc6e0105e -> Type: knowledge, Status: active, Coords: L24_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 291/339: model.layers.24.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 124]\n",
            "  Coordinate: L24_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ec753b535e64c0e825d6f890e4942417d1e74e4ae530f53015d9edabfabca6d7 -> Type: knowledge, Status: active, Coords: L24_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 292/339: model.layers.24.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 124]\n",
            "  Coordinate: L24_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9a5b76d83861590685d717a498e2633f1c9a849d185a87badc267ae78ba94e70 -> Type: knowledge, Status: active, Coords: L24_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 293/339: model.layers.24.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 124]\n",
            "  Coordinate: L24_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6e69d925e02fb965f70a9c44783133d99881240072fb2e5385708afca88c4480 -> Type: knowledge, Status: active, Coords: L24_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 294/339: model.layers.24.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 124]\n",
            "  Coordinate: L24_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9b20eafc3beb8fb419d5b26ae2967bae35eb97175d7c82bcb429b201b4ab064e -> Type: knowledge, Status: active, Coords: L24_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 295/339: model.layers.24.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 124]\n",
            "  Coordinate: L24_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8bf9590226556f0acd1eb48f133b9da3170064caf9bfd2533cbb91242aa2516e -> Type: knowledge, Status: active, Coords: L24_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 296/339: model.layers.24.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 124]\n",
            "  Coordinate: L24_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fb1a8233b6b77f30f74fd069bda62f9c30d93922cbd5523823c5b6326d1acd95 -> Type: knowledge, Status: active, Coords: L24_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 297/339: model.layers.24.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 124]\n",
            "  Coordinate: L24_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 81f8141c2d2d768185ee2d0daf8bd0534d2060d1677a074fb75ea6a660492f1b -> Type: knowledge, Status: active, Coords: L24_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 298/339: model.layers.24.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 124]\n",
            "  Coordinate: L24_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0f6490462c5bba222a81691e76c8465504ddcbbe369f2d338c5f95364089521f -> Type: knowledge, Status: active, Coords: L24_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 299/339: model.layers.24.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.24.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 124]\n",
            "  Coordinate: L24_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID cf8c2c2b08b1d114d46a7b104dc397953b43884463ebe7dbd5e017f60774bf19 -> Type: knowledge, Status: active, Coords: L24_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 300/339: model.layers.24.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 124]\n",
            "  Coordinate: L24_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0320a628fb20ee5ffa543d0c8b1744acac50a3345ff066765fb51494b0ba32bc -> Type: knowledge, Status: active, Coords: L24_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 301/339: model.layers.24.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 124]\n",
            "  Coordinate: L24_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 572076d071143599ac6b1afac6ca38ddf7ee5ab384019dca30cedfd67dc0323c -> Type: knowledge, Status: active, Coords: L24_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 302/339: model.layers.25.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 125]\n",
            "  Coordinate: L25_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a20bd9f2a26b173a580409ec2120cf530ef272720ade5237e38284b6cb11910b -> Type: knowledge, Status: active, Coords: L25_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 303/339: model.layers.25.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 125]\n",
            "  Coordinate: L25_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ea0657ba1605b289a95d4432f603778b06250c60a4ca348012466624fe0ee6ac -> Type: knowledge, Status: active, Coords: L25_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 304/339: model.layers.25.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 125]\n",
            "  Coordinate: L25_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 842e494626f7f174170697244de6419b6c9217e7d4971c9eb4d7f64319c94e51 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 305/339: model.layers.25.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 125]\n",
            "  Coordinate: L25_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 049c6818453c7d61a287f9c10c2f07b547da1f861a04f4499e0c013429a6762d -> Type: knowledge, Status: active, Coords: L25_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 306/339: model.layers.25.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 125]\n",
            "  Coordinate: L25_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID b3a21e22f6e3c8d1bdae0e5b2904bc8d0c45c343d32cd911fc83daff18b8155e -> Type: knowledge, Status: active, Coords: L25_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 307/339: model.layers.25.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 125]\n",
            "  Coordinate: L25_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fcd72f30787b0b425bafc33e9efcf1e92608eee140d6f6df7a6f3eba374814c0 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 308/339: model.layers.25.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 125]\n",
            "  Coordinate: L25_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7610216f042bc4c25df7e2a99e42fc19d8bfb6a7370e01ad164240f145551fe1 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 309/339: model.layers.25.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 125]\n",
            "  Coordinate: L25_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9bae0d64fa63df0645dab326234bff23d469b24a42d8a900b6f3461a8dee9435 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 310/339: model.layers.25.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 125]\n",
            "  Coordinate: L25_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 34214bfd6b623534debace40aeaf255f95bea83f50492bc132a538fde2f13cb7 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 311/339: model.layers.25.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.25.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 125]\n",
            "  Coordinate: L25_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 775fbd54639fca62b819dd18109bf85ab19d1a8e99e4b0b9f900b6fab484aab6 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 312/339: model.layers.25.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 125]\n",
            "  Coordinate: L25_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 494c6c01d61e1604939ac73561f81ac7c9905a41a07858ce1911dea52d3c35fa -> Type: knowledge, Status: active, Coords: L25_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 313/339: model.layers.25.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 125]\n",
            "  Coordinate: L25_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a6b8ff9c620bdbf7a9b8c1175ad1459b0f2ac4386533082251754efb8b812c36 -> Type: knowledge, Status: active, Coords: L25_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 314/339: model.layers.26.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 126]\n",
            "  Coordinate: L26_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 77be26278d6361d239102a4b3d92fc8cc9e8170689e96f7fad3afb8da3c3aa04 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 315/339: model.layers.26.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 126]\n",
            "  Coordinate: L26_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3f04520442a6e5c994da666ba3289523c4a30728c070766a2ef2392a9dae31ec -> Type: knowledge, Status: active, Coords: L26_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 316/339: model.layers.26.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 126]\n",
            "  Coordinate: L26_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID f6ba123864eaf0e48a5539bc6f1e0143fec2c08780cf2a546a6bb10e7028fd86 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 317/339: model.layers.26.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 126]\n",
            "  Coordinate: L26_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 36415bb1de341dcadcf6401f3bb33221b15cf814dec9c5d0651a9e5769cf1ebf -> Type: knowledge, Status: active, Coords: L26_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 318/339: model.layers.26.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 126]\n",
            "  Coordinate: L26_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 44e23bcef3dcf9fe2a3f457c08f4f6d1f714eab10a5815da212a01234f29abbb -> Type: knowledge, Status: active, Coords: L26_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 319/339: model.layers.26.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 126]\n",
            "  Coordinate: L26_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2f44950f7af7f61dae7446894a57ed27e0e9b8a5636155713ccdd6d5e6b2e439 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 320/339: model.layers.26.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 126]\n",
            "  Coordinate: L26_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 74688ff0cbba17596e94652e4b0f7d2f444dfee302502cc35f4380ec4f570c48 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 321/339: model.layers.26.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 126]\n",
            "  Coordinate: L26_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID a481e48b867c5d6c52cb622f100075fa7b7eb857607b45df368ff592baabd7e7 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 322/339: model.layers.26.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 126]\n",
            "  Coordinate: L26_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1ab4d6297f820aeb7dd2617acfdd93e7da4da1b0de4bd4222a86d80674f0b5dc -> Type: knowledge, Status: active, Coords: L26_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 323/339: model.layers.26.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.26.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 126]\n",
            "  Coordinate: L26_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 06747be57ede45f9715ba724cf5e31bcb2844042f58d19839adf1721cc39aa5c -> Type: knowledge, Status: active, Coords: L26_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 324/339: model.layers.26.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 126]\n",
            "  Coordinate: L26_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7e681ea83bc5642bb6e1b231c28721056960105949f6f4412781a0fe5d81175a -> Type: knowledge, Status: active, Coords: L26_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 325/339: model.layers.26.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 126]\n",
            "  Coordinate: L26_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID e6243d8cd89c9eda5e402529a268294a111bc22ffd07244d4fe10d5f032d0802 -> Type: knowledge, Status: active, Coords: L26_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 326/339: model.layers.27.self_attn.q_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.self_attn.q_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 33, 127]\n",
            "  Coordinate: L27_G100_N1_X10_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID dede52a3e89054be3676239376b08644a1f3aa178e5a386e712e5306666da67b -> Type: knowledge, Status: active, Coords: L27_G100_N1_X10_Y0_Z0\n",
            "\n",
            "Processing Param 327/339: model.layers.27.self_attn.q_proj.bias\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 33, 127]\n",
            "  Coordinate: L27_G100_N1_X11_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 519edb83facd3f04e7af6a9d99103136f906b0046323680da9dc4cc16346ceac -> Type: knowledge, Status: active, Coords: L27_G100_N1_X11_Y0_Z0\n",
            "\n",
            "Processing Param 328/339: model.layers.27.self_attn.k_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.self_attn.k_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 34, 127]\n",
            "  Coordinate: L27_G100_N1_X20_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 02dcf52df502e82010a83b3b83b11358b0727fcca9bde969f53ce1c2c521eb31 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X20_Y0_Z0\n",
            "\n",
            "Processing Param 329/339: model.layers.27.self_attn.k_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 34, 127]\n",
            "  Coordinate: L27_G100_N1_X21_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9b71b037b9e08de22f84147861b672cc462e853a3c518edd5834ff6f255b0851 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X21_Y0_Z0\n",
            "\n",
            "Processing Param 330/339: model.layers.27.self_attn.v_proj.weight\n",
            "  Original Shape: torch.Size([256, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.self_attn.v_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 35, 127]\n",
            "  Coordinate: L27_G100_N1_X30_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 256)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID def6a4800e0a9b04a275d2f5ea3a837285331543e58b0c4903f6939219c8f7df -> Type: knowledge, Status: active, Coords: L27_G100_N1_X30_Y0_Z0\n",
            "\n",
            "Processing Param 331/339: model.layers.27.self_attn.v_proj.bias\n",
            "  Original Shape: torch.Size([256]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 31, 35, 127]\n",
            "  Coordinate: L27_G100_N1_X31_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(256,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 015d0d2ece59a68bd52bacbe6a6efd32a021cc6c0a4bc02cbea8b6f28684de75 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X31_Y0_Z0\n",
            "\n",
            "Processing Param 332/339: model.layers.27.self_attn.o_proj.weight\n",
            "  Original Shape: torch.Size([1536, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.self_attn.o_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 36, 127]\n",
            "  Coordinate: L27_G100_N1_X40_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1b5eb8ea34982c9448299415b37b9c3d8eb6fc8fb6e4b2d0b568b0abcb657dc7 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X40_Y0_Z0\n",
            "\n",
            "Processing Param 333/339: model.layers.27.mlp.gate_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.mlp.gate_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 38, 127]\n",
            "  Coordinate: L27_G100_N1_X50_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4a4b4de46d03aeb7241b1a74a9a9726f281adf24abc49860761b5b7bc44e7693 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X50_Y0_Z0\n",
            "\n",
            "Processing Param 334/339: model.layers.27.mlp.up_proj.weight\n",
            "  Original Shape: torch.Size([8960, 1536]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.mlp.up_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 39, 127]\n",
            "  Coordinate: L27_G100_N1_X60_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536, 8960)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 70f3cbe8442e9f8db59c36b2d91ba5f2f0ce1124736a7d07b585213e6f0b9755 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X60_Y0_Z0\n",
            "\n",
            "Processing Param 335/339: model.layers.27.mlp.down_proj.weight\n",
            "  Original Shape: torch.Size([1536, 8960]) | Dtype: torch.float16\n",
            "  Transposing model.layers.27.mlp.down_proj.weight weights...\n",
            "  Final Tags: [2, 12, 21, 30, 40, 127]\n",
            "  Coordinate: L27_G100_N1_X70_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(8960, 1536)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 5608a8e5064a5b2bdfda4c16b205e214728bc2b0b90c6931c9281e2d6e336501 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X70_Y0_Z0\n",
            "\n",
            "Processing Param 336/339: model.layers.27.input_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 127]\n",
            "  Coordinate: L27_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 69b5aaabbe75163fe42c81bdecd0a9b9532c6643ac9863cb0ea7ed2824f76879 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X1_Y0_Z0\n",
            "\n",
            "Processing Param 337/339: model.layers.27.post_attention_layernorm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41, 127]\n",
            "  Coordinate: L27_G100_N1_X2_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 185e60d45a63ad9d92edd8e716e0eb048549511f304584f8e179206cd17c8c20 -> Type: knowledge, Status: active, Coords: L27_G100_N1_X2_Y0_Z0\n",
            "\n",
            "Processing Param 338/339: model.norm.weight\n",
            "  Original Shape: torch.Size([1536]) | Dtype: torch.float16\n",
            "  Final Tags: [2, 12, 21, 30, 41]\n",
            "  Coordinate: L28_G100_N1_X0_Y0_Z0\n",
            "  Data to save: dtype=float16, shape=(1536,)\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2dbca0bd79adb3681208482333751cd9fcc5ee77beb229ebfeec09fdb5b3a1b6 -> Type: knowledge, Status: active, Coords: L28_G100_N1_X0_Y0_Z0\n",
            "\n",
            "Processing Param 339/339: lm_head.weight\n",
            "  Original Shape: torch.Size([151936, 1536]) | Dtype: torch.float16\n",
            "  Transposing quantized LM Head weights...\n",
            "  Final Tags: [2, 12, 23, 30, 42]\n",
            "  Coordinate: L-1_G100_N1_X1_Y0_Z0\n",
            "  Data to save: dtype=<class 'numpy.int8'>, shape=(1536, 151936)\n",
            "  Extra Metadata: {'quantization_scale': 0.002583661349490285}\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID bf1df7c1407c9239f0aacd4e8618628236c0fa8b03b0fb738f001ab5b382d8f3 -> Type: knowledge, Status: active, Coords: L-1_G100_N1_X1_Y0_Z0\n",
            "\n",
            "--- Finished saving 339 knowledge tensors to /content/data/db ---\n",
            "\n",
            "Name <-> ID map saved to data/db/DeepSeek-R1-Distill-Qwen-1.5B_name_id_map.pkl\n",
            "\n",
            "--- Saving Knowledge Map (for Cell 5.5) ---\n",
            "  Knowledge map saved to data/db/DeepSeek-R1-Distill-Qwen-1.5B_knowledge_map.pkl\n",
            "\n",
            "'knowledge_map' created with 339 entries for Cell 5.5.\n",
            "\n",
            "Memory cleanup attempted.\n",
            "DB connection remains open for Cell 5.5/6.\n",
            "--- Cell 5 Finished in 443.94 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5.5: Save Intermediate Data for Cell 6 ===\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Running Cell 5.5: Saving Intermediate Data ---\")\n",
        "\n",
        "# --- Проверка наличия необходимых переменных из предыдущих ячеек ---\n",
        "if 'knowledge_map' not in locals() or not isinstance(knowledge_map, dict):\n",
        "    raise NameError(\"Variable 'knowledge_map' not found or invalid. Ensure Cell 5 ran successfully.\")\n",
        "if 'model' not in locals() or model is None:\n",
        "    # Нам нужен как минимум конфиг модели для num_layers\n",
        "    raise NameError(\"Variable 'model' (or model.config) not found. Ensure Cell 4 ran successfully.\")\n",
        "if 'HF_MODEL_NAME' not in locals() or not HF_MODEL_NAME:\n",
        "     raise NameError(\"Variable 'HF_MODEL_NAME' not defined. Check Cell 1.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "     raise NameError(\"Variable 'DB_PATH' not defined or invalid. Check Cell 1.\")\n",
        "\n",
        "# --- Данные для сохранения ---\n",
        "# Сохраняем только конфиг, а не всю модель, для экономии места\n",
        "cell6_input_data = {\n",
        "    'knowledge_map': knowledge_map,\n",
        "    'model_config': model.config, # Сохраняем конфиг\n",
        "    'hf_model_name': HF_MODEL_NAME,\n",
        "    'db_path': str(DB_PATH.resolve()) # Сохраняем путь к БД как строку\n",
        "}\n",
        "\n",
        "# --- Имя файла и сохранение ---\n",
        "intermediate_filename = f\"{HF_MODEL_NAME}_cell6_input_data.pkl\"\n",
        "intermediate_filepath = DB_PATH / intermediate_filename\n",
        "\n",
        "try:\n",
        "    # Убедимся, что директория DB_PATH существует\n",
        "    DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving intermediate data to: {intermediate_filepath}\")\n",
        "    with open(intermediate_filepath, 'wb') as f:\n",
        "        pickle.dump(cell6_input_data, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Intermediate data saved successfully.\")\n",
        "    print(f\"  Knowledge map entries: {len(knowledge_map)}\")\n",
        "    print(f\"  Model Config Type: {type(model.config)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"---!!! ERROR saving intermediate data: {e} !!!---\")\n",
        "    # Можно добавить raise e, если критично прервать выполнение\n",
        "else:\n",
        "    print(\"--- Cell 5.5 Finished ---\")\n",
        "\n",
        "# --- Очистка памяти от модели (если она больше не нужна до перезапуска) ---\n",
        "# Раскомментируй, если хочешь освободить память после сохранения промежуточных данных\n",
        "# import gc\n",
        "# if 'model' in locals(): del model\n",
        "# if 'torch' in locals() and hasattr(torch, 'cuda') and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "# print(\"Cleaned up model from memory (optional).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaC_YOwr4xHw",
        "outputId": "87abd061-3c7f-4159-e410-6bccd9570395"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Cell 5.5: Saving Intermediate Data ---\n",
            "Saving intermediate data to: data/db/DeepSeek-R1-Distill-Qwen-1.5B_cell6_input_data.pkl\n",
            "Intermediate data saved successfully.\n",
            "  Knowledge map entries: 339\n",
            "  Model Config Type: <class 'transformers.models.qwen2.configuration_qwen2.Qwen2Config'>\n",
            "--- Cell 5.5 Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5.6: Verify Tensor Data ===\n",
        "import numpy as np\n",
        "import torch\n",
        "import traceback\n",
        "\n",
        "print(\"\\n--- Running Cell 5.6: Verify Tensor Data ---\")\n",
        "\n",
        "# --- Proverka nalichija peremennyh ---\n",
        "if 'model' not in locals() or model is None: raise NameError(\"HF 'model' not loaded.\")\n",
        "if 'vec' not in locals() or vec is None: raise NameError(\"'vec' object not defined.\")\n",
        "if 'knowledge_map' not in locals() or not knowledge_map: raise NameError(\"'knowledge_map' not loaded or empty.\")\n",
        "\n",
        "# --- Funkcija dlja sravnenija ---\n",
        "def compare_weights(param_name: str, transpose_hf: bool = False):\n",
        "    \"\"\"\n",
        "    Sravnivaet ves iz original'noj modeli HF s vesom iz Veector knowledge tensor.\n",
        "\n",
        "    Args:\n",
        "        param_name (str): Imja parametra (kak v HF modeli).\n",
        "        transpose_hf (bool): Nuzhno li transponirovat' ves iz HF modeli dlja sravnenija.\n",
        "                             True dlja vesov linejnyh sloev (krome embedding).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Comparing: {param_name} ---\")\n",
        "    knowledge_id = knowledge_map.get(param_name)\n",
        "    if not knowledge_id:\n",
        "        print(f\"  ERROR: Knowledge ID not found in knowledge_map for '{param_name}'\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 1. Zagruzhaem original'nyj ves HF\n",
        "        param = model.get_parameter(param_name)\n",
        "        hf_weight = param.data.cpu().to(torch.float32).numpy()\n",
        "        if transpose_hf:\n",
        "            # Transponiruem dlja sootvetstvija NumPy matmul (input @ weight)\n",
        "            # Esli ishodnyj ves (out, in), delaem (in, out)\n",
        "            if hf_weight.ndim == 2: # Tol'ko dlja 2D vesov\n",
        "                 hf_weight = hf_weight.T\n",
        "            else:\n",
        "                 print(f\"  WARN: Transpose requested for non-2D HF weight ({param_name}, shape {hf_weight.shape}) - skipping transpose.\")\n",
        "\n",
        "        print(f\"  HF Weight: shape={hf_weight.shape}, dtype={hf_weight.dtype}, mean={np.mean(hf_weight):.4e}, std={np.std(hf_weight):.4e}\")\n",
        "\n",
        "        # 2. Zagruzhaem ves iz Veector (s dekvantovaniem, esli nuzhno)\n",
        "        veector_weight = vec.load_knowledge_tensor_data(knowledge_id)\n",
        "\n",
        "        if veector_weight is None:\n",
        "            print(f\"  ERROR: Failed to load Veector knowledge data for ID: {knowledge_id}\")\n",
        "            return\n",
        "\n",
        "        # load_knowledge_tensor_data uzhe vozvrashhaet float32 posle dekvantovanija\n",
        "        print(f\"  Veector Weight: shape={veector_weight.shape}, dtype={veector_weight.dtype}, mean={np.mean(veector_weight):.4e}, std={np.std(veector_weight):.4e}\")\n",
        "\n",
        "        # 3. Sravnivaem\n",
        "        if hf_weight.shape != veector_weight.shape:\n",
        "            print(f\"  ERROR: Shape mismatch! HF={hf_weight.shape}, Veector={veector_weight.shape}\")\n",
        "            # Mozhno popytat'sja sravnit' posle reshape/transpose, esli eto ozhidaemo\n",
        "            # if transpose_hf and hf_weight.T.shape == veector_weight.shape:\n",
        "            #     hf_weight = hf_weight.T\n",
        "            #     print(\"  INFO: Transposed HF weight for comparison attempt.\")\n",
        "            # else:\n",
        "            #     return # Ne sravnivaem, esli formy raznye\n",
        "            return # Poka prosto vyhodim pri nesootvetstvii form\n",
        "\n",
        "        max_abs_diff = np.max(np.abs(hf_weight - veector_weight))\n",
        "        mean_abs_diff = np.mean(np.abs(hf_weight - veector_weight))\n",
        "\n",
        "        print(f\"  Comparison (float32):\")\n",
        "        print(f\"    Max Abs Difference:  {max_abs_diff:.4e}\")\n",
        "        print(f\"    Mean Abs Difference: {mean_abs_diff:.4e}\")\n",
        "\n",
        "        # Proverka na bol'shie rashozhdenija\n",
        "        # Porogi nuzhno podbirat' (zavisjat ot float16/int8)\n",
        "        if max_abs_diff > 1e-2 or mean_abs_diff > 1e-3:\n",
        "             print(f\"  !!! WARNING: Significant difference detected !!!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR during comparison for '{param_name}': {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- Vyzyvaem sravnenie dlja neskol'kih kljuchevyh vesov ---\n",
        "\n",
        "# Embedding (ne transponiruem, kvantovan v int8)\n",
        "compare_weights(\"model.embed_tokens.weight\", transpose_hf=False)\n",
        "\n",
        "# Q-proekcija pervogo sloja (transponiruem, float16)\n",
        "compare_weights(\"model.layers.0.self_attn.q_proj.weight\", transpose_hf=True)\n",
        "\n",
        "# Gate-proekcija FFN pervogo sloja (transponiruem, float16)\n",
        "compare_weights(\"model.layers.0.mlp.gate_proj.weight\", transpose_hf=True)\n",
        "\n",
        "# Ves poslednego LayerNorm (ne transponiruem, float16)\n",
        "compare_weights(\"model.norm.weight\", transpose_hf=False)\n",
        "\n",
        "# LM Head (transponiruem, kvantovan v int8)\n",
        "compare_weights(\"lm_head.weight\", transpose_hf=True)\n",
        "\n",
        "# Mozhno dobavit' drugie vesovye matrichy dlja bolee detal'noj proverki\n",
        "# compare_weights(\"model.layers.10.self_attn.v_proj.weight\", transpose_hf=True)\n",
        "# compare_weights(\"model.layers.27.mlp.down_proj.weight\", transpose_hf=True)\n",
        "\n",
        "print(\"\\n--- Tensor Verification Finished ---\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxNccldxJiSH",
        "outputId": "ee77db1a-57be-4e72-c2a4-8d36b5cf89be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Cell 5.6: Verify Tensor Data ---\n",
            "\n",
            "--- Comparing: model.embed_tokens.weight ---\n",
            "  HF Weight: shape=(151936, 1536), dtype=float32, mean=3.8134e-04, std=3.0062e-02\n",
            "  Veector Weight: shape=(151936, 1536), dtype=float32, mean=3.8103e-04, std=3.0068e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  1.2995e-03\n",
            "    Mean Abs Difference: 6.4934e-04\n",
            "\n",
            "--- Comparing: model.layers.0.self_attn.q_proj.weight ---\n",
            "  HF Weight: shape=(1536, 1536), dtype=float32, mean=1.7502e-05, std=5.3037e-02\n",
            "  Veector Weight: shape=(1536, 1536), dtype=float16, mean=1.7524e-05, std=5.3070e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.mlp.gate_proj.weight ---\n",
            "  HF Weight: shape=(1536, 8960), dtype=float32, mean=-4.8338e-07, std=3.8473e-02\n",
            "  Veector Weight: shape=(1536, 8960), dtype=float16, mean=-4.7684e-07, std=3.9093e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.norm.weight ---\n",
            "  HF Weight: shape=(1536,), dtype=float32, mean=2.3067e+00, std=4.1719e-01\n",
            "  Veector Weight: shape=(1536,), dtype=float16, mean=2.3066e+00, std=4.1748e-01\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: lm_head.weight ---\n",
            "  HF Weight: shape=(1536, 151936), dtype=float32, mean=4.5009e-04, std=2.9313e-02\n",
            "  Veector Weight: shape=(1536, 151936), dtype=float32, mean=4.5046e-04, std=2.9323e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  1.2918e-03\n",
            "    Mean Abs Difference: 6.4594e-04\n",
            "\n",
            "--- Tensor Verification Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5.7: Verify Tensor Data (Fixed is_connected check) ===\n",
        "import numpy as np\n",
        "import torch\n",
        "import traceback\n",
        "from pathlib import Path # Dobavim import Path, esli ego net vyshe v jachejke\n",
        "import pickle # Dobavim import pickle dlja zagruzki knowledge_map\n",
        "\n",
        "print(\"\\n--- Running Cell 5.7: Verify Tensor Data ---\")\n",
        "\n",
        "# --- Ubedimsja, chto DB_PATH opredelen (nuzhno dlja perezagruzki vec, esli nado) ---\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "    # Poprobuem ego poluchit' iz HF_MODEL_NAME, esli on est'\n",
        "    if 'HF_MODEL_NAME' in locals() and HF_MODEL_NAME:\n",
        "         # Predpolagaem standartnuju strukturu puti\n",
        "         script_dir_simulated = Path('.') # V Colab eto /content\n",
        "         DB_PATH = script_dir_simulated / \"data\" / \"db\"\n",
        "         print(f\"WARN: DB_PATH not found, assuming default: {DB_PATH}\")\n",
        "         DB_PATH.mkdir(parents=True, exist_ok=True) # Sozdaem, esli net\n",
        "    else:\n",
        "         raise NameError(\"DB_PATH not defined.\")\n",
        "\n",
        "\n",
        "# --- Proverka nalichija peremennyh ---\n",
        "if 'model' not in locals() or model is None: raise NameError(\"HF 'model' not loaded. Run previous cells.\")\n",
        "if 'HF_MODEL_NAME' not in locals() or not HF_MODEL_NAME: raise NameError(\"HF_MODEL_NAME not defined.\")\n",
        "\n",
        "# --- Zagruzka knowledge_map (esli ne zagruzhen) ---\n",
        "if 'knowledge_map' not in locals() or not knowledge_map:\n",
        "    print(\"Attempting to load knowledge_map...\")\n",
        "    knowledge_map_filename = f\"{HF_MODEL_NAME}_knowledge_map.pkl\"\n",
        "    knowledge_map_filepath = DB_PATH / knowledge_map_filename\n",
        "    if knowledge_map_filepath.is_file():\n",
        "        try:\n",
        "            with open(knowledge_map_filepath, 'rb') as f:\n",
        "                knowledge_map = pickle.load(f)\n",
        "            print(f\"Loaded knowledge_map from {knowledge_map_filepath}\")\n",
        "            if not knowledge_map: raise ValueError(\"Loaded knowledge_map is empty.\")\n",
        "        except Exception as e:\n",
        "             raise FileNotFoundError(f\"Failed to load knowledge_map from {knowledge_map_filepath}: {e}\")\n",
        "    else:\n",
        "         raise FileNotFoundError(f\"knowledge_map file not found: {knowledge_map_filepath}. Run Cell 5 first.\")\n",
        "\n",
        "# --- Inicializacija Veector (esli nuzhno) ---\n",
        "if 'vec' not in locals() or vec is None:\n",
        "    print(\"Attempting to initialize Veector...\")\n",
        "    try:\n",
        "        # Importiruem zavisimosti dlja Veector\n",
        "        from core import Veector\n",
        "        from veectordb import VeectorDB # Importiruem zdes', tak kak mozhet byt' ne nuzhno vyshe\n",
        "        vec = Veector(db_dir=DB_PATH)\n",
        "        print(\"Veector initialized.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to initialize Veector: {e}\")\n",
        "# --- ИСПРАВЛЕНО: Убрана проверка is_connected() ---\n",
        "elif not hasattr(vec, 'db') or vec.db is None:\n",
        "     print(\"Attempting to re-initialize Veector DB object...\")\n",
        "     try:\n",
        "         from veectordb import VeectorDB\n",
        "         # Prosto pereinicializiruem db, esli on None ili otsutstvuet\n",
        "         vec.db = VeectorDB(db_dir=DB_PATH)\n",
        "         print(\"Veector DB object re-initialized.\")\n",
        "     except Exception as db_reinit_e:\n",
        "         raise AttributeError(f\"DB re-init failed: {db_reinit_e}\")\n",
        "# --- КОНЕЦ ИСПРАВЛЕНИЯ ---\n",
        "\n",
        "\n",
        "# --- Funkcija dlja sravnenija ---\n",
        "def compare_weights(param_name: str, transpose_hf: bool = False):\n",
        "    \"\"\"\n",
        "    Sravnivaet ves iz original'noj modeli HF s vesom iz Veector knowledge tensor.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Comparing: {param_name} ---\")\n",
        "    knowledge_id = knowledge_map.get(param_name)\n",
        "    if not knowledge_id:\n",
        "        print(f\"  ERROR: Knowledge ID not found in knowledge_map for '{param_name}'\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 1. Zagruzhaem original'nyj ves HF\n",
        "        # Ubedimsja, chto model' na CPU dlja konvertacii v NumPy\n",
        "        param = model.get_parameter(param_name)\n",
        "        hf_weight = param.data.cpu().to(torch.float32).numpy()\n",
        "        original_hf_shape = hf_weight.shape # Sohranim original'nuju formu\n",
        "\n",
        "        if transpose_hf:\n",
        "            if hf_weight.ndim == 2:\n",
        "                 hf_weight = hf_weight.T\n",
        "            else:\n",
        "                 print(f\"  WARN: Transpose requested for non-2D HF weight ({param_name}, shape {original_hf_shape}) - skipping transpose.\")\n",
        "\n",
        "        print(f\"  HF Weight (as float32, {'TRANSPOSED' if transpose_hf and hf_weight.ndim==2 else 'original'}): shape={hf_weight.shape}, dtype={hf_weight.dtype}, mean={np.mean(hf_weight):.4e}, std={np.std(hf_weight):.4e}\")\n",
        "\n",
        "        # 2. Zagruzhaem ves iz Veector (s dekvantovaniem, esli nuzhno)\n",
        "        # Predpolagaem, chto vec.db uzhe inicializirovan\n",
        "        veector_weight = vec.load_knowledge_tensor_data(knowledge_id)\n",
        "\n",
        "        if veector_weight is None:\n",
        "            print(f\"  ERROR: Failed to load Veector knowledge data for ID: {knowledge_id}\")\n",
        "            return\n",
        "\n",
        "        # load_knowledge_tensor_data dolzhen vernut' float32\n",
        "        print(f\"  Veector Weight (loaded as float32): shape={veector_weight.shape}, dtype={veector_weight.dtype}, mean={np.mean(veector_weight):.4e}, std={np.std(veector_weight):.4e}\")\n",
        "\n",
        "        # 3. Sravnivaem\n",
        "        if hf_weight.shape != veector_weight.shape:\n",
        "            print(f\"  ERROR: Shape mismatch! HF={hf_weight.shape}, Veector={veector_weight.shape}\")\n",
        "            # Dopolnitel'naja proverka: mozhet, nuzhno bylo transponirovat' HF, a my ne sdelali?\n",
        "            if not transpose_hf and hf_weight.ndim == 2 and hf_weight.T.shape == veector_weight.shape:\n",
        "                 print(f\"  INFO: Trying comparison after transposing original HF weight...\")\n",
        "                 hf_weight = hf_weight.T\n",
        "                 # Povtorjaem sravnenie posle transpozicii\n",
        "                 max_abs_diff = np.max(np.abs(hf_weight - veector_weight))\n",
        "                 mean_abs_diff = np.mean(np.abs(hf_weight - veector_weight))\n",
        "                 print(f\"  Comparison (float32, HF Transposed):\")\n",
        "                 print(f\"    Max Abs Difference:  {max_abs_diff:.4e}\")\n",
        "                 print(f\"    Mean Abs Difference: {mean_abs_diff:.4e}\")\n",
        "                 if max_abs_diff > 1e-2 or mean_abs_diff > 1e-3: print(f\"  !!! WARNING: Significant difference detected !!!\")\n",
        "\n",
        "            return # Vyhodim, esli formy vse ravno ne sovpali\n",
        "\n",
        "        # Esli formy sovpali srazu\n",
        "        max_abs_diff = np.max(np.abs(hf_weight - veector_weight))\n",
        "        mean_abs_diff = np.mean(np.abs(hf_weight - veector_weight))\n",
        "\n",
        "        print(f\"  Comparison (float32):\")\n",
        "        print(f\"    Max Abs Difference:  {max_abs_diff:.4e}\")\n",
        "        print(f\"    Mean Abs Difference: {mean_abs_diff:.4e}\")\n",
        "\n",
        "        # Proverka na bol'shie rashozhdenija\n",
        "        # Porogi primernye, mogut zaviset' ot tochnosti (float16 vs int8)\n",
        "        threshold = 1e-2 # Porog dlja maksimal'noj raznicy\n",
        "        mean_threshold = 1e-3 # Porog dlja srednej raznicy\n",
        "        if max_abs_diff > threshold or mean_abs_diff > mean_threshold:\n",
        "             print(f\"  !!! WARNING: Significant difference detected (Max > {threshold:.1e} or Mean > {mean_threshold:.1e}) !!!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR during comparison for '{param_name}': {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- Vyzyvaem sravnenie dlja neskol'kih kljuchevyh vesov ---\n",
        "\n",
        "# Embedding (ne transponiruem, kvantovan v int8)\n",
        "compare_weights(\"model.embed_tokens.weight\", transpose_hf=False)\n",
        "\n",
        "# Q-proekcija pervogo sloja (transponiruem, float16)\n",
        "compare_weights(\"model.layers.0.self_attn.q_proj.weight\", transpose_hf=True)\n",
        "\n",
        "# Gate-proekcija FFN pervogo sloja (transponiruem, float16)\n",
        "compare_weights(\"model.layers.0.mlp.gate_proj.weight\", transpose_hf=True)\n",
        "\n",
        "# Ves poslednego LayerNorm (ne transponiruem, float16)\n",
        "compare_weights(\"model.norm.weight\", transpose_hf=False)\n",
        "\n",
        "# LM Head (transponiruem, kvantovan v int8)\n",
        "compare_weights(\"lm_head.weight\", transpose_hf=True)\n",
        "\n",
        "# --- Dopolnitel'nye proverki (mozhno raskommentirovat') ---\n",
        "compare_weights(\"model.layers.0.self_attn.k_proj.weight\", transpose_hf=True)\n",
        "compare_weights(\"model.layers.0.self_attn.v_proj.weight\", transpose_hf=True)\n",
        "compare_weights(\"model.layers.0.self_attn.o_proj.weight\", transpose_hf=True)\n",
        "compare_weights(\"model.layers.0.mlp.up_proj.weight\", transpose_hf=True)\n",
        "compare_weights(\"model.layers.0.mlp.down_proj.weight\", transpose_hf=True)\n",
        "compare_weights(\"model.layers.0.input_layernorm.weight\", transpose_hf=False)\n",
        "compare_weights(\"model.layers.0.post_attention_layernorm.weight\", transpose_hf=False)\n",
        "\n",
        "print(\"\\n--- Tensor Verification Finished ---\")\n",
        "\n",
        "# Vazhno: zakryt' soedinenie s BD posle proverki, esli ono bylo otkryto zdes'\n",
        "# Ne zakryvaem avtomaticheski, tak kak 'vec' mozhet byt' ispol'zovan dalshe\n",
        "# if 'vec' in locals() and vec and hasattr(vec, 'db') and vec.db and vec.db.is_connected():\n",
        "#      print(\"\\nClosing DB connection after verification...\")\n",
        "#      vec.db.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry66m9_kQ2Y2",
        "outputId": "271a77e9-8904-4d85-89f0-21bf9cb418a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Cell 5.7: Verify Tensor Data ---\n",
            "\n",
            "--- Comparing: model.embed_tokens.weight ---\n",
            "  HF Weight (as float32, original): shape=(151936, 1536), dtype=float32, mean=3.8134e-04, std=3.0062e-02\n",
            "  Veector Weight (loaded as float32): shape=(151936, 1536), dtype=float32, mean=3.8103e-04, std=3.0068e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  1.2995e-03\n",
            "    Mean Abs Difference: 6.4934e-04\n",
            "\n",
            "--- Comparing: model.layers.0.self_attn.q_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 1536), dtype=float32, mean=1.7502e-05, std=5.3037e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 1536), dtype=float16, mean=1.7524e-05, std=5.3070e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.mlp.gate_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 8960), dtype=float32, mean=-4.8338e-07, std=3.8473e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 8960), dtype=float16, mean=-4.7684e-07, std=3.9093e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.norm.weight ---\n",
            "  HF Weight (as float32, original): shape=(1536,), dtype=float32, mean=2.3067e+00, std=4.1719e-01\n",
            "  Veector Weight (loaded as float32): shape=(1536,), dtype=float16, mean=2.3066e+00, std=4.1748e-01\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: lm_head.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 151936), dtype=float32, mean=4.5009e-04, std=2.9313e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 151936), dtype=float32, mean=4.5046e-04, std=2.9323e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  1.2918e-03\n",
            "    Mean Abs Difference: 6.4594e-04\n",
            "\n",
            "--- Comparing: model.layers.0.self_attn.k_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 256), dtype=float32, mean=-8.4400e-05, std=6.1757e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 256), dtype=float16, mean=-8.4400e-05, std=6.1768e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.self_attn.v_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 256), dtype=float32, mean=-1.6609e-05, std=3.0817e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 256), dtype=float16, mean=-1.6630e-05, std=3.0823e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.self_attn.o_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 1536), dtype=float32, mean=3.3827e-06, std=4.6337e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 1536), dtype=float16, mean=3.3975e-06, std=4.6417e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.mlp.up_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(1536, 8960), dtype=float32, mean=-5.6223e-06, std=3.4311e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536, 8960), dtype=float16, mean=-5.6028e-06, std=3.2410e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.mlp.down_proj.weight ---\n",
            "  HF Weight (as float32, TRANSPOSED): shape=(8960, 1536), dtype=float32, mean=1.2630e-05, std=3.6591e-02\n",
            "  Veector Weight (loaded as float32): shape=(8960, 1536), dtype=float16, mean=1.2636e-05, std=3.5522e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.input_layernorm.weight ---\n",
            "  HF Weight (as float32, original): shape=(1536,), dtype=float32, mean=2.1499e-01, std=1.3617e-01\n",
            "  Veector Weight (loaded as float32): shape=(1536,), dtype=float16, mean=2.1497e-01, std=1.3611e-01\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Comparing: model.layers.0.post_attention_layernorm.weight ---\n",
            "  HF Weight (as float32, original): shape=(1536,), dtype=float32, mean=2.6350e-01, std=6.3033e-02\n",
            "  Veector Weight (loaded as float32): shape=(1536,), dtype=float16, mean=2.6343e-01, std=6.2988e-02\n",
            "  Comparison (float32):\n",
            "    Max Abs Difference:  0.0000e+00\n",
            "    Mean Abs Difference: 0.0000e+00\n",
            "\n",
            "--- Tensor Verification Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/db/g500"
      ],
      "metadata": {
        "id": "Ln-wXut3MyS3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Define Processor Tensors (KV Placeholders) ===\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import traceback\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# --- Версия ---\n",
        "# Obnovili versiju, chtoby otrazit' izmenenija dlja KV\n",
        "CONVERTER_CELL6_VERSION = \"Creating ALL Layer Processors v0.7.13 - KV Placeholders\"\n",
        "# --- Конец Версии ---\n",
        "\n",
        "print(f\"\\n--- Running Converter Cell 6 v{CONVERTER_CELL6_VERSION} ---\")\n",
        "start_cell6_time = time.time()\n",
        "\n",
        "# --- Загрузка промежуточных данных ---\n",
        "print(\"--- Loading Intermediate Data for Cell 6 ---\")\n",
        "# Predpolagaem, chto HF_MODEL_NAME i DB_PATH opredeleny rannee\n",
        "if 'HF_MODEL_NAME' not in locals() or not HF_MODEL_NAME:\n",
        "    raise NameError(\"HF_MODEL_NAME not defined.\")\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "    raise NameError(\"DB_PATH not defined or invalid.\")\n",
        "\n",
        "intermediate_filename = f\"{HF_MODEL_NAME}_cell6_input_data.pkl\"\n",
        "intermediate_filepath = DB_PATH / intermediate_filename\n",
        "cell6_input_data: Optional[Dict] = None\n",
        "knowledge_map: Optional[Dict[str, str]] = None\n",
        "model_config: Optional[Any] = None # Tip zavisit ot transformers\n",
        "\n",
        "try:\n",
        "    print(f\"Attempting to load data from: {intermediate_filepath}\")\n",
        "    if not intermediate_filepath.is_file():\n",
        "        raise FileNotFoundError(\"Intermediate data file not found. Run Cell 5.5.\")\n",
        "    with open(intermediate_filepath, 'rb') as f:\n",
        "        cell6_input_data = pickle.load(f)\n",
        "\n",
        "    if not isinstance(cell6_input_data, dict):\n",
        "        raise TypeError(\"Loaded intermediate data is not a dictionary.\")\n",
        "\n",
        "    knowledge_map = cell6_input_data.get('knowledge_map')\n",
        "    model_config = cell6_input_data.get('model_config')\n",
        "    loaded_hf_model_name = cell6_input_data.get('hf_model_name')\n",
        "    loaded_db_path_str = cell6_input_data.get('db_path')\n",
        "\n",
        "    if not isinstance(knowledge_map, dict) or not knowledge_map:\n",
        "        raise ValueError(\"'knowledge_map' missing or invalid in loaded data.\")\n",
        "    if model_config is None:\n",
        "        raise ValueError(\"'model_config' missing in loaded data.\")\n",
        "    if loaded_hf_model_name != HF_MODEL_NAME:\n",
        "        print(f\"Warning: HF_MODEL_NAME mismatch ('{HF_MODEL_NAME}' vs loaded\"\n",
        "              f\" '{loaded_hf_model_name}'). Using '{HF_MODEL_NAME}'.\")\n",
        "    if loaded_db_path_str != str(DB_PATH.resolve()):\n",
        "        print(f\"Warning: DB_PATH mismatch ('{str(DB_PATH.resolve())}' vs loaded\"\n",
        "              f\" '{loaded_db_path_str}'). Using '{str(DB_PATH.resolve())}'.\")\n",
        "\n",
        "    print(\"Intermediate data loaded successfully.\")\n",
        "    print(f\"  Knowledge map entries: {len(knowledge_map)}\")\n",
        "    print(f\"  Model Config Type: {type(model_config)}\")\n",
        "\n",
        "except Exception as load_e:\n",
        "    print(f\"---!!! FATAL ERROR loading intermediate data: {load_e} !!!---\")\n",
        "    raise\n",
        "\n",
        "# --- Poluchaem parametry iz konfiga ---\n",
        "required_attrs = [\n",
        "    'num_hidden_layers', 'num_attention_heads',\n",
        "    'num_key_value_heads', 'hidden_size'\n",
        "]\n",
        "if not all(hasattr(model_config, attr) for attr in required_attrs):\n",
        "    raise AttributeError(\"Loaded model_config missing required attributes.\")\n",
        "\n",
        "num_layers: int = model_config.num_hidden_layers\n",
        "num_attention_heads: int = model_config.num_attention_heads\n",
        "num_key_value_heads: int = model_config.num_key_value_heads\n",
        "hidden_size: int = model_config.hidden_size\n",
        "if num_attention_heads == 0:\n",
        "    raise ValueError(\"num_attention_heads cannot be zero.\")\n",
        "head_dim: int = hidden_size // num_attention_heads\n",
        "if hidden_size % num_attention_heads != 0:\n",
        "     print(f\"WARN: hidden_size {hidden_size} not perfectly divisible by num_attention_heads {num_attention_heads}\")\n",
        "\n",
        "print(f\"Using config: L={num_layers}, H={num_attention_heads}, \"\n",
        "      f\"H_KV={num_key_value_heads}, HDim={head_dim}\")\n",
        "# --- Konec Zagruzki ---\n",
        "\n",
        "\n",
        "# --- Importy i Proverki Versij ---\n",
        "# Ozhidaemye versii: core >= 0.7.2, tensors >= 0.7.6, operations >= 0.8.3\n",
        "TENSORS_VERSION_REQ_CELL6 = \"0.7.6\"\n",
        "CORE_VERSION_REQ_CELL6 = \"0.7.2\" # Versija s obrabotkoj kortezha\n",
        "OPERATIONS_VERSION_REQ_CELL6 = \"0.8.3\" # Versija s novymi operacijami\n",
        "try:\n",
        "    from tensors import (\n",
        "        TENSORS_VERSION, TensorCoordinate, create_tensor, MetadataTuple,\n",
        "        validate_tensor, validate_tensor_tuple, get_tensor_hash,\n",
        "        get_tensor_metadata, get_tensor_coord, get_tensor_tags,\n",
        "        TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, TAG_FUNC_ATTENTION,\n",
        "        TAG_FUNC_FFN, TAG_FUNC_LINEAR, TAG_COMP_LAYERNORM, TAG_MODEL_DEEPSEEK,\n",
        "        tag_layer, GROUP_IDX_QWEN_PROCESSOR, TAG_COMP_EMBEDDING,\n",
        "        TAG_COMP_WEIGHTS, TAG_COMP_BIAS, TAG_COMP_ATTN_Q, TAG_COMP_ATTN_K,\n",
        "        TAG_COMP_ATTN_V, TAG_COMP_ATTN_O, TAG_COMP_FFN_GATE, TAG_COMP_FFN_UP,\n",
        "        TAG_COMP_FFN_DOWN, TAG_COMP_LM_HEAD, TAG_PREC_INT8, TAG_PREC_FLOAT16\n",
        "    )\n",
        "    if TENSORS_VERSION < TENSORS_VERSION_REQ_CELL6:\n",
        "        raise ImportError(f\"Requires tensors v{TENSORS_VERSION_REQ_CELL6}+\")\n",
        "    from core import Veector, CORE_VERSION\n",
        "    if CORE_VERSION < CORE_VERSION_REQ_CELL6:\n",
        "        raise ImportError(f\"Requires core v{CORE_VERSION_REQ_CELL6}+\")\n",
        "    from operations import OPERATIONS_VERSION\n",
        "    if OPERATIONS_VERSION < OPERATIONS_VERSION_REQ_CELL6:\n",
        "        raise ImportError(f\"Requires operations v{OPERATIONS_VERSION_REQ_CELL6}+\")\n",
        "\n",
        "    # --- Lokal'noe opredelenie OP kodov (VKLUCHAJA NOVYE) ---\n",
        "    OP_SUM=[0,0,0]; OP_SUBTRACT=[0,0,1]; OP_ADD=[0,0,2]; OP_MULTIPLY=[0,1,0]\n",
        "    OP_DIVIDE=[0,1,1]; OP_SQRT=[0,2,0]; OP_POWER=[0,2,1]; OP_ABS=[0,3,0]\n",
        "    OP_MOD=[0,5,0]; OP_FLOOR=[0,6,0]; OP_CEIL=[0,6,1]; OP_SIN=[1,0,0]\n",
        "    OP_COS=[1,0,1]; OP_TAN=[1,1,0]; OP_COT=[1,1,1]; OP_ASIN=[1,2,0]\n",
        "    OP_ACOS=[1,2,1]; OP_ATAN=[1,3,0]; OP_GREATER=[2,0,0]; OP_EQUAL=[2,0,1]\n",
        "    OP_AND=[2,1,0]; OP_OR=[2,1,1]; OP_NOT=[2,2,0]; OP_XOR=[2,3,0]\n",
        "    OP_NAND=[2,4,0]; OP_NOR=[2,4,1]; OP_IF=[3,0,0]; OP_LOOP_MULT=[4,0,0]\n",
        "    OP_CHOICE=[7,0,0]; OP_RAND_UNIFORM=[5,1,0]; OP_RAND_NORMAL=[5,1,1]\n",
        "    OP_MEDIAN=[5,2,0]; OP_PRINT=[8,0,0]; OP_IDENTITY=[9,0,0]\n",
        "    OP_TRIGGER_REASON=[10,0,0]; OP_DFS=[15,0,0]; OP_MEAN=[16,0,0]\n",
        "    OP_STDDEV=[16,1,0]; OP_RELU=[18,0,0]; OP_SIGMOID=[18,1,0]\n",
        "    OP_SOFTMAX=[18,2,0]; OP_LEAKY_RELU=[18,3,0]; OP_SILU=[18,4,0]\n",
        "    OP_GELU=[40,5,0]; OP_EXP_SMOOTHING=[19,0,0]; OP_NORMALIZE_01=[20,0,0]\n",
        "    OP_INTERPOLATE=[20,1,0]; OP_LAYER_NORM=[40,1,0]; OP_BATCH_NORM=[40,4,0]\n",
        "    OP_DROPOUT=[40,3,0]; OP_MATRIX_MULTIPLY=[30,0,0]; OP_DETERMINANT=[30,1,0]\n",
        "    OP_EIGENVALUES=[30,2,0]; OP_CONVOLUTION=[30,3,0]; OP_TRANSPOSE=[30,4,0]\n",
        "    OP_INVERSE=[30,5,0]; OP_TRACE=[30,6,0]; OP_ATTENTION_MULTIHEAD=[40,2,0]\n",
        "    OP_EMBEDDING_LOOKUP=[40,6,0]; OP_APPLY_ROPE=[40,7,0]; OP_GET_Q_ROT=[40,7,1]\n",
        "    OP_GET_K_ROT=[40,7,2]; OP_RESHAPE_HEADS=[40,9,0]; OP_REPEAT_KV_HEADS=[40,9,1]\n",
        "    OP_SCALED_DOT_PROD_ATTN=[40,9,2]; OP_MERGE_HEADS=[40,9,3]; OP_ADD_BIAS=[0,0,3]\n",
        "    OP_UPDATE_KV_CACHE = [40, 10, 0]    # <<< Dobavleno\n",
        "    OP_CREATE_CAUSAL_MASK = [40, 10, 1] # <<< Dobavleno\n",
        "    OP_RESIDUAL_ADD=OP_ADD; OP_LINEAR=OP_MATRIX_MULTIPLY; OP_FINAL_NORM=OP_LAYER_NORM\n",
        "    OP_LINEAR_HEAD=OP_LINEAR; OP_QUANTUM_HADAMARD=[50,0,0]; OP_QUANTUM_PAULI_X=[50,0,1]\n",
        "    OP_QUANTUM_CNOT=[50,1,0]; OP_QUANTUM_MEASURE=[50,2,0]; OP_QUANTUM_SUPERPOS=[50,3,0]\n",
        "    OP_QUANTUM_ENTANGLE=[50,4,0]; META_OP_CATEGORY=99; OP_STORE=[99,0,0]\n",
        "    OP_LOAD=[99,0,1]; OP_LOAD_INITIAL_INPUT=[99,0,3]; OP_DEBUG_CONTEXT=[99,1,0]\n",
        "    OP_MAKE_TUPLE = [99, 2, 0]         # <<< Dobavleno\n",
        "    # --- Konec OP kodov ---\n",
        "\n",
        "except ImportError as e: print(f\"FATAL ERROR imports: {e}\"); raise\n",
        "except Exception as e_other: print(f\"FATAL ERROR imports: {e_other}\"); traceback.print_exc(); raise\n",
        "\n",
        "\n",
        "# --- Ochistka staroj karty processorov ---\n",
        "processor_map_filename = f\"{HF_MODEL_NAME}_proc_map.pkl\"\n",
        "processor_map_filepath = DB_PATH / processor_map_filename\n",
        "if processor_map_filepath.is_file():\n",
        "    try:\n",
        "        print(f\"Deleting existing processor map: {processor_map_filepath}\")\n",
        "        os.remove(processor_map_filepath)\n",
        "    except OSError as e:\n",
        "        print(f\"Warning: Could not delete existing processor map: {e}\")\n",
        "\n",
        "# --- Inicializacija Veector i dr. ---\n",
        "print(\"--- Defining ALL Processor Tensors (Embedding, Layers 0-27, FinalNorm, LMHead) ---\")\n",
        "if 'vec' not in locals() or vec is None or not hasattr(vec, 'db') or vec.db is None:\n",
        "     print(\"Re-initializing Veector for Cell 6...\")\n",
        "     try:\n",
        "         vec = Veector(db_dir=DB_PATH)\n",
        "         print(f\"Veector core v{CORE_VERSION} initialized for Cell 6\")\n",
        "     except Exception as e: print(f\"FATAL: Veector re-init failed: {e}\"); raise\n",
        "else:\n",
        "     print(\"Using existing Veector object.\")\n",
        "\n",
        "processor_errors = 0\n",
        "processor_map: Dict[str, str] = {}\n",
        "knowledge_map_by_name = knowledge_map # Ispol'zuem zagruzhennuju kartu\n",
        "def find_knowledge_id(pattern: str) -> Optional[str]:\n",
        "    return knowledge_map_by_name.get(pattern)\n",
        "\n",
        "processor_group_idx = GROUP_IDX_QWEN_PROCESSOR # 500\n",
        "model_tag = TAG_MODEL_DEEPSEEK # 12\n",
        "prec_tag_weights = TAG_PREC_FLOAT16 # 21\n",
        "\n",
        "# --- 1. Embedding Processor ---\n",
        "proc_name = \"Embedding Processor\"\n",
        "print(f\"\\n--- Defining {proc_name} ---\")\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=0)\n",
        "    tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_EMBED_LOOKUP, model_tag]\n",
        "    param_name = \"embedding_matrix\"; kn_tags = [TAG_COMP_EMBEDDING, model_tag, TAG_COMP_WEIGHTS, TAG_PREC_INT8] # Prec INT8\n",
        "    interface = {\"inputs\": [{\"name\":\"token_ids\",\"dtype\":\"int64\"}], \"outputs\": [{\"name\":\"hidden_states\",\"dtype\":\"float16\"}], \"knowledge_needed\": [{\"param_name\": param_name, \"tags\": kn_tags}]}\n",
        "    ops_sequences = {'default': [ [OP_EMBEDDING_LOOKUP, {\"embedding_matrix\": param_name}] ]}\n",
        "    tensor_structure = vec.create_tensor(coord=coord, tensor_type=\"processor\", tags=tags, interface=interface, ops_sequences=ops_sequences, status=\"active\", name_id=-1)\n",
        "    if not validate_tensor(tensor_structure): raise ValueError(\"Invalid list structure (Embed)\")\n",
        "    proc_id = vec.save_tensor(tensor_structure);\n",
        "    if proc_id: processor_map[\"embedding\"] = proc_id; print(f\"  SUCCESS: Saved {proc_name}: {proc_id}\")\n",
        "    else: processor_errors += 1; print(f\"  ERROR saving {proc_name}\")\n",
        "except Exception as e: print(f\"  ERROR during {proc_name}: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- 2. Cikl po Slojam Transformera ---\n",
        "print(f\"\\n--- Defining Transformer Layer Processors (0 to {num_layers-1}) ---\")\n",
        "for layer_idx in range(num_layers):\n",
        "    # --- Nachalo bloka dlja Attention Processor L{layer_idx} ---\n",
        "    proc_attn_name = f\"Attention Processor L{layer_idx}\"\n",
        "    print(f\"\\n--- Defining {proc_attn_name} ---\")\n",
        "    layer_tag = tag_layer(layer_idx)\n",
        "\n",
        "    # --- Opredeljaem neobhodimye znanija dlja etogo processora ---\n",
        "    kn_defs_attn = [\n",
        "        {\"p\":\"norm_weight\", \"t\":[TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.input_layernorm.weight\"},\n",
        "        {\"p\":\"q_weights\",   \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"},\n",
        "        {\"p\":\"q_bias\",      \"t\":[TAG_COMP_ATTN_Q, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.q_proj.bias\", \"opt\": True},\n",
        "        {\"p\":\"k_weights\",   \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"},\n",
        "        {\"p\":\"k_bias\",      \"t\":[TAG_COMP_ATTN_K, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.k_proj.bias\", \"opt\": True},\n",
        "        {\"p\":\"v_weights\",   \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"},\n",
        "        {\"p\":\"v_bias\",      \"t\":[TAG_COMP_ATTN_V, layer_tag, model_tag, TAG_COMP_BIAS, prec_tag_weights],    \"f\":f\"model.layers.{layer_idx}.self_attn.v_proj.bias\", \"opt\": True},\n",
        "        {\"p\":\"o_weights\",   \"t\":[TAG_COMP_ATTN_O, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\":f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"},\n",
        "    ]\n",
        "    knowledge_needs_attn = [] # Inicializiruem pustoj spisok\n",
        "    missing_essential_attn = False\n",
        "    for kdef in kn_defs_attn:\n",
        "        kid = find_knowledge_id(kdef[\"f\"])\n",
        "        is_opt = kdef.get(\"opt\", False)\n",
        "        if kid:\n",
        "            knowledge_needs_attn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"optional\": is_opt})\n",
        "        elif not is_opt:\n",
        "            missing_essential_attn = True\n",
        "            processor_errors += 1\n",
        "            print(f\"      ERROR: Missing Attn L{layer_idx} knowledge: {kdef['p']} (pattern: {kdef['f']})\")\n",
        "\n",
        "    # --- Opredeljaem interface s vhodami/vyhodami dlja KV kjesha ---\n",
        "    interface_attn = {\n",
        "        \"inputs\": [\n",
        "            {\"name\": \"hidden_state_in\"},\n",
        "            {\"name\": \"residual_input\"},\n",
        "            {\"name\": \"position_ids\"},\n",
        "            {\"name\": \"attention_mask\", \"optional\": True}, # Budet kauzal'naja maska\n",
        "            {\"name\": \"past_key\", \"optional\": True},\n",
        "            {\"name\": \"past_value\", \"optional\": True},\n",
        "            {\"name\": \"start_pos\", \"dtype\": \"int\", \"optional\": True},\n",
        "            {\"name\": \"total_seq_len\", \"dtype\": \"int\", \"optional\": True}, # Nuzhno dlja kauzal'noj maski\n",
        "        ],\n",
        "        \"outputs\": [\n",
        "            # Vyhod teper' kortezh: (attn_output, new_key, new_value)\n",
        "            {\"name\": \"attn_processor_output_tuple\"}\n",
        "        ],\n",
        "        \"knowledge_needed\": knowledge_needs_attn\n",
        "    }\n",
        "\n",
        "    # --- Opredeljaem posledovatel'nost' operacij s KV keshom ---\n",
        "    ops_sequences_attn = {'default': [\n",
        "        # --- Nachalo bloka Attention ---\n",
        "        [OP_STORE, 'attn_residual_input'],                      # 0: Sohranjaem vhod dlja residual connection\n",
        "        [OP_LAYER_NORM, {\"norm_weight\": \"norm_weight\"}],       # 1: Normalizacija vhoda\n",
        "        [OP_STORE, 'norm_output'],                              # 2: Sohranjaem normalizovannyj vyhod\n",
        "\n",
        "        # --- Vychislenie Q (dlja tekushhego shaga) ---\n",
        "        [OP_LOAD, 'norm_output'],                               # 3\n",
        "        [OP_LINEAR, {\"weights\": \"q_weights\", \"bias\": \"q_bias\"}],# 4\n",
        "        [OP_RESHAPE_HEADS, {\"num_heads\": num_attention_heads}], # 5\n",
        "        [OP_STORE, 'q_proj_4d'],                                # 6\n",
        "\n",
        "        # --- Vychislenie K (dlja tekushhego shaga) ---\n",
        "        [OP_LOAD, 'norm_output'],                               # 7\n",
        "        [OP_LINEAR, {\"weights\": \"k_weights\", \"bias\": \"k_bias\"}],# 8\n",
        "        [OP_RESHAPE_HEADS, {\"num_heads\": num_key_value_heads}], # 9\n",
        "        [OP_STORE, 'k_proj_4d'],                                # 10\n",
        "\n",
        "        # --- Vychislenie V (dlja tekushhego shaga) ---\n",
        "        [OP_LOAD, 'norm_output'],                               # 11\n",
        "        [OP_LINEAR, {\"weights\": \"v_weights\", \"bias\": \"v_bias\"}],# 12\n",
        "        [OP_RESHAPE_HEADS, {\"num_heads\": num_key_value_heads}], # 13\n",
        "        [OP_STORE, 'v_proj_4d'],                                # 14\n",
        "\n",
        "        # --- Obnovlenie V Kjesha ---\n",
        "        [OP_LOAD, 'past_value'], # Zagruzhaem staryj kesh V iz konteksta\n",
        "        [OP_UPDATE_KV_CACHE, {\"cache\": \"past_value\", \"new_values\": \"v_proj_4d\", \"start_pos\": \"start_pos\"}], # 15 Obnovljaem kesh V tekushhimi znachenijami\n",
        "        [OP_STORE, 'v_cache_out'],                              # 16 Sohranjaem obnovlennyj kesh V\n",
        "\n",
        "        # --- RoPE (Primenjaetsja k Q i K tekushhego shaga) ---\n",
        "        [OP_LOAD, 'q_proj_4d'], [OP_STORE, 'q_for_rope'],       # 17, 18\n",
        "        [OP_LOAD, 'k_proj_4d'], [OP_STORE, 'k_for_rope'],       # 19, 20\n",
        "        [OP_APPLY_ROPE, {\"q\": \"q_for_rope\", \"k\": \"k_for_rope\", \"position_ids\": \"position_ids\", \"head_dim\": head_dim }], # 21\n",
        "        [OP_STORE, 'rope_output'],                              # 22\n",
        "        [OP_LOAD, 'rope_output'], [OP_GET_Q_ROT], [OP_STORE, 'q_rot'],# 23,24,25 (q_rot dlja tekushhego shaga)\n",
        "        [OP_LOAD, 'rope_output'], [OP_GET_K_ROT], [OP_STORE, 'k_rot'],# 26,27,28 (k_rot dlja tekushhego shaga)\n",
        "\n",
        "        # --- Obnovlenie K Kjesha (POSLE RoPE) ---\n",
        "        [OP_LOAD, 'past_key'],   # Zagruzhaem staryj kesh K iz konteksta\n",
        "        [OP_UPDATE_KV_CACHE, {\"cache\": \"past_key\", \"new_values\": \"k_rot\", \"start_pos\": \"start_pos\"}], # 29 Obnovljaem kesh K tekushhimi znachenijami (posle RoPE)\n",
        "        [OP_STORE, 'k_cache_out'],                              # 30 Sohranjaem obnovlennyj kesh K\n",
        "\n",
        "        # --- Podgotovka k SDPA ---\n",
        "        # Sozdaem kauzal'nuju masku dlja tekushhej polnoj dliny\n",
        "        # Predpolagaem, chto 'total_seq_len' peredan v kontexte\n",
        "        [OP_CREATE_CAUSAL_MASK, {\"size\": \"total_seq_len\"}],     # 31\n",
        "        [OP_STORE, 'causal_mask'],                              # 32\n",
        "\n",
        "        # --- Attention (SDPA) - Teper' ispol'zuem KESH i kauzal'nuju masku ---\n",
        "        # VAZHNO: SDPA dolzhen umet' rabotat' s polnym keshem K/V i kauzal'noj maskoj.\n",
        "        # Predpolagaem, chto on beret q_rot (tekushhij) i k_cache_out/v_cache_out (polnyj kesh)\n",
        "        # i pravil'no ispol'zuet kauzal'nuju masku.\n",
        "        # Eto mozhet potrebovat' modifikacii samoj funkcii scaled_dot_product_attention\n",
        "        # ili dobavlenija shagov OP_SLICE_CACHE pered ee vyzovom.\n",
        "        [OP_SCALED_DOT_PROD_ATTN, {\n",
        "            \"query\": \"q_rot\",           # Q tekushhego shaga (posle RoPE)\n",
        "            \"key\": \"k_cache_out\",       # Polnyj obnovlennyj K kesh\n",
        "            \"value\": \"v_cache_out\",     # Polnyj obnovlennyj V kesh\n",
        "            \"attention_mask\": \"causal_mask\" # Kauzal'naja maska\n",
        "        }],                                                     # 33\n",
        "        [OP_STORE, 'attn_context'],                             # 34\n",
        "\n",
        "        # --- Output Projection ---\n",
        "        [OP_LOAD, 'attn_context'],                              # 35\n",
        "        [OP_LINEAR, {\"weights\": \"o_weights\"}],                  # 36\n",
        "        [OP_STORE, 'attn_output_proj'],                         # 37\n",
        "\n",
        "        # --- Residual Add ---\n",
        "        [OP_LOAD, 'attn_residual_input'], [OP_STORE, 'input_a'],# 38, 39\n",
        "        [OP_LOAD, 'attn_output_proj'], [OP_STORE, 'input_b'],   # 40, 41\n",
        "        [OP_ADD, {\"input_a\": \"input_a\", \"input_b\": \"input_b\"}], # 42\n",
        "        [OP_STORE, 'final_attn_output'],                        # 43\n",
        "\n",
        "        # --- Vozvrat rezul'tata i kjesha ---\n",
        "        [OP_MAKE_TUPLE, { # Formiruem kortezh dlja vozvrata\n",
        "            \"elem0\": \"final_attn_output\", # Rezul'tat sloja\n",
        "            \"elem1\": \"k_cache_out\",       # Obnovlennyj K kesh\n",
        "            \"elem2\": \"v_cache_out\"        # Obnovlennyj V kesh\n",
        "        }]                                                      # 44\n",
        "    ]}\n",
        "\n",
        "    # --- Sozdanie i sohranenie processora Attention ---\n",
        "    try:\n",
        "        if not missing_essential_attn:\n",
        "            coord_attn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=0)\n",
        "            tags_attn = [TAG_TYPE_PROCESSOR, TAG_FUNC_ATTENTION, layer_tag, model_tag]\n",
        "            tensor_structure_attn = vec.create_tensor(\n",
        "                coord=coord_attn, tensor_type=\"processor\", tags=tags_attn,\n",
        "                interface=interface_attn, # Obnovlennyj interface\n",
        "                ops_sequences=ops_sequences_attn, # Obnovlennaja posledovatel'nost'\n",
        "                status=\"active\", name_id=-1\n",
        "            )\n",
        "            if not validate_tensor(tensor_structure_attn): raise ValueError(\"Invalid list structure (Attn)\")\n",
        "            proc_id_attn = vec.save_tensor(tensor_structure_attn);\n",
        "            if proc_id_attn:\n",
        "                processor_map[f\"attn_{layer_idx}\"] = proc_id_attn\n",
        "                print(f\"  SUCCESS: Saved {proc_attn_name}: {proc_id_attn}\")\n",
        "            else:\n",
        "                processor_errors += 1\n",
        "                print(f\"  ERROR saving {proc_attn_name}\")\n",
        "        else:\n",
        "            print(f\"  Skipping {proc_attn_name} due to missing essential knowledge.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR during {proc_attn_name} definition: {e}\")\n",
        "        traceback.print_exc()\n",
        "        processor_errors += 1\n",
        "    # --- Konec bloka dlja Attention Processor L{layer_idx} ---\n",
        "\n",
        "\n",
        "    # --- FFN Processor ---\n",
        "    # (Ostavljaem FFN processor bez izmenenij na etom etape)\n",
        "    proc_ffn_name = f\"FFN Processor L{layer_idx}\"\n",
        "    print(f\"\\n--- Defining {proc_ffn_name} ---\")\n",
        "    ops_sequences_ffn = {'default': [ [OP_STORE, 'ffn_residual_input'], [OP_LAYER_NORM, {\"norm_weight\": \"norm_weight\"}], [OP_STORE, 'norm_out'], [OP_LOAD, 'norm_out'], [OP_LINEAR, {\"weights\": \"gate_weights\"}], [OP_STORE, 'gate_proj'], [OP_LOAD, 'norm_out'], [OP_LINEAR, {\"weights\": \"up_weights\"}], [OP_STORE, 'up_proj'], [OP_LOAD, 'gate_proj'], [OP_SILU], [OP_STORE, 'silu_gate'], [OP_LOAD, 'silu_gate'], [OP_STORE, 'factor1'], [OP_LOAD, 'up_proj'], [OP_STORE, 'factor2'], [OP_MULTIPLY, {\"factor1\": \"factor1\", \"factor2\": \"factor2\"}], [OP_STORE, 'activated'], [OP_LOAD, 'activated'], [OP_LINEAR, {\"weights\": \"down_weights\"}], [OP_STORE, 'ffn_out'], [OP_LOAD, 'ffn_residual_input'], [OP_STORE, 'input_a'], [OP_LOAD, 'ffn_out'], [OP_STORE, 'input_b'], [OP_ADD, {\"input_a\": \"input_a\", \"input_b\": \"input_b\"}] ]}\n",
        "    try:\n",
        "        coord_ffn = TensorCoordinate(layer=layer_idx, group=processor_group_idx, nest=0, x=1)\n",
        "        tags_ffn = [TAG_TYPE_PROCESSOR, TAG_FUNC_FFN, layer_tag, model_tag]\n",
        "        kn_defs_ffn = [\n",
        "            {\"p\": \"norm_weight\",  \"t\": [TAG_COMP_LAYERNORM, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights], \"f\": f\"model.layers.{layer_idx}.post_attention_layernorm.weight\"},\n",
        "            {\"p\": \"gate_weights\", \"t\": [TAG_COMP_FFN_GATE, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.gate_proj.weight\"},\n",
        "            {\"p\": \"up_weights\",   \"t\": [TAG_COMP_FFN_UP, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],    \"f\": f\"model.layers.{layer_idx}.mlp.up_proj.weight\"},\n",
        "            {\"p\": \"down_weights\", \"t\": [TAG_COMP_FFN_DOWN, layer_tag, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights],  \"f\": f\"model.layers.{layer_idx}.mlp.down_proj.weight\"},\n",
        "        ]\n",
        "        knowledge_needs_ffn = []; missing_essential_ffn = False\n",
        "        for kdef in kn_defs_ffn:\n",
        "            kid = find_knowledge_id(kdef[\"f\"]); is_opt = kdef.get(\"opt\", False)\n",
        "            if kid: knowledge_needs_ffn.append({\"param_name\": kdef[\"p\"], \"tags\": kdef[\"t\"], \"optional\": is_opt})\n",
        "            elif not is_opt: missing_essential_ffn = True; processor_errors += 1; print(f\"      ERROR: Missing FFN L{layer_idx} knowledge: {kdef['p']}\")\n",
        "        if not missing_essential_ffn:\n",
        "            interface_ffn = { \"inputs\": [{\"name\":\"attn_output\"}, {\"name\":\"residual_input\"}], \"outputs\": [{\"name\":\"ffn_output\"}], \"knowledge_needed\": knowledge_needs_ffn }\n",
        "            tensor_structure_ffn = vec.create_tensor( coord=coord_ffn, tensor_type=\"processor\", tags=tags_ffn, interface=interface_ffn, ops_sequences=ops_sequences_ffn, status=\"active\", name_id=-1)\n",
        "            if not validate_tensor(tensor_structure_ffn): raise ValueError(\"Invalid list structure (FFN)\")\n",
        "            proc_id_ffn = vec.save_tensor(tensor_structure_ffn)\n",
        "            if proc_id_ffn: processor_map[f\"ffn_{layer_idx}\"] = proc_id_ffn; print(f\"  SUCCESS: Saved {proc_ffn_name}: {proc_id_ffn}\")\n",
        "            else: processor_errors += 1; print(f\"  ERROR saving {proc_ffn_name}\")\n",
        "        else: print(f\"  Skipping {proc_ffn_name}\")\n",
        "    except Exception as e: print(f\"  ERROR during {proc_ffn_name} definition: {e}\"); traceback.print_exc(); processor_errors += 1\n",
        "# --- KONEC CIKLA PO SLOJAM ---\n",
        "\n",
        "\n",
        "# --- 3. Final Norm Processor ---\n",
        "# ... (bez izmenenij) ...\n",
        "proc_name = \"Final Norm Processor\"; print(f\"\\n--- Defining {proc_name} ---\")\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=1); tags = [TAG_TYPE_PROCESSOR, TAG_COMP_LAYERNORM, model_tag]; prec_tag_weights = TAG_PREC_FLOAT16\n",
        "    kn_tags = [TAG_COMP_LAYERNORM, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights]; knowledge_needs = []; pattern = \"model.norm.weight\"; kid = find_knowledge_id(pattern)\n",
        "    print(f\"  Checking knowledge: Param: norm_weight, Pattern: '{pattern}', Found: {'Yes' if kid else 'No'}\")\n",
        "    if kid: knowledge_needs.append({\"param_name\": \"norm_weight\", \"tags\": kn_tags, \"optional\": False})\n",
        "    else: processor_errors += 1; print(f\"  ERROR: Essential knowledge missing!\")\n",
        "    if knowledge_needs:\n",
        "        interface = { \"inputs\": [{\"name\":\"final_hidden_state\"}], \"outputs\": [{\"name\":\"final_normed_state\"}], \"knowledge_needed\": knowledge_needs }\n",
        "        ops_sequences = {'default': [[OP_FINAL_NORM, {\"norm_weight\": \"norm_weight\"}]]}\n",
        "        tensor_structure = vec.create_tensor( coord=coord, tensor_type=\"processor\", tags=tags, interface=interface, ops_sequences=ops_sequences, status=\"active\", name_id=-1)\n",
        "        if not validate_tensor(tensor_structure): raise ValueError(\"Invalid list structure (Final Norm)\")\n",
        "        proc_id = vec.save_tensor(tensor_structure);\n",
        "        if proc_id: processor_map[\"final_norm\"] = proc_id; print(f\"  SUCCESS: Saved {proc_name}: {proc_id}\")\n",
        "        else: processor_errors += 1; print(f\"  ERROR saving {proc_name}\")\n",
        "    else: print(f\"  Skipping {proc_name}\")\n",
        "except Exception as e: print(f\"  ERROR during {proc_name} definition: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- 4. LM Head Processor ---\n",
        "# ... (bez izmenenij) ...\n",
        "proc_name = \"LM Head Processor\"; print(f\"\\n--- Defining {proc_name} ---\")\n",
        "try:\n",
        "    coord = TensorCoordinate(layer=-1, group=processor_group_idx, nest=0, x=2); tags = [TAG_TYPE_PROCESSOR, TAG_FUNC_LINEAR, model_tag]; prec_tag_weights = TAG_PREC_INT8\n",
        "    kn_tags = [TAG_COMP_LM_HEAD, model_tag, TAG_COMP_WEIGHTS, prec_tag_weights]; knowledge_needs = []; pattern = \"lm_head.weight\"; kid = find_knowledge_id(pattern)\n",
        "    print(f\"  Checking knowledge: Param: lm_head_weights, Pattern: '{pattern}', Found: {'Yes' if kid else 'No'}\")\n",
        "    if kid: knowledge_needs.append({\"param_name\": \"lm_head_weights\", \"tags\": kn_tags, \"optional\": False})\n",
        "    else: processor_errors += 1; print(f\"  ERROR: Essential knowledge missing!\")\n",
        "    if knowledge_needs:\n",
        "        interface = { \"inputs\": [{\"name\":\"final_normed_state\"}], \"outputs\": [{\"name\":\"logits\"}], \"knowledge_needed\": knowledge_needs }\n",
        "        ops_sequences = {'default': [[OP_LINEAR_HEAD, {\"weights\": \"lm_head_weights\"}]]}\n",
        "        tensor_structure = vec.create_tensor( coord=coord, tensor_type=\"processor\", tags=tags, interface=interface, ops_sequences=ops_sequences, status=\"active\", name_id=-1)\n",
        "        if not validate_tensor(tensor_structure): raise ValueError(\"Invalid list structure (LM Head)\")\n",
        "        proc_id = vec.save_tensor(tensor_structure)\n",
        "        if proc_id: processor_map[\"lm_head\"] = proc_id; print(f\"  SUCCESS: Saved {proc_name}: {proc_id}\")\n",
        "        else: processor_errors += 1; print(f\"  ERROR saving {proc_name}\")\n",
        "    else: print(f\"  Skipping {proc_name}\")\n",
        "except Exception as e: print(f\"  ERROR during {proc_name} definition: {e}\"); processor_errors += 1\n",
        "\n",
        "\n",
        "# --- Zavershenie ---\n",
        "# ... (bez izmenenij) ...\n",
        "print(f\"\\n--- Processor Definition Phase Completed ({processor_errors} errors) ---\")\n",
        "processor_map_filename = f\"{HF_MODEL_NAME}_proc_map.pkl\"; processor_map_filepath = DB_PATH / processor_map_filename\n",
        "try:\n",
        "    if processor_errors == 0:\n",
        "        expected_proc_count = 3 + 2 * num_layers\n",
        "        if len(processor_map) == expected_proc_count:\n",
        "             with open(processor_map_filepath, 'wb') as f: pickle.dump(processor_map, f)\n",
        "             print(f\"\\nProcessor map saved to {processor_map_filepath} ({len(processor_map)} entries)\")\n",
        "        else: print(f\"\\nWARN: Processor map has incorrect entry count ({len(processor_map)} vs {expected_proc_count}). NOT SAVED.\")\n",
        "    else: print(f\"\\nProcessor map NOT saved due to {processor_errors} errors.\")\n",
        "except Exception as e: print(f\"Error saving processor map: {e}\")\n",
        "if 'vec' in locals() and vec and hasattr(vec, 'db') and vec.db:\n",
        "    print(\"\\nExplicitly saving index and closing DB at end of Cell 6...\")\n",
        "    if hasattr(vec.db, '_save_index') and callable(vec.db._save_index): vec.db._save_index()\n",
        "    if hasattr(vec.db, 'close') and callable(vec.db.close): vec.db.close()\n",
        "    print(\"Index saved and DB connection closed for Cell 6.\")\n",
        "else: print(\"Warning: Could not explicitly save/close DB index at end of Cell 6.\")\n",
        "end_cell6_time = time.time()\n",
        "print(f\"--- Cell 6 Finished in {end_cell6_time - start_cell6_time:.3f} seconds ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tMp5yYRLlVqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791053b6-355e-4a84-e574-ac58dff82e69"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Converter Cell 6 vCreating ALL Layer Processors v0.7.13 - KV Placeholders ---\n",
            "--- Loading Intermediate Data for Cell 6 ---\n",
            "Attempting to load data from: data/db/DeepSeek-R1-Distill-Qwen-1.5B_cell6_input_data.pkl\n",
            "Intermediate data loaded successfully.\n",
            "  Knowledge map entries: 339\n",
            "  Model Config Type: <class 'transformers.models.qwen2.configuration_qwen2.Qwen2Config'>\n",
            "Using config: L=28, H=12, H_KV=2, HDim=128\n",
            "--- Defining ALL Processor Tensors (Embedding, Layers 0-27, FinalNorm, LMHead) ---\n",
            "Using existing Veector object.\n",
            "\n",
            "--- Defining Embedding Processor ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fb769b8034ba0aae87a82183e23c06bab7160aaea5b151b94080a49f5fa7abc9 -> Type: processor, Status: active, Coords: L-1_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Embedding Processor: fb769b8034ba0aae87a82183e23c06bab7160aaea5b151b94080a49f5fa7abc9\n",
            "\n",
            "--- Defining Transformer Layer Processors (0 to 27) ---\n",
            "\n",
            "--- Defining Attention Processor L0 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 637fa4ff0945661a2373135c652dc8406ecef73c01fbf3884fb0d10196529c32 -> Type: processor, Status: active, Coords: L0_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L0: 637fa4ff0945661a2373135c652dc8406ecef73c01fbf3884fb0d10196529c32\n",
            "\n",
            "--- Defining FFN Processor L0 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 550644eeb529f7b42d61eb4f3776d6403548a1c5935db231a8637b9e391bf982 -> Type: processor, Status: active, Coords: L0_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L0: 550644eeb529f7b42d61eb4f3776d6403548a1c5935db231a8637b9e391bf982\n",
            "\n",
            "--- Defining Attention Processor L1 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0861d409409e03f0d6368f3850356593610a0a210380e05bc824a26886f9cd3d -> Type: processor, Status: active, Coords: L1_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L1: 0861d409409e03f0d6368f3850356593610a0a210380e05bc824a26886f9cd3d\n",
            "\n",
            "--- Defining FFN Processor L1 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d9e18793c9c1c87a6645af46e385cf03e4a7edb6c51fe5d85140355434a02ff1 -> Type: processor, Status: active, Coords: L1_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L1: d9e18793c9c1c87a6645af46e385cf03e4a7edb6c51fe5d85140355434a02ff1\n",
            "\n",
            "--- Defining Attention Processor L2 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3cd51f182f4a9a1b96cbedb99b704f540ed0b3037562a67cdc1ad516e5361394 -> Type: processor, Status: active, Coords: L2_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L2: 3cd51f182f4a9a1b96cbedb99b704f540ed0b3037562a67cdc1ad516e5361394\n",
            "\n",
            "--- Defining FFN Processor L2 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 4864857723698fa8181e9eddff8f4a87068469b2a537ad1fff9b87698b0a5f61 -> Type: processor, Status: active, Coords: L2_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L2: 4864857723698fa8181e9eddff8f4a87068469b2a537ad1fff9b87698b0a5f61\n",
            "\n",
            "--- Defining Attention Processor L3 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2d649133d071e9b74a348c0a404d17ba563d915248797c7b83b63297ef3b8565 -> Type: processor, Status: active, Coords: L3_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L3: 2d649133d071e9b74a348c0a404d17ba563d915248797c7b83b63297ef3b8565\n",
            "\n",
            "--- Defining FFN Processor L3 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9d55d5ff00cda6ee096e60ebf3468983698c8a36b1e95addcba2745ce3f0a899 -> Type: processor, Status: active, Coords: L3_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L3: 9d55d5ff00cda6ee096e60ebf3468983698c8a36b1e95addcba2745ce3f0a899\n",
            "\n",
            "--- Defining Attention Processor L4 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID de2539c4583cfbe14a397b58f09ec6e690ef4c040ffe8a45ce70a250a4ba738f -> Type: processor, Status: active, Coords: L4_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L4: de2539c4583cfbe14a397b58f09ec6e690ef4c040ffe8a45ce70a250a4ba738f\n",
            "\n",
            "--- Defining FFN Processor L4 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 8c743aeea195bd3b4c9cf1d0699cdd0ec7f636a3852c4a610d96f2c3f3295fa2 -> Type: processor, Status: active, Coords: L4_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L4: 8c743aeea195bd3b4c9cf1d0699cdd0ec7f636a3852c4a610d96f2c3f3295fa2\n",
            "\n",
            "--- Defining Attention Processor L5 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 7f0c3eea60bc11cb5c2a72d129a032a84b2c693c757683c5e1e18aeee4bcc720 -> Type: processor, Status: active, Coords: L5_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L5: 7f0c3eea60bc11cb5c2a72d129a032a84b2c693c757683c5e1e18aeee4bcc720\n",
            "\n",
            "--- Defining FFN Processor L5 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 03f29ab21bc568b16352618a45a6a36d049209ad3f42075c8f6b5c45e2020273 -> Type: processor, Status: active, Coords: L5_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L5: 03f29ab21bc568b16352618a45a6a36d049209ad3f42075c8f6b5c45e2020273\n",
            "\n",
            "--- Defining Attention Processor L6 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID faadc6861ec7ecbcc78444da7028ed6defdc073365c5379ed8e14114016d6a18 -> Type: processor, Status: active, Coords: L6_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L6: faadc6861ec7ecbcc78444da7028ed6defdc073365c5379ed8e14114016d6a18\n",
            "\n",
            "--- Defining FFN Processor L6 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 89f25095721848a72defb224c12261f1d52125dfba80947e593a9fd5b132d232 -> Type: processor, Status: active, Coords: L6_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L6: 89f25095721848a72defb224c12261f1d52125dfba80947e593a9fd5b132d232\n",
            "\n",
            "--- Defining Attention Processor L7 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 97bb0486dd31c380e99c1536db7691a05ba55a758fc8387eea0c03c281af4b35 -> Type: processor, Status: active, Coords: L7_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L7: 97bb0486dd31c380e99c1536db7691a05ba55a758fc8387eea0c03c281af4b35\n",
            "\n",
            "--- Defining FFN Processor L7 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 901262d53a2fd6726e3b481497fbb9f623aa449b174d07b7860857dd24a46bd4 -> Type: processor, Status: active, Coords: L7_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L7: 901262d53a2fd6726e3b481497fbb9f623aa449b174d07b7860857dd24a46bd4\n",
            "\n",
            "--- Defining Attention Processor L8 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 40df7dccdc06a7a480232ce7290eed65bfc24277931dc1b0f32e7a19cdc8703d -> Type: processor, Status: active, Coords: L8_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L8: 40df7dccdc06a7a480232ce7290eed65bfc24277931dc1b0f32e7a19cdc8703d\n",
            "\n",
            "--- Defining FFN Processor L8 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID d2a4ba55b42ff87eab85f2248cfa6c3ad481767e3e10997d2426f960b5990e60 -> Type: processor, Status: active, Coords: L8_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L8: d2a4ba55b42ff87eab85f2248cfa6c3ad481767e3e10997d2426f960b5990e60\n",
            "\n",
            "--- Defining Attention Processor L9 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 64deb9968d0a9575aefd6137f7a032a57cfe43da4435f56fd5f34c2e29050f29 -> Type: processor, Status: active, Coords: L9_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L9: 64deb9968d0a9575aefd6137f7a032a57cfe43da4435f56fd5f34c2e29050f29\n",
            "\n",
            "--- Defining FFN Processor L9 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ab3b8ee16707460c8084c1bcd225e908604fd6f7c1720ed53a39ac4b88671c03 -> Type: processor, Status: active, Coords: L9_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L9: ab3b8ee16707460c8084c1bcd225e908604fd6f7c1720ed53a39ac4b88671c03\n",
            "\n",
            "--- Defining Attention Processor L10 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 56b250e7146f8530a777d8175b7dc9d1c2231d7ff2885791abef665fbb59dee5 -> Type: processor, Status: active, Coords: L10_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L10: 56b250e7146f8530a777d8175b7dc9d1c2231d7ff2885791abef665fbb59dee5\n",
            "\n",
            "--- Defining FFN Processor L10 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c92c838d4ee07558d5684d74c6539287847ac085fbbc8175111184d5cc7558d1 -> Type: processor, Status: active, Coords: L10_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L10: c92c838d4ee07558d5684d74c6539287847ac085fbbc8175111184d5cc7558d1\n",
            "\n",
            "--- Defining Attention Processor L11 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9778ccef5502ed004e5d8a1f3ba9ef600c4be2967f14a51fea09484b25ae3b99 -> Type: processor, Status: active, Coords: L11_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L11: 9778ccef5502ed004e5d8a1f3ba9ef600c4be2967f14a51fea09484b25ae3b99\n",
            "\n",
            "--- Defining FFN Processor L11 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9c635b13896dc129de7bc223c75dc57ecce7f3c329514834f7de9f282857d97e -> Type: processor, Status: active, Coords: L11_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L11: 9c635b13896dc129de7bc223c75dc57ecce7f3c329514834f7de9f282857d97e\n",
            "\n",
            "--- Defining Attention Processor L12 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 574e7e6464873af27e0e8190e0967cdabf82ef1b7021670096aaed57e3111748 -> Type: processor, Status: active, Coords: L12_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L12: 574e7e6464873af27e0e8190e0967cdabf82ef1b7021670096aaed57e3111748\n",
            "\n",
            "--- Defining FFN Processor L12 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID fad55724dcec62a204cf5a80830b31b0df82399a56ad75dfd7c155d56f7bcc88 -> Type: processor, Status: active, Coords: L12_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L12: fad55724dcec62a204cf5a80830b31b0df82399a56ad75dfd7c155d56f7bcc88\n",
            "\n",
            "--- Defining Attention Processor L13 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c8b90262b8b7ce06662944acb023f4186563d131d0c1a1fd7aabd1bcfbceb53f -> Type: processor, Status: active, Coords: L13_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L13: c8b90262b8b7ce06662944acb023f4186563d131d0c1a1fd7aabd1bcfbceb53f\n",
            "\n",
            "--- Defining FFN Processor L13 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c6a4941e5913f468ff6d54643e2d42a2d75d20de0c0c6f245ee3da40905bd6d2 -> Type: processor, Status: active, Coords: L13_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L13: c6a4941e5913f468ff6d54643e2d42a2d75d20de0c0c6f245ee3da40905bd6d2\n",
            "\n",
            "--- Defining Attention Processor L14 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 143a1e0c65b53483d3725d6e38bc51ed2f19cef26c36b9d1821813bbe2811a0a -> Type: processor, Status: active, Coords: L14_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L14: 143a1e0c65b53483d3725d6e38bc51ed2f19cef26c36b9d1821813bbe2811a0a\n",
            "\n",
            "--- Defining FFN Processor L14 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0151e0b5cbdbecef71f47dd9678c62a7b19efbd970e80a41c80ff33c2f788558 -> Type: processor, Status: active, Coords: L14_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L14: 0151e0b5cbdbecef71f47dd9678c62a7b19efbd970e80a41c80ff33c2f788558\n",
            "\n",
            "--- Defining Attention Processor L15 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 64556ca3d7f5425f468f55ed89450d816af1a9c6f8456877c22084224c0a20b8 -> Type: processor, Status: active, Coords: L15_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L15: 64556ca3d7f5425f468f55ed89450d816af1a9c6f8456877c22084224c0a20b8\n",
            "\n",
            "--- Defining FFN Processor L15 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 82ad5e380e335a78d27ff166986ccc394db24b0bd6190b4eb7aaa14adffc5493 -> Type: processor, Status: active, Coords: L15_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L15: 82ad5e380e335a78d27ff166986ccc394db24b0bd6190b4eb7aaa14adffc5493\n",
            "\n",
            "--- Defining Attention Processor L16 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID cfacecbaf6e51c03917b4a5621b71364e86acde011edca3e97bf01c74e703e32 -> Type: processor, Status: active, Coords: L16_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L16: cfacecbaf6e51c03917b4a5621b71364e86acde011edca3e97bf01c74e703e32\n",
            "\n",
            "--- Defining FFN Processor L16 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 0202a95974699c954ddf897600347d9b8131e53a91c834748f7126346a8bbb4b -> Type: processor, Status: active, Coords: L16_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L16: 0202a95974699c954ddf897600347d9b8131e53a91c834748f7126346a8bbb4b\n",
            "\n",
            "--- Defining Attention Processor L17 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 44d38022daed9d3ecba98533107542d5d18a406933df864db02263fb5995ed47 -> Type: processor, Status: active, Coords: L17_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L17: 44d38022daed9d3ecba98533107542d5d18a406933df864db02263fb5995ed47\n",
            "\n",
            "--- Defining FFN Processor L17 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 37c84881f491ecc697b95d5103a19a3b4637440839ff28f4da1edadd0c430653 -> Type: processor, Status: active, Coords: L17_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L17: 37c84881f491ecc697b95d5103a19a3b4637440839ff28f4da1edadd0c430653\n",
            "\n",
            "--- Defining Attention Processor L18 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c674aedd40814de7f85c3186cbd8b8d833275cbef503eae3cbea4042b29d655f -> Type: processor, Status: active, Coords: L18_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L18: c674aedd40814de7f85c3186cbd8b8d833275cbef503eae3cbea4042b29d655f\n",
            "\n",
            "--- Defining FFN Processor L18 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 3a9b0992b02e95156c3f3c2db1ade6a4fb5d9cbb9a692a6d96a9f86cf2ccdf25 -> Type: processor, Status: active, Coords: L18_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L18: 3a9b0992b02e95156c3f3c2db1ade6a4fb5d9cbb9a692a6d96a9f86cf2ccdf25\n",
            "\n",
            "--- Defining Attention Processor L19 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID aab937927654fef350ed0dd09a1e03b0202034a2c7537bfc229c8bb274e0df3e -> Type: processor, Status: active, Coords: L19_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L19: aab937927654fef350ed0dd09a1e03b0202034a2c7537bfc229c8bb274e0df3e\n",
            "\n",
            "--- Defining FFN Processor L19 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 809df867886fd187f9671b8d3fe61c48445041135c2a81f77592229f80d9fa1b -> Type: processor, Status: active, Coords: L19_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L19: 809df867886fd187f9671b8d3fe61c48445041135c2a81f77592229f80d9fa1b\n",
            "\n",
            "--- Defining Attention Processor L20 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 59f7742189f586a07e6e250c88671da7f96f37322906b405e64efc7f570f31d8 -> Type: processor, Status: active, Coords: L20_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L20: 59f7742189f586a07e6e250c88671da7f96f37322906b405e64efc7f570f31d8\n",
            "\n",
            "--- Defining FFN Processor L20 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID e822c377439827581999ba6cb48c784ff847c40bf6947c191a444f6c6c1882d4 -> Type: processor, Status: active, Coords: L20_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L20: e822c377439827581999ba6cb48c784ff847c40bf6947c191a444f6c6c1882d4\n",
            "\n",
            "--- Defining Attention Processor L21 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 98b21f6ef24dfcacbd640df8b28dc33e33b38718cb7156b29dc77db8e5e0a7d0 -> Type: processor, Status: active, Coords: L21_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L21: 98b21f6ef24dfcacbd640df8b28dc33e33b38718cb7156b29dc77db8e5e0a7d0\n",
            "\n",
            "--- Defining FFN Processor L21 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID dace53a6fb3a620019070051264d3ba3350dad8c5587da43e3481251accb859a -> Type: processor, Status: active, Coords: L21_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L21: dace53a6fb3a620019070051264d3ba3350dad8c5587da43e3481251accb859a\n",
            "\n",
            "--- Defining Attention Processor L22 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ed8f41d0dad9b888a1f35cb9d6040cf64a6f43699091c3a8e5cb0c30b261b2ff -> Type: processor, Status: active, Coords: L22_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L22: ed8f41d0dad9b888a1f35cb9d6040cf64a6f43699091c3a8e5cb0c30b261b2ff\n",
            "\n",
            "--- Defining FFN Processor L22 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 2f530b26be9960932f898d2543f32b1793a27d38a4a53359061eca7d3e47a0be -> Type: processor, Status: active, Coords: L22_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L22: 2f530b26be9960932f898d2543f32b1793a27d38a4a53359061eca7d3e47a0be\n",
            "\n",
            "--- Defining Attention Processor L23 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 201d932aaa4968e0d4aba4f788ca2e6322167909d33ff4e23c8bbae8753d01ec -> Type: processor, Status: active, Coords: L23_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L23: 201d932aaa4968e0d4aba4f788ca2e6322167909d33ff4e23c8bbae8753d01ec\n",
            "\n",
            "--- Defining FFN Processor L23 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID c4dfc81d4061355eba63b9fd4bb0713db85d90386fd5f50b71c4c97e098b6bd1 -> Type: processor, Status: active, Coords: L23_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L23: c4dfc81d4061355eba63b9fd4bb0713db85d90386fd5f50b71c4c97e098b6bd1\n",
            "\n",
            "--- Defining Attention Processor L24 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 38aa7a3311c4b57c2608c57c585f11a9ac8c327bf683edbaa8f86becb7b883e1 -> Type: processor, Status: active, Coords: L24_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L24: 38aa7a3311c4b57c2608c57c585f11a9ac8c327bf683edbaa8f86becb7b883e1\n",
            "\n",
            "--- Defining FFN Processor L24 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 115810c96bb42652e7a4348498c47935b387fd9328a7d0fbd6eab5bfe5b90f43 -> Type: processor, Status: active, Coords: L24_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L24: 115810c96bb42652e7a4348498c47935b387fd9328a7d0fbd6eab5bfe5b90f43\n",
            "\n",
            "--- Defining Attention Processor L25 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 76713249cf969988fc3806db75060458e4faa18287ce202eeb3e9e2464720e6f -> Type: processor, Status: active, Coords: L25_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L25: 76713249cf969988fc3806db75060458e4faa18287ce202eeb3e9e2464720e6f\n",
            "\n",
            "--- Defining FFN Processor L25 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 730cd4c86f2c584b5027407fa94a0f59100754b6e8277fc950ac0a13b4e40145 -> Type: processor, Status: active, Coords: L25_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L25: 730cd4c86f2c584b5027407fa94a0f59100754b6e8277fc950ac0a13b4e40145\n",
            "\n",
            "--- Defining Attention Processor L26 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 10d8b93a85f1796067ec73214934215addcac60cd90dcbed17182a1001cd2542 -> Type: processor, Status: active, Coords: L26_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L26: 10d8b93a85f1796067ec73214934215addcac60cd90dcbed17182a1001cd2542\n",
            "\n",
            "--- Defining FFN Processor L26 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9561fb42a94dbbbfcec26d21a0899f0b809f4aa242a7a8638ff9b041052d098a -> Type: processor, Status: active, Coords: L26_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L26: 9561fb42a94dbbbfcec26d21a0899f0b809f4aa242a7a8638ff9b041052d098a\n",
            "\n",
            "--- Defining Attention Processor L27 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 9818bcc8fc8dd23d4e4128acfd14b8b76a720335003cc7679c905b5f7422b7c8 -> Type: processor, Status: active, Coords: L27_G500_N0_X0_Y0_Z0\n",
            "  SUCCESS: Saved Attention Processor L27: 9818bcc8fc8dd23d4e4128acfd14b8b76a720335003cc7679c905b5f7422b7c8\n",
            "\n",
            "--- Defining FFN Processor L27 ---\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 1238d786628e4209e025de35d377209a6d2b9484afc8dbf6bd499b297c293d23 -> Type: processor, Status: active, Coords: L27_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved FFN Processor L27: 1238d786628e4209e025de35d377209a6d2b9484afc8dbf6bd499b297c293d23\n",
            "\n",
            "--- Defining Final Norm Processor ---\n",
            "  Checking knowledge: Param: norm_weight, Pattern: 'model.norm.weight', Found: Yes\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID 6577c7a755eccbecc951e4e2d491bb1a44683b70d45c8f6fc02828f37084e3b7 -> Type: processor, Status: active, Coords: L-1_G500_N0_X1_Y0_Z0\n",
            "  SUCCESS: Saved Final Norm Processor: 6577c7a755eccbecc951e4e2d491bb1a44683b70d45c8f6fc02828f37084e3b7\n",
            "\n",
            "--- Defining LM Head Processor ---\n",
            "  Checking knowledge: Param: lm_head_weights, Pattern: 'lm_head.weight', Found: Yes\n",
            "DEBUG INDEX UPDATE: Adding/Updating ID ab2f6f273bbbdb6ad3e897cd85f5107bf787cc85fe28b1ddcca5706b21a8535a -> Type: processor, Status: active, Coords: L-1_G500_N0_X2_Y0_Z0\n",
            "  SUCCESS: Saved LM Head Processor: ab2f6f273bbbdb6ad3e897cd85f5107bf787cc85fe28b1ddcca5706b21a8535a\n",
            "\n",
            "--- Processor Definition Phase Completed (0 errors) ---\n",
            "\n",
            "Processor map saved to data/db/DeepSeek-R1-Distill-Qwen-1.5B_proc_map.pkl (59 entries)\n",
            "\n",
            "Explicitly saving index and closing DB at end of Cell 6...\n",
            "DEBUG INDEX SAVE (v0.9.7): Attempting DIRECT save.\n",
            "DEBUG INDEX SAVE: Size in memory BEFORE save: 398\n",
            "DEBUG INDEX SAVE: Last 5 keys in memory: ['9561fb42a94dbbbfcec26d21a0899f0b809f4aa242a7a8638ff9b041052d098a', '9818bcc8fc8dd23d4e4128acfd14b8b76a720335003cc7679c905b5f7422b7c8', '1238d786628e4209e025de35d377209a6d2b9484afc8dbf6bd499b297c293d23', '6577c7a755eccbecc951e4e2d491bb1a44683b70d45c8f6fc02828f37084e3b7', 'ab2f6f273bbbdb6ad3e897cd85f5107bf787cc85fe28b1ddcca5706b21a8535a']\n",
            "DEBUG INDEX SAVE: pickle.dump completed for /content/data/db/tensor_index.pkl.\n",
            "DEBUG INDEX SAVE: Attempting immediate reload for verification...\n",
            "DEBUG INDEX SAVE: Immediate reload SUCCESS. Size loaded: 398\n",
            "DEBUG INDEX SAVE: Size in memory AFTER save: 398\n",
            "Closing VeectorDB v0.9.7 connection...\n",
            "DEBUG INDEX SAVE: Size in memory AFTER save: 398\n",
            "Index saved and DB connection closed for Cell 6.\n",
            "--- Cell 6 Finished in 0.089 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 7: Analyze Veector DB Structure (Index-First Analysis v0.7.5+) ===\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "print(f\"\\n--- Running DB Analysis Cell (Index-First Analysis) ---\")\n",
        "start_cell7_time = time.time()\n",
        "\n",
        "# --- Импорты из tensors.py (Нужны новые геттеры и валидатор кортежа) ---\n",
        "ANALYSIS_IMPORTS_OK = False\n",
        "TENSORS_VERSION_REQ = \"0.7.5\" # Ожидаем последнюю версию\n",
        "try:\n",
        "    from tensors import TENSORS_VERSION\n",
        "    print(f\"DEBUG: Importing from tensors version: {TENSORS_VERSION}\")\n",
        "    if TENSORS_VERSION < TENSORS_VERSION_REQ:\n",
        "        print(f\"WARN: Analysis script requires tensors v{TENSORS_VERSION_REQ}+. Found v{TENSORS_VERSION}\")\n",
        "\n",
        "    from tensors import (\n",
        "        TensorCoordinate, MetadataTuple, validate_tensor_tuple, # Валидатор кортежа\n",
        "        # Новые геттеры для кортежа\n",
        "        get_type_code_from_meta, get_dtype_code_from_meta, get_has_blob_flag_from_meta,\n",
        "        get_coord_list_from_meta, get_coord_obj_from_meta, get_tags_list_from_meta,\n",
        "        # Маппинги и константы\n",
        "        REVERSE_DATA_TYPE_MAPPING, DTYPE_MAPPING,\n",
        "        TAG_COMP_EMBEDDING, TAG_COMP_LM_HEAD, TAG_PREC_INT8\n",
        "    )\n",
        "    ANALYSIS_IMPORTS_OK = True\n",
        "    print(\"DEBUG: Imports for analysis successful.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"WARN: Cannot import from tensors.py for full analysis: {e}\")\n",
        "    ANALYSIS_IMPORTS_OK = False\n",
        "\n",
        "    # Заглушки\n",
        "    TAG_COMP_EMBEDDING = -1000\n",
        "    TAG_COMP_LM_HEAD = -1001\n",
        "    TAG_PREC_INT8 = -1002\n",
        "\n",
        "    class TensorCoordinate:\n",
        "        \"\"\"Заглушка для класса TensorCoordinate.\"\"\"\n",
        "        @staticmethod\n",
        "        def from_string(s):\n",
        "            \"\"\"Метод-заглушка. Возвращает None.\"\"\"\n",
        "            return None\n",
        "\n",
        "    def validate_tensor_tuple(t):\n",
        "        \"\"\"Заглушка для функции validate_tensor_tuple.\"\"\"\n",
        "        return False\n",
        "\n",
        "    def get_type_code_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_type_code_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    def get_has_blob_flag_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_has_blob_flag_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    def get_tags_list_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_tags_list_from_meta.\"\"\"\n",
        "        return []\n",
        "\n",
        "    def get_coord_list_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_coord_list_from_meta.\"\"\"\n",
        "        return [-1] * 6\n",
        "\n",
        "    def get_coord_obj_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_coord_obj_from_meta.\"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_dtype_code_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_dtype_code_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    REVERSE_DATA_TYPE_MAPPING = {}\n",
        "    DTYPE_MAPPING = {}\n",
        "\n",
        "except Exception as e_other:\n",
        "    print(f\"WARN: Unexpected error during imports for analysis: {e_other}\")\n",
        "    traceback.print_exc()\n",
        "    ANALYSIS_IMPORTS_OK = False\n",
        "\n",
        "    # Заглушки\n",
        "    TAG_COMP_EMBEDDING = -1000\n",
        "    TAG_COMP_LM_HEAD = -1001\n",
        "    TAG_PREC_INT8 = -1002\n",
        "\n",
        "    class TensorCoordinate:\n",
        "        \"\"\"Заглушка для класса TensorCoordinate.\"\"\"\n",
        "        @staticmethod\n",
        "        def from_string(s):\n",
        "            \"\"\"Метод-заглушка. Возвращает None.\"\"\"\n",
        "            return None\n",
        "\n",
        "    def validate_tensor_tuple(t):\n",
        "        \"\"\"Заглушка для функции validate_tensor_tuple.\"\"\"\n",
        "        return False\n",
        "\n",
        "    def get_type_code_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_type_code_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    def get_has_blob_flag_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_has_blob_flag_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    def get_tags_list_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_tags_list_from_meta.\"\"\"\n",
        "        return []\n",
        "\n",
        "    def get_coord_list_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_coord_list_from_meta.\"\"\"\n",
        "        return [-1] * 6\n",
        "\n",
        "    def get_coord_obj_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_coord_obj_from_meta.\"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_dtype_code_from_meta(t):\n",
        "        \"\"\"Заглушка для функции get_dtype_code_from_meta.\"\"\"\n",
        "        return 0\n",
        "\n",
        "    REVERSE_DATA_TYPE_MAPPING = {}\n",
        "    DTYPE_MAPPING = {}\n",
        "\n",
        "print(f\"\\n--- Analyzing Veector DB Structure (Index-First Method) ---\")\n",
        "\n",
        "# --- Конфигурация ---\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "     print(\"ERROR: DB_PATH not defined. Assuming './data/db/'\")\n",
        "     DB_PATH = Path(\"./data/db/\")\n",
        "\n",
        "INDEX_FILENAME = \"tensor_index.pkl\"\n",
        "\n",
        "if not DB_PATH.is_dir():\n",
        "    print(f\"ERROR: Database directory not found at {DB_PATH}\")\n",
        "else:\n",
        "    print(f\"Analyzing DB at: {DB_PATH.resolve()}\")\n",
        "\n",
        "    # Инициализация счетчиков\n",
        "    total_meta_files_in_index = 0; total_meta_size = 0; total_blob_files_npy = 0\n",
        "    total_blob_files_pickle = 0; total_blob_size = 0; processor_count = 0\n",
        "    knowledge_count = 0; state_count = 0; converter_count = 0; other_count = 0\n",
        "    error_count = 0; missing_meta_files = 0; invalid_tuples = 0\n",
        "    index_entry_count = 0; index_total_size = 0; index_data = {}\n",
        "\n",
        "    # --- 1. Загрузка Индекса ---\n",
        "    index_path = DB_PATH / INDEX_FILENAME\n",
        "    if not index_path.is_file(): print(f\"\\nIndex file '{index_path.name}' not found.\")\n",
        "    else:\n",
        "        try:\n",
        "            index_total_size = index_path.stat().st_size\n",
        "            print(f\"\\nFound index file: {index_path.name}\")\n",
        "            with open(index_path, 'rb') as f: index_data = pickle.load(f)\n",
        "            if isinstance(index_data, dict):\n",
        "                 index_entry_count = len(index_data); print(f\"Index contains {index_entry_count} entries.\")\n",
        "                 count = 0; print(\"  Sample Index Entries:\")\n",
        "                 for key, value in index_data.items():\n",
        "                     path_val=value.get('path','N/A'); type_val=value.get('type','N/A'); stat_val=value.get('stat','N/A')\n",
        "                     g=value.get('g','N/A'); l=value.get('l','N/A'); n=value.get('n','N/A')\n",
        "                     print(f\"    ID: {key} -> Path: {path_val}, Type: {type_val}, Stat: {stat_val}, G:{g}, L:{l}, N:{n}\")\n",
        "                     count += 1;\n",
        "                     if count >= 5: break\n",
        "            else: print(\"  Error: Index file content is not a dictionary.\"); index_entry_count = 0\n",
        "        except Exception as e: print(f\"  Error opening or reading index file '{index_path}': {e}\"); index_entry_count = 0\n",
        "        print(f\"Total Index File Size: {index_total_size / 1024:.2f} KB\")\n",
        "\n",
        "    # --- 2. Анализ по Индексу ---\n",
        "    print(f\"\\nAnalyzing tensors listed in index ({index_entry_count} entries)...\")\n",
        "    if index_entry_count > 0 and isinstance(index_data, dict):\n",
        "        for tensor_id, index_entry in index_data.items():\n",
        "            meta_tuple = None; tensor_type = \"unknown\"\n",
        "            relative_meta_path = index_entry.get('path')\n",
        "            if not relative_meta_path:\n",
        "                 print(f\"  WARN: Missing 'path' in index entry for ID: {tensor_id}\"); error_count += 1; continue\n",
        "\n",
        "            meta_file_path = DB_PATH / relative_meta_path\n",
        "\n",
        "            if not meta_file_path.is_file():\n",
        "                print(f\"  WARN: Meta file referenced in index not found: {meta_file_path}\")\n",
        "                missing_meta_files += 1; continue # Эта запись индекса невалидна\n",
        "\n",
        "            # Файл существует, увеличиваем счетчик и размер\n",
        "            total_meta_files_in_index += 1\n",
        "            try: total_meta_size += meta_file_path.stat().st_size\n",
        "            except Exception: pass # Игнорируем ошибку получения размера\n",
        "\n",
        "            # Загружаем КОРТЕЖ метаданных\n",
        "            try:\n",
        "                with open(meta_file_path, 'rb') as f: meta_tuple = pickle.load(f)\n",
        "            except Exception as load_e:\n",
        "                 print(f\"  ERROR loading meta file {meta_file_path}: {load_e}\"); error_count += 1; continue\n",
        "\n",
        "            # Валидируем КОРТЕЖ\n",
        "            if not validate_tensor_tuple(meta_tuple):\n",
        "                 print(f\"  WARN: Invalid metadata tuple structure in {meta_file_path.name}\"); invalid_tuples += 1; continue\n",
        "\n",
        "            # Анализируем КОРТЕЖ с помощью НОВЫХ геттеров\n",
        "            try:\n",
        "                type_code = get_type_code_from_meta(meta_tuple)\n",
        "                tensor_type = REVERSE_DATA_TYPE_MAPPING.get(type_code, \"unknown\")\n",
        "\n",
        "                # Считаем типы\n",
        "                if tensor_type == \"processor\": processor_count += 1\n",
        "                elif tensor_type == \"knowledge\": knowledge_count += 1\n",
        "                elif tensor_type == \"state\": state_count += 1\n",
        "                elif tensor_type == \"converter\": converter_count += 1\n",
        "                else: other_count +=1\n",
        "\n",
        "                # Проверяем блоб по флагу из кортежа\n",
        "                if get_has_blob_flag_from_meta(meta_tuple) == 1:\n",
        "                    tags = get_tags_list_from_meta(meta_tuple)\n",
        "                    dtype_code = get_dtype_code_from_meta(meta_tuple)\n",
        "                    coord_list = get_coord_list_from_meta(meta_tuple)\n",
        "\n",
        "                    if not coord_list or not ANALYSIS_IMPORTS_OK: continue # Не можем построить путь\n",
        "\n",
        "                    # Реконструируем объект координат ТОЛЬКО для пути\n",
        "                    coord_obj = TensorCoordinate(*coord_list) # Не используем from_string\n",
        "                    if not coord_obj: continue\n",
        "\n",
        "                    # Определяем формат блоба\n",
        "                    blob_format = 'pickle'\n",
        "                    is_int8 = (dtype_code == DTYPE_MAPPING.get('int8'))\n",
        "                    is_embed_or_lm = (TAG_COMP_EMBEDDING in tags) or (TAG_COMP_LM_HEAD in tags)\n",
        "                    if is_embed_or_lm or is_int8: blob_format = 'npy'\n",
        "\n",
        "                    # Проверяем файл блоба\n",
        "                    blob_file = meta_file_path.parent / f\"{tensor_id}.{blob_format}\"\n",
        "                    if blob_file.is_file():\n",
        "                        try:\n",
        "                            total_blob_size += blob_file.stat().st_size\n",
        "                            if blob_format == 'npy': total_blob_files_npy += 1\n",
        "                            else: total_blob_files_pickle += 1\n",
        "                        except Exception as stat_e: print(f\"  WARN: Cannot get size for {blob_file}: {stat_e}\")\n",
        "\n",
        "            except Exception as parse_e:\n",
        "                print(f\"  ERROR parsing metadata tuple from {meta_file.name}: {parse_e}\"); error_count += 1;\n",
        "                # Коррекция счетчиков типов при ошибке парсинга\n",
        "                if tensor_type == \"processor\": processor_count = max(0, processor_count - 1)\n",
        "                elif tensor_type == \"knowledge\": knowledge_count = max(0, knowledge_count - 1)\n",
        "                # ... и т.д.\n",
        "\n",
        "    # --- Печать итогов ---\n",
        "    print(f\"\\n--- Analysis Summary (Based on Index) ---\")\n",
        "    print(f\"Total Index Entries: {index_entry_count}\")\n",
        "    print(f\"Meta Files Found (from index): {total_meta_files_in_index}\")\n",
        "    if missing_meta_files > 0: print(f\"  WARNING: Missing meta files referenced in index: {missing_meta_files}\")\n",
        "    if invalid_tuples > 0: print(f\"  WARNING: Invalid metadata tuples found: {invalid_tuples}\")\n",
        "    print(f\"Total Metadata Size (of found files): {total_meta_size / 1024:.2f} KB\")\n",
        "    print(f\"Total Blob Files (.npy): {total_blob_files_npy}\")\n",
        "    print(f\"Total Blob Files (.blob): {total_blob_files_pickle}\")\n",
        "    total_blobs = total_blob_files_npy + total_blob_files_pickle\n",
        "    print(f\"Total Blob Files (sum): {total_blobs}\")\n",
        "    print(f\"Total Blob Size: {total_blob_size / (1024*1024):.2f} MB\")\n",
        "    if total_blobs > 0: avg_blob_kb = (total_blob_size / total_blobs) / 1024; print(f\"Average Blob Size: {avg_blob_kb:.2f} KB\")\n",
        "    print(f\"Tensor Counts (based on 'type_code' in successfully parsed meta tuples):\")\n",
        "    print(f\"  Knowledge: {knowledge_count}\")\n",
        "    print(f\"  Processor: {processor_count}\")\n",
        "    print(f\"  State: {state_count}\")\n",
        "    print(f\"  Converter: {converter_count}\")\n",
        "    print(f\"  Other/Unknown Type: {other_count}\")\n",
        "    print(f\"  Errors during tuple processing: {error_count + invalid_tuples}\") # Суммируем ошибки парсинга и невалидные кортежи\n",
        "\n",
        "end_cell7_time = time.time()\n",
        "print(f\"--- Analysis Cell Finished in {end_cell7_time - start_cell7_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtGBC6PMB26X",
        "outputId": "7cd1c8ca-c190-4370-e303-acc52b71540d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running DB Analysis Cell (Index-First Analysis) ---\n",
            "DEBUG: Importing from tensors version: 0.7.6\n",
            "DEBUG: Imports for analysis successful.\n",
            "\n",
            "--- Analyzing Veector DB Structure (Index-First Method) ---\n",
            "Analyzing DB at: /content/data/db\n",
            "\n",
            "Found index file: tensor_index.pkl\n",
            "Index contains 398 entries.\n",
            "  Sample Index Entries:\n",
            "    ID: fb4ef375e208da470d6841a89c441ca29e016873a80c8519660c22cfa227be8d -> Path: g100/l-1/n1/fb4ef375e208da470d6841a89c441ca29e016873a80c8519660c22cfa227be8d.meta, Type: knowledge, Stat: active, G:100, L:-1, N:1\n",
            "    ID: 3e405f866732b9da27a6bc1c1ff7b0977a66bc51b7e58a476f9f559855c46dc1 -> Path: g100/l0/n1/3e405f866732b9da27a6bc1c1ff7b0977a66bc51b7e58a476f9f559855c46dc1.meta, Type: knowledge, Stat: active, G:100, L:0, N:1\n",
            "    ID: 7e35759fe8c401a67215f0914efcf0130a3523aec2106b85ccc604a9204ba17d -> Path: g100/l0/n1/7e35759fe8c401a67215f0914efcf0130a3523aec2106b85ccc604a9204ba17d.meta, Type: knowledge, Stat: active, G:100, L:0, N:1\n",
            "    ID: 5edca5c7dcc96773e1d3801d80799891cc6a8913aff06dd5b15effc5c3b5f148 -> Path: g100/l0/n1/5edca5c7dcc96773e1d3801d80799891cc6a8913aff06dd5b15effc5c3b5f148.meta, Type: knowledge, Stat: active, G:100, L:0, N:1\n",
            "    ID: f665d55801290b3994e246a3b7eb684ea39772978e429a63deaca87446a769c7 -> Path: g100/l0/n1/f665d55801290b3994e246a3b7eb684ea39772978e429a63deaca87446a769c7.meta, Type: knowledge, Stat: active, G:100, L:0, N:1\n",
            "Total Index File Size: 68.97 KB\n",
            "\n",
            "Analyzing tensors listed in index (398 entries)...\n",
            "\n",
            "--- Analysis Summary (Based on Index) ---\n",
            "Total Index Entries: 398\n",
            "Meta Files Found (from index): 398\n",
            "Total Metadata Size (of found files): 97.58 KB\n",
            "Total Blob Files (.npy): 2\n",
            "Total Blob Files (.blob): 0\n",
            "Total Blob Files (sum): 2\n",
            "Total Blob Size: 445.13 MB\n",
            "Average Blob Size: 227904.12 KB\n",
            "Tensor Counts (based on 'type_code' in successfully parsed meta tuples):\n",
            "  Knowledge: 339\n",
            "  Processor: 59\n",
            "  State: 0\n",
            "  Converter: 0\n",
            "  Other/Unknown Type: 0\n",
            "  Errors during tuple processing: 0\n",
            "--- Analysis Cell Finished in 0.07 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Detailed Index File Analysis ===\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "from collections import Counter\n",
        "\n",
        "print(f\"\\n--- Running Detailed Index Analysis Cell ---\")\n",
        "start_cell8_time = time.time()\n",
        "\n",
        "# --- Конфигурация ---\n",
        "if 'DB_PATH' not in locals() or not isinstance(DB_PATH, Path):\n",
        "     print(\"ERROR: DB_PATH not defined. Assuming './data/db/'\")\n",
        "     DB_PATH = Path(\"./data/db/\")\n",
        "\n",
        "INDEX_FILENAME = \"tensor_index.pkl\"\n",
        "index_path = DB_PATH / INDEX_FILENAME\n",
        "\n",
        "print(f\"Attempting to load index file: {index_path}\")\n",
        "\n",
        "# --- Загрузка и Анализ ---\n",
        "index_data = None\n",
        "load_error = None\n",
        "index_entry_count = 0\n",
        "type_counts = Counter() # Счетчик для типов\n",
        "loaded_ids = []\n",
        "\n",
        "if not index_path.is_file():\n",
        "    print(f\"ERROR: Index file not found at {index_path}\")\n",
        "else:\n",
        "    try:\n",
        "        with open(index_path, 'rb') as f:\n",
        "            # Пытаемся загрузить весь файл\n",
        "            index_data = pickle.load(f)\n",
        "\n",
        "        if isinstance(index_data, dict):\n",
        "            index_entry_count = len(index_data)\n",
        "            print(f\"SUCCESS: Index file loaded successfully.\")\n",
        "            print(f\"Total entries loaded by pickle: {index_entry_count}\")\n",
        "\n",
        "            # Анализируем загруженные записи\n",
        "            print(\"\\nAnalyzing loaded index entries...\")\n",
        "            for tensor_id, entry_data in index_data.items():\n",
        "                if isinstance(entry_data, dict):\n",
        "                    tensor_type = entry_data.get('type', 'unknown')\n",
        "                    type_counts[tensor_type] += 1\n",
        "                    loaded_ids.append(tensor_id)\n",
        "                else:\n",
        "                    print(f\"  WARN: Invalid entry format for ID {tensor_id}: {type(entry_data)}\")\n",
        "                    type_counts['invalid_format'] += 1\n",
        "\n",
        "        else:\n",
        "            load_error = f\"Loaded object is not a dictionary (Type: {type(index_data)}).\"\n",
        "            print(f\"ERROR: {load_error}\")\n",
        "            index_entry_count = 0 # Считаем, что ничего не загрузили корректно\n",
        "\n",
        "    # Ловим конкретные ошибки UnpicklingError\n",
        "    except pickle.UnpicklingError as upe:\n",
        "        load_error = f\"UnpicklingError: File corrupted or invalid format ({upe}).\"\n",
        "        print(f\"ERROR during pickle load: {load_error}\")\n",
        "        # Попытаться прочитать файл построчно или частично здесь сложно\n",
        "        index_entry_count = 0\n",
        "    except EOFError:\n",
        "        load_error = \"EOFError: File seems truncated or corrupted (unexpected end).\"\n",
        "        print(f\"ERROR during pickle load: {load_error}\")\n",
        "        index_entry_count = 0\n",
        "    except Exception as e:\n",
        "        load_error = f\"Unexpected error during pickle load: {e}\"\n",
        "        print(f\"ERROR during pickle load: {load_error}\")\n",
        "        traceback.print_exc()\n",
        "        index_entry_count = 0\n",
        "\n",
        "# --- Вывод Результатов ---\n",
        "print(\"\\n--- Index Analysis Results ---\")\n",
        "if load_error:\n",
        "    print(f\"Index Loading Status: FAILED ({load_error})\")\n",
        "elif index_data is not None:\n",
        "    print(f\"Index Loading Status: SUCCESS\")\n",
        "    print(f\"Number of Entries Loaded: {index_entry_count}\")\n",
        "    print(\"Counts by Tensor Type (from loaded index):\")\n",
        "    for tensor_type, count in type_counts.items():\n",
        "        print(f\"  - {tensor_type}: {count}\")\n",
        "    # Опционально: вывести все ID (может быть очень много)\n",
        "    # print(\"\\nLoaded Tensor IDs:\")\n",
        "    # for tensor_id in loaded_ids:\n",
        "    #     print(f\"  {tensor_id}\")\n",
        "else:\n",
        "     print(\"Index Loading Status: File not found.\")\n",
        "\n",
        "\n",
        "end_cell8_time = time.time()\n",
        "print(f\"\\n--- Index Analysis Cell Finished in {end_cell8_time - start_cell8_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "id": "UOhGN4QMFR8y",
        "outputId": "5a9d0059-7ea4-4b91-82be-917611bb61e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Detailed Index Analysis Cell ---\n",
            "Attempting to load index file: data/db/tensor_index.pkl\n",
            "SUCCESS: Index file loaded successfully.\n",
            "Total entries loaded by pickle: 398\n",
            "\n",
            "Analyzing loaded index entries...\n",
            "\n",
            "--- Index Analysis Results ---\n",
            "Index Loading Status: SUCCESS\n",
            "Number of Entries Loaded: 398\n",
            "Counts by Tensor Type (from loaded index):\n",
            "  - knowledge: 339\n",
            "  - processor: 59\n",
            "\n",
            "--- Index Analysis Cell Finished in 0.00 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Архивация и скачивание\n",
        "import shutil\n",
        "shutil.make_archive(\"model_DeepSeek-r1-distill-1.5b\", \"zip\", \"data\")\n",
        "zip_name = \"model_DeepSeek-r1-distill-1.5b.zip\""
      ],
      "metadata": {
        "id": "KZTHUDdLzeuW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh  # Посмотрите список файлов в текущей директории"
      ],
      "metadata": {
        "id": "9R8iZOVw1RW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выгрузка на Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "destination_path = f\"/content/drive/My Drive/models/\"\n",
        "shutil.copy(zip_name, destination_path)\n",
        "print(f\"🟢 [LOG] ✅ Архив загружен на Google Drive: {destination_path}\")"
      ],
      "metadata": {
        "id": "BtKQBP8PzaL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fed8d83-8275-4eae-f33d-8287df2b405f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "🟢 [LOG] ✅ Архив загружен на Google Drive: /content/drive/My Drive/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving processor tensors.\n",
        "Do not forget save\n",
        "tensor_index.pkl,\n",
        "DeepSeek-R1-Distill-Qwen-1.5B_name_id_map.pkl,\n",
        "DeepSeek-R1-Distill-Qwen-1.5B_proc_map.pkl"
      ],
      "metadata": {
        "id": "6Xe_92F8TGpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Архивация и скачивание\n",
        "import shutil\n",
        "shutil.make_archive(\"g500\", \"zip\", \"data/db/g500\")\n",
        "zip_name = \"g500.zip\""
      ],
      "metadata": {
        "id": "9_lvAEfTTdVV"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}