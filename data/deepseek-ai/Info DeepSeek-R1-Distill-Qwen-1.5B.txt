# script to google colab to creating blocks of virtual matrix with DeepSeek-R1-Distill-Qwen-1.5B

# Установка зависимостей
!pip install torch transformers

import torch
from transformers import AutoModelForCausalLM
import os
from pathlib import Path
import shutil
import gc
from huggingface_hub import login
from google.colab import userdata

# Очистка диска от предыдущих блоков
!rm -rf blocks/

# Аутентификация с Hugging Face
hf_token = userdata.get('HF_TOKEN')
if not hf_token:
    raise ValueError("Добавьте HF_TOKEN в секреты Colab!")
login(hf_token)
print("Аутентификация с Hugging Face прошла успешно")

# Класс VirtualMatrix
class VirtualMatrix:
    def __init__(self, block_size=(1024, 1024)):  # Новый размер
        self.block_size = block_size
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def allocate_model(self, model_name, model, output_dir="blocks"):
        total_params = sum(p.numel() for p in model.parameters())
        width = self.block_size[1]
        height = (total_params + width - 1) // width
        padded_height = ((height + self.block_size[0] - 1) // self.block_size[0]) * self.block_size[0]
        
        print(f"Общее количество параметров: {total_params}")
        print(f"Размер матрицы: [{padded_height}, {width}]")
        
        os.makedirs(output_dir, exist_ok=True)
        
        flat_params = torch.cat([p.view(-1).to("cpu", dtype=torch.float16) 
                                for p in model.parameters()])
        print(f"Размер flat_params: {flat_params.numel()} элементов, {flat_params.element_size() * flat_params.numel() / 1024 / 1024:.2f} MB")
        
        del model
        gc.collect()
        
        padded_size = padded_height * width
        padded_params = torch.zeros(padded_size, dtype=torch.float16)
        padded_params[:total_params] = flat_params
        del flat_params
        gc.collect()
        
        matrix = padded_params.view(padded_height, width)
        print(f"Размер matrix: {matrix.shape}, {matrix.element_size() * matrix.numel() / 1024 / 1024:.2f} MB")
        
        num_blocks = 0
        for i in range(0, padded_height, self.block_size[0]):
            for j in range(0, width, self.block_size[1]):
                block = matrix[i:i + self.block_size[0], j:j + self.block_size[1]]
                coords = (i // self.block_size[0], j // self.block_size[1])
                block_name = f"{model_name}_row{coords[0]}_col{coords[1]}"
                block_path = f"{output_dir}/{block_name}.pt"
                
                expected_size = self.block_size[0] * self.block_size[1] * 2 / 1024 / 1024  # MB в float16
                actual_size = block.element_size() * block.numel() / 1024 / 1024  # MB
                print(f"Сохранение блока {block_name}: {block.shape}, {actual_size:.2f} MB (ожидаемый: {expected_size:.2f} MB)")
                
                torch.save(block.clone(), block_path)
                num_blocks += 1
                
                del block
                gc.collect()
        
        print(f"Модель {model_name} разбита на {num_blocks} блоков")
        del matrix
        gc.collect()

# Загрузка модели
model_name = "DeepSeek-R1-Distill-Qwen-1.5B"
print(f"Загрузка модели {model_name}...")
model = AutoModelForCausalLM.from_pretrained(f"deepseek-ai/{model_name}", torch_dtype=torch.float16)

# Разложение модели на блоки
virtual_matrix = VirtualMatrix(block_size=(1024, 1024))
virtual_matrix.allocate_model(model_name, model)

# Архивирование блоков
shutil.make_archive("model_blocks", "zip", "blocks")
print("Блоки сохранены в model_blocks.zip")
