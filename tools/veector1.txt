===== Код из файла: /workspaces/Veector/src/observervatoria.py =====

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import gc
import psutil
import random

logger = logging.getLogger(__name__)

class TokenAgent(nn.Module):
    def __init__(self, dispatcher, token_idx, hidden_size, rank):
        super().__init__()
        self.dispatcher = dispatcher
        self.device = dispatcher.device
        self.token_idx = token_idx
        self.hidden_size = hidden_size
        self.rank = rank
        self.attn_scorer = nn.Linear(hidden_size, 1, dtype=torch.float16)
        self.norm = nn.LayerNorm(hidden_size, dtype=torch.float16)  # Добавляем нормализацию

    def process_attention(self, hidden_states, layer_idx, active_rooms):
        hidden_states = hidden_states.to(torch.float16)
        if self.rank == "Lackey":
            logger.info(f"Token {self.token_idx} (Lackey) skipped")
            return torch.zeros(1, 1, self.hidden_size, dtype=torch.float16, device=self.device)

        batch_size, seq_len, _ = hidden_states.shape
        mask = torch.triu(torch.ones(seq_len, seq_len, device=self.device, dtype=torch.float16), diagonal=1) * -1e9
        mask = mask.unsqueeze(0).unsqueeze(1)

        block_key_q = f"{self.dispatcher.model_name}_layer{layer_idx}_self_attn_q_proj_weight_block0"
        block_key_k = f"{self.dispatcher.model_name}_layer{layer_idx}_self_attn_k_proj_weight_block0"
        block_key_v = f"{self.dispatcher.model_name}_layer{layer_idx}_self_attn_v_proj_weight_block0"
        block_key_o = f"{self.dispatcher.model_name}_layer{layer_idx}_self_attn_o_proj_weight_block0"

        q_block = self.dispatcher.load_block(block_key_q)  # [1536, 1536]
        k_block = self.dispatcher.load_block(block_key_k)  # [256, 1536]
        v_block = self.dispatcher.load_block(block_key_v)  # [256, 1536]
        o_block = self.dispatcher.load_block(block_key_o)  # [1536, 1536]

        key_dim = k_block.shape[0]  # 256
        num_heads = 4  # Оставляем 4 для экономии
        total_dim = key_dim * len(active_rooms)  # 768 (256 * 3)
        head_dim = total_dim // num_heads  # 192

        q_rooms = [q_block[i*key_dim:(i+1)*key_dim, :] for i in range(self.hidden_size // key_dim)]
        k_rooms = [k_block for _ in range(self.hidden_size // key_dim)]
        v_rooms = [v_block for _ in range(self.hidden_size // key_dim)]
        o_rooms = [o_block[i*key_dim:(i+1)*key_dim, :] for i in range(self.hidden_size // key_dim)]

        q_outs, k_outs, v_outs = [], [], []
        with torch.no_grad():
            for room_idx in active_rooms:
                q = torch.matmul(hidden_states, q_rooms[room_idx].t())
                k = torch.matmul(hidden_states, k_rooms[room_idx].t())
                v = torch.matmul(hidden_states, v_rooms[room_idx].t())
                q_outs.append(q)
                k_outs.append(k)
                v_outs.append(v)

        q = torch.cat(q_outs, dim=-1)  # [1, seq_len, 768]
        k = torch.cat(k_outs, dim=-1)  # [1, seq_len, 768]
        v = torch.cat(v_outs, dim=-1)  # [1, seq_len, 768]

        q = q.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)
        k = k.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)
        v = v.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)

        scores = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)
        scores = scores + mask
        attn_weights = F.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v).permute(0, 2, 1, 3).reshape(batch_size, seq_len, total_dim)

        o_active = torch.cat([o_rooms[i] for i in active_rooms], dim=0)  # [768, 1536]
        o_out = torch.matmul(attn_output, o_active)
        o_out = self.norm(o_out)  # Нормализация выхода

        del q, k, v, attn_weights, attn_output, q_outs, k_outs, v_outs, o_active
        return o_out[:, self.token_idx:self.token_idx+1]

class Observer(nn.Module):
    def __init__(self, dispatcher, max_layers=28, top_k=10):
        super().__init__()
        self.dispatcher = dispatcher
        self.device = dispatcher.device
        self.hidden_size = dispatcher.hidden_size
        self.num_layers = min(dispatcher.num_layers, max_layers)
        self.query_scorer = nn.Linear(self.hidden_size, 3, dtype=torch.float16)
        self.layer_scorer = nn.Linear(self.hidden_size, self.num_layers, dtype=torch.float16)
        self.room_scorer = nn.Linear(self.hidden_size, self.hidden_size // 256, dtype=torch.float16)
        self.attn_scorer = nn.Linear(self.hidden_size, 1, dtype=torch.float16)
        self.top_k = top_k
        self.norm = nn.LayerNorm(self.hidden_size, dtype=torch.float16)

    def classify_query(self, hidden_states):
        scores = self.query_scorer(hidden_states.mean(dim=1)).softmax(dim=-1)[0]
        return "light" if scores[0] > max(scores[1], scores[2]) else "medium" if scores[1] > scores[2] else "deep"

    def classify_tokens(self, hidden_states):
        scores = self.attn_scorer(hidden_states).squeeze(-1)[0]
        sorted_scores = scores.argsort(descending=True)
        ranks = {idx.item(): "Boss" if i < 2 else "Racketeer" if i < 4 else "Thief" if i < 6 else "Lackey"
                 for i, idx in enumerate(sorted_scores)}
        return ranks

    def select_layers(self, hidden_states, strategy):
        scores = self.layer_scorer(hidden_states.mean(dim=1))[0]
        num_active = {"light": 3, "medium": 5, "deep": 7}[strategy]
        return scores.argsort(descending=True)[:num_active].tolist()

    def select_rooms(self, hidden_states):
        scores = self.room_scorer(hidden_states.mean(dim=1))[0]
        return scores.argsort(descending=True)[:3].tolist()

    def process_layer(self, hidden_states, layer_idx):
        ranks = self.classify_tokens(hidden_states)
        active_rooms = self.select_rooms(hidden_states)
        tokenagents = [TokenAgent(self.dispatcher, idx, self.hidden_size, ranks[idx]) 
                   for idx in range(hidden_states.shape[1])]
        layer_states = [tokenagent.process_attention(hidden_states, layer_idx, active_rooms) for tokenagent in tokenagents]
        return torch.cat(layer_states, dim=1)

    def forward(self, input_ids, tokenizer, return_top_k=False):
        with torch.no_grad():
            hidden_states = self.dispatcher.virtual_matrix.embedding(input_ids, f"{self.dispatcher.model_name}_embed").to(torch.float16)
            logger.info(f"Embeddings shape: {hidden_states.shape}")

            strategy = self.classify_query(hidden_states)
            active_layers = self.select_layers(hidden_states, strategy)
            logger.info(f"Active layers: {active_layers}")

            for layer_idx in active_layers:
                hidden_states = self.process_layer(hidden_states, layer_idx)
                hidden_states = self.norm(hidden_states)  # Нормализация после слоя
                self.clear_memory()

            if return_top_k:
                next_token_hidden = hidden_states[:, -1, :]
                output_blocks = sorted([key for key in self.dispatcher.metadata.keys() if "_output" in key],
                                       key=lambda x: int(x.split("_block")[1]))
                all_logits = []
                for block_key in output_blocks:
                    block = self.dispatcher.load_block(block_key).to(torch.float16)
                    logits = torch.matmul(next_token_hidden, block.t())
                    all_logits.append(logits)
                    del block
                    self.clear_memory()
                next_token_logits = torch.cat(all_logits, dim=-1)
                top_k_logits, top_k_indices = torch.topk(next_token_logits, k=self.top_k, dim=-1)
                probs = F.softmax(top_k_logits / 0.7, dim=-1)
                next_token_id = top_k_indices[0, torch.multinomial(probs, 1).squeeze()]
                logger.info(f"Next token ID: {next_token_id.item()}, decoded: {tokenizer.decode([next_token_id.item()])}")
                return next_token_id
            return hidden_states

    def clear_memory(self):
        self.dispatcher.virtual_matrix.clear_cache()
        gc.collect()
        logger.info(f"Очистка памяти, RAM: {psutil.Process().memory_info().rss / 1024**2:.2f} MB")

===== Код из файла: /workspaces/Veector/src/core.py =====

# /workspaces/Veector/src/core.py
import numpy as np
import torch
import torch.nn as nn
import queue
import threading
import time
import random
from qiskit import QuantumCircuit
from qiskit.primitives import Sampler  # Для измерений (если нужно позже)
from qiskit_aer import AerSimulator  # Для statevector симуляции
from qiskit_aer.noise import NoiseModel, depolarizing_error
from veectordb import VeectorDB
from operations import (
    mod, floor, ceil, arcsin, arccos, arctan, xor, nand, nor, matrix_multiply,
    gradient_descent, softmax, matrix_determinant, matrix_eigenvalues, matrix_lu_decomposition,
    convolution, transpose, mean, std_dev, relu, leaky_relu, batch_norm, sigmoid,
    exponential_smoothing, normalize, interpolate, inverse, trace, random_uniform,
    random_normal, median, dropout, self_attention, layer_normalization,
    multi_head_attention, quantum_hadamard, quantum_pauli_x, quantum_cnot,
    quantum_measure, quantum_superposition, quantum_entanglement, causal_mask, masked_fill
)
from memory import Memory
from evolution import Evolution
from model_manager import ModelManager
from sync import P2PNode
from tensors import create_tensor, validate_tensor, reshape_tensor, get_tensor_metadata
import ipfshttpclient
import os

class NeuralStorage(nn.Module):
    def __init__(self, input_dim=16, hidden_dim=64, bottleneck_dim=32, activation_fn=nn.ReLU):
        super(NeuralStorage, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            activation_fn(),
            nn.Linear(hidden_dim, bottleneck_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(bottleneck_dim, hidden_dim),
            activation_fn(),
            nn.Linear(hidden_dim, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

class Veector:
    def __init__(self, db_path="../data/db/veectordb.json", use_neural_storage=False, cache_size=1000,
                 eviction_strategy="LRU", dropout_rate=0.0, use_memory=False, model_manager=None, 
                 p2p_node=None, ipfs_enabled=True, ipfs_address='/ip4/127.0.0.1/tcp/5001'):
        self.db = VeectorDB(db_path)
        self.use_neural_storage = use_neural_storage
        self.neural_model = None
        self.max_coord = 0
        self.space = {}
        self.neural_embeddings = {}
        self.sync_queue = queue.Queue()
        self.cache = {}
        self.cache_size = cache_size
        self.eviction_strategy = eviction_strategy.upper()
        self.cache_access_count = {}
        self.cache_timestamps = {}
        self.dropout_rate = dropout_rate
        self.use_memory = Memory() if use_memory else None
        self.evolution = Evolution(self)        
        self.p2p_node = p2p_node        
        self.ipfs_client = None
        if ipfs_enabled and p2p_node and ipfs_address:  # Подключаем IPFS только если ipfs_enabled=True
            self.ipfs_client = ipfshttpclient.connect(addr=ipfs_address) if ipfs_address else None
        self.model_manager = model_manager or ModelManager(self)
        self.models_dir = "../data/models"
        self.tensors_dir = "../data/tensors"

        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.tensors_dir, exist_ok=True)

        if use_neural_storage:
            self._init_neural_storage()
        self._start_sync_thread()

        self.core = {
            # Арифметика (0-9)
            (0, 0, 0): lambda x: np.sum(x, dtype=np.complex128),
            (0, 0, 1): lambda x: x[0] - x[1],
            (0, 1, 0): lambda x: x[0] * x[1],
            (0, 1, 1): lambda x: x[0] / x[1],
            (0, 2, 0): lambda x: np.sqrt(x[0], dtype=np.complex128),
            (0, 2, 1): lambda x: np.power(x[0], x[1], dtype=np.complex128),
            (0, 3, 0): lambda x: np.abs(x[0]),
            (0, 4, 0): lambda x: np.dot(x[0], x[1]) if x[0].shape[1] == x[1].shape[0] else None,
            (0, 5, 0): lambda x: mod(x[0], x[1]),
            (0, 6, 0): lambda x: floor(x[0]),
            (0, 6, 1): lambda x: ceil(x[0]),

            # Тригонометрия (1)
            (1, 0, 0): lambda x: np.sin(x[0], dtype=np.complex128),
            (1, 0, 1): lambda x: np.cos(x[0], dtype=np.complex128),
            (1, 1, 0): lambda x: np.tan(x[0], dtype=np.complex128),
            (1, 1, 1): lambda x: 1 / np.tan(x[0], dtype=np.complex128) if np.tan(x[0]) != 0 else np.nan,
            (1, 2, 0): lambda x: arcsin(x[0]),
            (1, 2, 1): lambda x: arccos(x[0]),
            (1, 3, 0): lambda x: arctan(x[0]),

            # Логика (2)
            (2, 0, 0): lambda x: 1 if x[0] > x[1] else 0,
            (2, 0, 1): lambda x: 1 if x[0] == x[1] else 0,
            (2, 1, 0): lambda x: 1 if x[0] and x[1] else 0,
            (2, 1, 1): lambda x: 1 if x[0] or x[1] else 0,
            (2, 2, 0): lambda x: 1 if not x[0] else 0,
            (2, 3, 0): lambda x: xor(x[0], x[1]),
            (2, 4, 0): nand,
            (2, 4, 1): nor,

            # Условные операции (3)
            (3, 0, 0): lambda x, t, f: t if x[0] else f,

            # Циклы (4)
            (4, 0, 0): lambda x, n: x[0] * n,

            # Рандом (5)
            (5, 1, 0): lambda x: random_uniform(x[0], x[1]),
            (5, 1, 1): lambda x: random_normal(x[0], x[1]),
            (5, 2, 0): lambda x: median(x[0]),

            # Выбор (7)
            (7, 0, 0): lambda x, *opts: opts[int(x[0])] if opts else None,

            # Вывод (8)
            (8, 0, 0): lambda x: print(f"Output: {x[0]}") or x[0],

            # Идентичность (9)
            (9, 0, 0): lambda x: x[0],

            # Эволюция (10)
            (10, 0, 0): lambda x: self._reason(x),

            # Графовые операции (15)
            (15, 0, 0): lambda x: self._dfs(x[0], x[1]),

            # Статистика (16)
            (16, 0, 0): lambda x: np.mean(x[0], dtype=np.complex128),
            (16, 1, 0): lambda x: np.std(x[0], dtype=np.complex128),

            # Регуляризация (17)
            (17, 0, 0): lambda x: self._dropout(x[0]),

            # Активации (18)
            (18, 0, 0): lambda x: np.maximum(x[0], 0),
            (18, 1, 0): lambda x: 1 / (1 + np.exp(-x[0])),
            (18, 2, 0): softmax,
            (18, 3, 0): leaky_relu,

            # Сглаживание (19)
            (19, 0, 0): exponential_smoothing,

            # Нормализация (20)
            (20, 0, 0): normalize,
            (20, 1, 0): interpolate,

            # Матричные операции (30)
            (30, 0, 0): matrix_multiply,
            (30, 1, 0): matrix_determinant,
            (30, 2, 0): matrix_eigenvalues,
            (30, 3, 0): convolution,
            (30, 4, 0): transpose,
            (30, 5, 0): inverse,
            (30, 6, 0): trace,

            # Нейросетевые операции (40)
            (40, 0, 0): lambda x: self.model_manager.perform_inference("DeepSeek-R1-Distill-Qwen-1.5B", x),
            (40, 1, 0): layer_normalization,
            (40, 2, 0): lambda x: multi_head_attention(x, num_heads=8),
            (40, 3, 0): lambda x: dropout(x[0], rate=0.5),
            (40, 4, 0): batch_norm,

            # Квантовые операции (50)
            (50, 0, 0): lambda x: self._quantum_operation(x[0], "hadamard"),
            (50, 0, 1): lambda x: self._quantum_operation(x[0], "pauli_x"),
            (50, 1, 0): lambda x: self._quantum_operation([x[0], x[1]], "cnot"),
            (50, 2, 0): lambda x: self._quantum_operation(x[0], "measure"),
            (50, 3, 0): lambda x: self._quantum_operation(x[0], "superposition"),
            (50, 4, 0): lambda x: self._quantum_operation([x[0], x[1]], "entanglement"),
        }

    def _quantum_operation(self, data, op_type):
        """Выполнение квантовых операций через Qiskit 1.4.1 с шумом."""
        if isinstance(data, list):
            num_qubits = len(data)
            initial_state = np.array(data, dtype=np.complex128).flatten()
        else:
            num_qubits = 1
            initial_state = np.array([data, 0], dtype=np.complex128) if np.isscalar(data) else data

        # Нормализация начального состояния
        initial_state = initial_state / np.linalg.norm(initial_state)

        # Создаём квантовую цепь
        qc = QuantumCircuit(num_qubits)
        qc.initialize(initial_state, range(num_qubits))

        # Применяем операцию
        if op_type == "hadamard":
            qc.h(0)
        elif op_type == "pauli_x":
            qc.x(0)
        elif op_type == "cnot" and num_qubits >= 2:
            qc.cx(0, 1)
        elif op_type == "measure":
            qc.measure_all()
        elif op_type == "superposition":
            qc.h(0)
        elif op_type == "entanglement" and num_qubits >= 2:
            qc.h(0)
            qc.cx(0, 1)

        # Добавляем квантовый шум (деполяризация)
        noise_model = NoiseModel()
        error = depolarizing_error(0.05, num_qubits)  # 5% шум
        noise_model.add_all_qubit_quantum_error(error, ['h', 'x', 'cx'])

        # Симуляция через AerSimulator
        simulator = AerSimulator(method='statevector')
        result = simulator.run(qc, noise_model=noise_model).result()
        statevector = result.get_statevector()
        return np.array(statevector, dtype=np.complex128)

    def _apply_quantum_ops(self, op, data):
        """Устаревший метод, теперь используется _quantum_operation."""
        return data

    def _apply_neural_ops(self, op, data):
        """Устаревший метод, теперь операции в self.core."""
        return data

    def _next_coords(self):
        coords = max([key[1][0] for key in self.space.keys()] + [self.max_coord]) + 1
        self.max_coord = coords
        return [coords, coords, coords]

    def _init_neural_storage(self):
        print("Инициализация нейронного хранилища")
        input_dim = self._get_max_input_dim()
        self.neural_model = NeuralStorage(input_dim=input_dim, activation_fn=nn.ReLU)
        self.neural_optimizer = torch.optim.Adam(self.neural_model.parameters(), lr=0.001)
        self.neural_loss = nn.MSELoss()
        self._train_neural_storage()

    def _get_max_input_dim(self):
        results = self.db.find_by_type("tensor_result")
        max_dim = 16
        for doc in results:
            result = doc["data"]
            if isinstance(result, np.ndarray):
                flat_len = len(result.flatten())
                max_dim = max(max_dim, flat_len)
        return max_dim

    def _train_neural_storage(self):
        results = self.db.find_by_type("tensor_result")
        if not results:
            print("Нет данных для обучения нейросети")
            return

        input_dim = self._get_max_input_dim()
        train_data = []
        for doc in results:
            result = doc["data"]
            if isinstance(result, (int, float, complex)):
                data = np.array([complex(result)] + [0] * (input_dim - 1), dtype=np.complex128)
            elif isinstance(result, list) and all(isinstance(x, (int, float, complex)) for x in result):
                flat = np.array(result, dtype=np.complex128).flatten()
                data = np.pad(flat, (0, max(0, input_dim - len(flat))), mode='constant')[:input_dim]
            else:
                data = np.array([0] * input_dim, dtype=np.complex128)
            train_data.append(np.real(data))

        train_data = torch.tensor(train_data, dtype=torch.float32)

        for epoch in range(50):
            self.neural_optimizer.zero_grad()
            encoded, decoded = self.neural_model(train_data)
            loss = self.neural_loss(decoded, train_data)
            loss.backward()
            self.neural_optimizer.step()
            if epoch % 10 == 0:
                print(f"Эпоха {epoch + 1}, Loss: {loss.item()}")

    def _store_in_neural(self, result, doc_id):
        if not self.use_neural_storage or not self.neural_model:
            return

        input_dim = self._get_max_input_dim()
        if isinstance(result, (int, float, complex, np.number)):
            data = np.array([complex(result)] + [0] * (input_dim - 1), dtype=np.complex128)
        elif isinstance(result, np.ndarray):
            flat = result.flatten()
            data = np.pad(flat, (0, max(0, input_dim - len(flat))), mode='constant')[:input_dim]
        else:
            data = np.array([0] * input_dim, dtype=np.complex128)

        tensor_data = torch.tensor(np.real(data), dtype=torch.float32)
        encoded, _ = self.neural_model(tensor_data)
        self.neural_embeddings[doc_id] = encoded.detach().numpy()
        print(f"Сохранено в нейросеть: {doc_id} -> {encoded.detach().numpy()[:5]}...")

    def _retrieve_from_neural(self, doc_id):
        if not self.use_neural_storage or not self.neural_model or doc_id not in self.neural_embeddings:
            return None

        encoded = torch.tensor(self.neural_embeddings[doc_id], dtype=torch.float32)
        decoded = self.neural_model.decoder(encoded)
        return decoded.detach().numpy()

    def _start_sync_thread(self):
        def sync_worker():
            while True:
                peer_veector = self.sync_queue.get()
                self._sync_neural_blocking(peer_veector)
                self.sync_queue.task_done()

        t = threading.Thread(target=sync_worker, daemon=True)
        t.start()

    def _sync_neural_blocking(self, peer_veector):
        if not self.use_neural_storage or not peer_veector.use_neural_storage:
            return

        if not self.neural_model or not peer_veector.neural_model:
            return

        print("Синхронизация нейронных моделей (федеративное обучение)")
        self_data_count = len(self.db.find_by_type("tensor_result"))
        peer_data_count = len(peer_veector.db.find_by_type("tensor_result"))
        total_data = self_data_count + peer_data_count

        if total_data == 0:
            return

        state_dict = self.neural_model.state_dict()
        peer_state_dict = peer_veector.neural_model.state_dict()

        for key in state_dict:
            self_weight = self_data_count / total_data
            peer_weight = peer_data_count / total_data
            state_dict[key] = self_weight * state_dict[key] + peer_weight * peer_state_dict[key]

        self.neural_model.load_state_dict(state_dict)

    def sync_neural(self, peer_veector):
        self.sync_queue.put(peer_veector)

    def _reason(self, x):
        print(f"Reason input: {x}")
        if self.use_memory:
            cached_result = self.use_memory.retrieve(x)
            if cached_result is not None:
                print(f"Использована память для Reason: {x} -> {cached_result}")
                return cached_result
        
        if isinstance(x, (int, float, complex, np.number)):
            result = self._apply_rl_strategy(x)
        elif isinstance(x, list):
            result = self._evolve_program(x)
        elif isinstance(x, np.ndarray):
            result = self._optimize_tensor(x)
        else:
            result = self.evolution.evolve(x)

        if self.use_memory:
            self.use_memory.store(x, result, reward=self._calculate_reward(result, x))
        
        print(f"Reason result: {result}")
        return result

    def _apply_rl_strategy(self, x):
        """Обучение с подкреплением для числовых данных (заглушка)."""
        return x

    def _evolve_program(self, x):
        """Эволюция программ (заглушка)."""
        return x

    def _optimize_tensor(self, x):
        """Оптимизация тензоров (заглушка)."""
        return x

    def _calculate_reward(self, result, input_data):
        if isinstance(input_data, (int, float, complex)):
            return 1.0 if np.abs(result) > np.abs(input_data) else -1.0
        elif isinstance(input_data, np.ndarray):
            return -np.mean(np.abs(result - input_data) ** 2)
        return 0.0

    def _dfs(self, graph, start):
        visited = set()
        result = []

        def dfs(node):
            if node not in visited:
                visited.add(node)
                result.append(node)
                for neighbor in graph.get(node, []):
                    dfs(neighbor)

        dfs(start)
        return result

    def _lru_cache_evict(self):
        if len(self.cache) >= self.cache_size:
            oldest_key = min(self.cache_timestamps, key=self.cache_timestamps.get)
            del self.cache[oldest_key]
            del self.cache_timestamps[oldest_key]
            if oldest_key in self.cache_access_count:
                del self.cache_access_count[oldest_key]

    def _lfu_cache_evict(self):
        if len(self.cache) >= self.cache_size:
            least_frequent_key = min(self.cache_access_count, key=self.cache_access_count.get)
            del self.cache[least_frequent_key]
            del self.cache_access_count[least_frequent_key]
            if least_frequent_key in self.cache_timestamps:
                del self.cache_timestamps[least_frequent_key]

    def _dropout(self, x):
        if self.dropout_rate > 0 and isinstance(x, np.ndarray):
            mask = (np.random.rand(*x.shape) < self.dropout_rate)
            x[mask] = 0
        return x

    def _store_tensor_in_ipfs(self, tensor_data):
        """Сохранение тензора в IPFS."""
        if not self.ipfs_client:
            return None
        try:
            res = self.ipfs_client.add(tensor_data.tobytes())
            return res['Hash']
        except Exception as e:
            print(f"Ошибка сохранения в IPFS: {e}")
            return None

    def _load_tensor_from_ipfs(self, ipfs_hash, shape, dtype=np.complex128):
        """Загрузка тензора из IPFS."""
        if not self.ipfs_client:
            return None
        try:
            data = self.ipfs_client.cat(ipfs_hash)
            return np.frombuffer(data, dtype=dtype).reshape(shape)
        except Exception as e:
            print(f"Ошибка загрузки из IPFS: {e}")
            return None

    def _save_model_metadata(self, model_name, ipfs_hash):
        """Сохранение метаданных модели в veectordb."""
        model_metadata = {
            "name": model_name,
            "ipfs_hash": ipfs_hash,
            "location": "ipfs",
            "timestamp": time.time()
        }
        self.db.insert_model(model_name, model_metadata)
        print(f"Метаданные модели сохранены: {model_name} -> {ipfs_hash}")

    def load_model(self, model_name):
        """Загрузка метаданных модели."""
        model_metadata = self.db.get_model_metadata(model_name)
        if model_metadata:
            return model_metadata
        else:
            print(f"Модель {model_name} не найдена в базе данных.")
            return None

    def save_tensor(self, tensor, tensor_id, use_ipfs=True):
        """Сохранение тензора в IPFS или локально."""
        if use_ipfs and self.ipfs_client:
            ipfs_hash = self._store_tensor_in_ipfs(tensor)
            if ipfs_hash:
                tensor_metadata = {
                    "tensor_id": tensor_id,
                    "ipfs_hash": ipfs_hash,
                    "shape": tensor.shape,
                    "dtype": str(tensor.dtype),
                    "location": "ipfs",
                    "timestamp": time.time()
                }
                self.db.insert_tensor(tensor_id, tensor_metadata)
                print(f"Тензор {tensor_id} сохранён в IPFS: {ipfs_hash}")
                return ipfs_hash
            else:
                print(f"Не удалось сохранить тензор {tensor_id} в IPFS.")
                return None
        else:
            tensor_path = os.path.join(self.tensors_dir, f"{tensor_id}.npy")
            np.save(tensor_path, tensor)
            tensor_metadata = {
                "tensor_id": tensor_id,
                "path": tensor_path,
                "shape": tensor.shape,
                "dtype": str(tensor.dtype),
                "location": "local",
                "timestamp": time.time()
            }
            self.db.insert_tensor(tensor_id, tensor_metadata)
            print(f"Тензор {tensor_id} сохранён локально: {tensor_path}")
            return tensor_path

    def load_tensor(self, tensor_id):
        """Загрузка тензора из IPFS или локального хранилища."""
        tensor_metadata = self.db.get_tensor_metadata(tensor_id)
        if not tensor_metadata:
            print(f"Тензор {tensor_id} не найден в базе данных.")
            return None

        if tensor_metadata["location"] == "ipfs":
            ipfs_hash = tensor_metadata["ipfs_hash"]
            shape = tensor_metadata["shape"]
            dtype = np.dtype(tensor_metadata["dtype"])
            tensor_data = self._load_tensor_from_ipfs(ipfs_hash, shape, dtype)
            if tensor_data is not None:
                print(f"Тензор {tensor_id} загружен из IPFS: {ipfs_hash}")
                return tensor_data
            else:
                print(f"Не удалось загрузить тензор {tensor_id} из IPFS.")
                return None
        elif tensor_metadata["location"] == "local":
            tensor_path = tensor_metadata["path"]
            try:
                tensor_data = np.load(tensor_path)
                print(f"Тензор {tensor_id} загружен локально: {tensor_path}")
                return tensor_data
            except Exception as e:
                print(f"Ошибка загрузки тензора из локального файла: {e}")
                return None
        else:
            print(f"Неизвестное местоположение тензора: {tensor_metadata['location']}")
            return None

    def compute(self, tensor):
        if not validate_tensor(tensor):
            return tensor

        data_layer, data_coords, data, data_length = tensor[0]
        op_layer, op_coords, op, op_length = tensor[1]
        context = tensor[2]
        version = tensor[3]
        next_coords = tensor[4] if len(tensor) > 4 else []
        metadata = get_tensor_metadata(tensor)

        cache_key = (tuple(data_layer), tuple(data_coords), tuple(op))
        if cache_key in self.cache:
            self.cache_access_count[cache_key] = self.cache_access_count.get(cache_key, 0) + 1
            self.cache_timestamps[cache_key] = time.time()
            return self.cache[cache_key]

        if isinstance(data, list):
            data = [self.compute(d) if isinstance(d, list) else d for d in data]
        if len(data) == 1 and tuple(op) in [(2, 1, 0), (2, 1, 1), (2, 2, 0), (2, 2, 1)]:
            data = data
        elif len(data) == 1 and not isinstance(data[0], list):
            data = data[0]

        op_func = self.core.get(tuple(op), lambda x: x)

        if op == [3, 0, 0]:
            cond = self.compute(data[0]) if isinstance(data, list) else data
            true_val = self.compute(data[1])
            false_val = self.compute(data[2])
            result = op_func([cond], true_val, false_val)
        elif op == [4, 0, 0]:
            if isinstance(data, list) and len(data) > 1:
                result = op_func(data[0], data[1])
            else:
                result = op_func(data, 1)
        elif op == [5, 0, 0]:
            opts = [self.compute(opt) for opt in data[1:]]
            result = op_func(data[0], *opts)
        else:
            if isinstance(data, list) and tuple(op) not in [(2, 1, 0), (2, 1, 1), (2, 2, 0), (2, 2, 1)]:
                data = np.array(data, dtype=np.complex128)
            if self.dropout_rate > 0 and op != [59, 0, 0]:
                data = self._dropout(data)

            result = op_func(data)

        tensor_id = f"tensor_result_{time.time()}_{random.randint(1000, 9999)}"
        self.save_tensor(result, tensor_id, use_ipfs=self.ipfs_client is not None)
        metadata = {"tensor": tensor, "coords": (data_layer, data_coords), "tensor_id": tensor_id}

        if self.use_neural_storage and self.neural_model:
            self._store_in_neural(result, tensor_id)

        if self.p2p_node:
            sync_data = np.abs(result) if np.iscomplexobj(result) else result
            self.p2p_node.sync_tensor(sync_data, metadata)

        self.space[(tuple(data_layer), tuple(data_coords))] = tensor_id

        if len(self.cache) >= self.cache_size:
            if self.eviction_strategy == "LRU":
                self._lru_cache_evict()
            elif self.eviction_strategy == "LFU":
                self._lfu_cache_evict()
            else:
                self._lru_cache_evict()

        self.cache[cache_key] = result
        self.cache_access_count[cache_key] = 1
        self.cache_timestamps[cache_key] = time.time()

        return result

    def add_to_space(self, tensor):
        layer, coords = tensor[0][0], tuple(tensor[0][1])
        tensor_id = f"tensor_{time.time()}_{random.randint(1000, 9999)}"
        self.save_tensor(tensor[0][2], tensor_id, use_ipfs=self.ipfs_client is not None)
        self.space[(tuple(layer), coords)] = tensor_id

    def evolve_tensor(self, tensor):
        return self.evolution.log_evolution(tensor, self)

    def generate_program_tensor(self, prompt, max_steps=5):
        return self.model_manager.generate_program_tensor(prompt, max_steps)

    def share_program(self, program_tensors):
        return self.model_manager.share_program(program_tensors)

    def improve_program(self, program_tensors, feedback_data, iterations=3):
        return self.model_manager.improve_program(program_tensors, feedback_data, iterations)

    def execute_program(self, program_tensors, input_data=None):
        return self.model_manager.execute_program(program_tensors, input_data)

if __name__ == "__main__":
    # Пример использования
    p2p_node = P2PNode("localhost", 5000, use_ipfs=True)
    p2p_node.start()
    veector = Veector(p2p_node=p2p_node, use_memory=True)
    
    # Тест квантовой операции (Hadamard)
    tensor = create_tensor([0], [0, 0, 0], [1, 0], 2, op=[50, 0, 0])
    result = veector.compute(tensor)
    print(f"Результат квантовой операции Hadamard: {result}")

===== Код из файла: /workspaces/Veector/src/evolution.py =====

class Evolution:
    def __init__(self, veector):
        self.veector = veector
        self.evolution_strategy = "reason"  # Default strategy

    def evolve(self, tensor):
        """
        Выполняет эволюцию тензора в зависимости от выбранной стратегии.
        :param tensor: Тензор для эволюции.
        :return: Эволюционировавший тензор.
        """
        if self.evolution_strategy == "reason":
            return self._evolve_reason(tensor)
        elif self.evolution_strategy == "mutate":
            return self._evolve_mutate(tensor)
        else:
            raise ValueError(f"Неизвестная стратегия эволюции: {self.evolution_strategy}")

    def _evolve_reason(self, tensor):
        """
        Выполняет эволюцию тензора, используя операцию Reason.
        """
        # Выполняем операцию Reason
        evolved_tensor = self.veector.compute([
            tensor[0],
            [[0], tensor[0][1], [9, 0, 0], 1],  # Reason
            tensor[2],
            tensor[3],
            tensor[4] if len(tensor) > 4 else [],
            tensor[5] if len(tensor) > 5 else {}
        ])
        return evolved_tensor

    def _evolve_mutate(self, tensor, mutation_rate=0.1):
        """
        Выполняет эволюцию тензора путем случайной мутации.
        :param tensor: Тензор для мутации.
        :param mutation_rate: Вероятность мутации каждого элемента.
        :return: Мутировавший тензор.
        """
        mutated_data = tensor[0][2]
        if isinstance(mutated_data, np.ndarray):
            mask = np.random.rand(*mutated_data.shape) < mutation_rate
            mutation = np.random.normal(size=mutated_data.shape)
            mutated_data = np.where(mask, mutated_data + mutation, mutated_data)
        elif isinstance(mutated_data, list):
             mutated_data = [x + np.random.normal(0, 0.1) if np.random.rand() < mutation_rate else x for x in mutated_data]
        
        evolved_tensor = [
            tensor[0],
            tensor[1],
            mutated_data,
            tensor[3],
            tensor[4] if len(tensor) > 4 else [],
            tensor[5] if len(tensor) > 5 else {}
        ]
        return evolved_tensor
        
    def log_evolution(self, tensor):
        """
        Логирует процесс эволюции.
        :param tensor: Тензор для логирования.
        :return: Результат эволюции.
        """
        print(f"Начало эволюции для тензора: {tensor}")
        result = self.evolve(tensor)
        print(f"Результат эволюции: {result}")
        return result

    def set_evolution_strategy(self, strategy):
        """
        Устанавливает стратегию эволюции.
        :param strategy: Стратегия эволюции ("reason" или "mutate").
        """
        if strategy not in ["reason", "mutate"]:
            raise ValueError(f"Неподдерживаемая стратегия эволюции: {strategy}")
        self.evolution_strategy = strategy


===== Код из файла: /workspaces/Veector/src/federated_learning.py =====

def local_fine_tuning(user_data):
    # Загрузка только необходимых блоков
    classifier = matrix.load_block('classifier', block_hashes['classifier'])
    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)
    
    for text, label in user_data:
        inputs = tokenizer(text, return_tensors='pt').to(device)
        outputs = dynamic_inference(text)
        loss = F.cross_entropy(outputs, label)
        loss.backward()
        optimizer.step()
    
    # Сохранение обновлений в IPFS
    new_hash = client.add('classifier_updated.pt')['Hash']
    return new_hash

def sync_updates(new_hash):
    # Шифрование обновлений
    encrypted = encrypt(new_hash)
    client.add(encrypted)
    p2p.broadcast(encrypted)

===== Код из файла: /workspaces/Veector/src/file_transfer.py =====

# src/file_transfer.py
import socket
import tqdm
import os
import hashlib
import time
import secrets  # Для генерации случайных ключей шифрования
from cryptography.fernet import Fernet, InvalidToken
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.backends import default_backend
import base64

BUFFER_SIZE = 4096
SEPARATOR = "<SEPARATOR>"  # Используем уникальный разделитель
RESUME_POSITION_REQUEST = "<RESUME_POS>"  # Запрос позиции для возобновления передачи
KEY_LENGTH = 32  # Длина ключа шифрования (32 байта = 256 бит)

def generate_hash(file_path):
    """Генерирует хеш-сумму файла."""
    hasher = hashlib.sha256()
    with open(file_path, "rb") as file:
        while chunk := file.read(BUFFER_SIZE):
            hasher.update(chunk)
    return hasher.hexdigest()

def generate_key(password): # Добавлена функция generate_key
    password = password.encode()
    salt = os.urandom(16) # Generate a unique salt for each session
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=390000,
        backend=default_backend()
    )
    key = base64.urlsafe_b64encode(kdf.derive(password))
    return key, salt # Return the derived key and the salt

def encrypt_file(file_path, key):
    """Шифрует файл с использованием Fernet."""
    try:
        fernet = Fernet(key)
        with open(file_path, "rb") as file:
            data = file.read()
        encrypted_data = fernet.encrypt(data)
        return encrypted_data
    except Exception as e:
        print(f"Ошибка при шифровании файла: {e}")
        return None

def decrypt_file(encrypted_data, key):
     """Расшифровывает данные с использованием ключа Fernet."""
     try:
        fernet = Fernet(key)
        decrypted_data = fernet.decrypt(encrypted_data)
        return decrypted_data
     except InvalidToken:
        print("Invalid key - decryption failed")
        return None

def send_file(file_path, target_host, target_port, password=None):
    """
    Отправляет файл на указанный хост и порт с проверкой целостности,
    возможностью возобновления передачи и шифрованием.
    """
    try:
        filesize = os.path.getsize(file_path)
        filename = os.path.basename(file_path)  # Получаем только имя файла
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((target_host, target_port))
        print(f"Подключено к {target_host}:{target_port}, отправка {filename}...")

        # Генерируем хеш-сумму
        file_hash = generate_hash(file_path)

        # Шифруем файл, если указан пароль
        if password:
            key, salt = generate_key(password) # key generation added here
            encrypted_data = encrypt_file(file_path, key) # Encrypt_data now uses Key and Salt together to secure process
            if encrypted_data is None:
                s.close()
                return
            data_to_send = encrypted_data
            filesize = len(encrypted_data)  # Размер шифрованного файла
        else:
            with open(file_path, "rb") as file:
                data_to_send = file.read()

        # Отправляем имя файла, размер, хеш-сумму и наличие шифрования
        header = f"{filename}{SEPARATOR}{filesize}{SEPARATOR}{file_hash}{SEPARATOR}{password is not None}"
        s.send(header.encode())
        
        # Отправляем соль (если шифрование включено)
        if password:
            s.send(salt)

        # Отправляем файл с возможностью возобновления
        sent_bytes = 0
        progress = tqdm.tqdm(range(filesize), f"Отправка {filename}", unit="B", unit_scale=True, unit_divisor=1024)
        while sent_bytes < filesize:
            # Запрашиваем позицию для возобновления на стороне клиента
            s.send(RESUME_POSITION_REQUEST.encode())
            resume_position = int(s.recv(BUFFER_SIZE).decode())
            
            # Проверяем, нужно ли возобновлять
            if resume_position > sent_bytes:
                print(f"Возобновление отправки с позиции {resume_position}")
                sent_bytes = resume_position  # Обновляем sent_bytes
                progress.update(resume_position - progress.n)

            # Отправляем данные
            bytes_to_send = data_to_send[sent_bytes:sent_bytes + BUFFER_SIZE]
            s.sendall(bytes_to_send)
            sent_bytes += len(bytes_to_send)
            progress.update(len(bytes_to_send))

        s.close()
        print(f"{filename} успешно отправлен.")

    except Exception as e:
        print(f"Ошибка при отправке файла: {e}")

def receive_file(host, port, save_dir=".", password=None):
    """
    Принимает файл, сохраняя его в указанную директорию.
    """
    try:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.bind((host, port))
        s.listen(1)
        print(f"Ожидание входящего соединения на {host}:{port}...")

        conn, addr = s.accept()
        print(f"Подключено к {addr[0]}:{addr[1]}")

        # Получаем имя файла, размер, хеш-сумму и наличие шифрования
        received = conn.recv(BUFFER_SIZE).decode()
        filename, filesize, file_hash, is_encrypted = received.split(SEPARATOR)
        filename = os.path.basename(filename)  # Извлекаем имя файла из пути
        filesize = int(filesize)
        is_encrypted = is_encrypted.lower() == "true"

        # Получаем соль, если шифрование включено
        salt = None
        if is_encrypted:
            salt = conn.recv(16) # Corrected salt size
           
        file_path = os.path.join(save_dir, filename)
        received_bytes = 0
        
        # Проверяем наличие файла и определяем позицию для возобновления
        if os.path.exists(file_path):
            received_bytes = os.path.getsize(file_path)
            print(f"Файл {filename} уже существует. Возобновление с позиции {received_bytes}.")
        
        # Отправляем клиенту позицию для возобновления
        conn.send(str(received_bytes).encode())

        # Получаем файл
        progress = tqdm.tqdm(range(filesize), f"Приём {filename}", unit="B", unit_scale=True, unit_divisor=1024)
        with open(file_path, "ab" if received_bytes > 0 else "wb") as file:
            while received_bytes < filesize:
                bytes_to_read = min(BUFFER_SIZE, filesize - received_bytes)
                bytes_read = conn.recv(bytes_to_read)
                if not bytes_read:
                    break
                file.write(bytes_read)
                received_bytes += len(bytes_read)
                progress.update(len(bytes_read))
        conn.close()
        s.close()

        # Расшифровываем, если необходимо
        if is_encrypted and password:
            # Generate key with the provided password and received salt
            key, _ = generate_key(password)
            
            # Read the encrypted data from the file
            with open(file_path, "rb") as f:
                encrypted_data = f.read()
            
            # Decrypt the data
            decrypted_data = decrypt_file(encrypted_data, key)
            
            if decrypted_data:
                # Save the decrypted data back to the file, overwriting the encrypted content
                with open(file_path, "wb") as f:
                    f.write(decrypted_data)
                print(f"{filename} успешно расшифрован.")
            else:
                print(f"Не удалось расшифровать {filename}.")

        # Проверяем хеш-сумму
        received_hash = generate_hash(file_path)
        if received_hash == file_hash:
            print(f"Хеш-сумма {filename} совпадает. Файл успешно получен.")
        else:
            print(f"Хеш-сумма {filename} не совпадает. Файл повреждён.")

    except Exception as e:
        print(f"Ошибка при получении файла: {e}")

# Дополнительные функции
def list_files(target_host, target_port):
    """
    Запрашивает список файлов с удаленного хоста.
    """
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((target_host, target_port))
        s.send("LIST".encode())  # Отправляем команду LIST

        # Получаем данные (список файлов)
        data = s.recv(4096).decode()
        print("Список файлов на удаленном хосте:")
        print(data)
        s.close()
    except Exception as e:
        print(f"Ошибка при получении списка файлов: {e}")

def respond_to_list_request(host, port, directory="."):
    """
    Отвечает на запрос списка файлов.
    """
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.bind((host, port))
        s.listen(1)
        print(f"Ожидание запроса списка файлов на {host}:{port}...")

        conn, addr = s.accept()
        with conn:
            data = conn.recv(4096).decode()
            if data == "LIST":
                files = os.listdir(directory)
                conn.send("\n".join(files).encode())
                print("Список файлов отправлен.")
            else:
                print("Неизвестный запрос.")
        s.close()
    except Exception as e:
        print(f"Ошибка при отправке списка файлов: {e}")

# Пример использования
if __name__ == "__main__":
    # Режим отправки
    # send_file("large_file.zip", "127.0.0.1", 5001, password="mysecretpassword")

    # Режим приёма
    # receive_file("127.0.0.1", 5001, "received_files", password="mysecretpassword")

    # Запрос списка файлов
    # list_files("127.0.0.1", 5001)

    # Ответ на запрос списка файлов (запускаем в отдельном терминале)
    # respond_to_list_request("127.0.0.1", 5001)

    print("Закомментируйте или раскомментируйте нужные строки для запуска в нужном режиме.")


===== Код из файла: /workspaces/Veector/src/interface.py =====

# src/interface.py
import numpy as np
from datetime import datetime

def display_text(text, element_id=None):
    """Отображает текст."""
    element_id = element_id or f"text-{datetime.now().timestamp()}"
    return f"<div id='{element_id}'>{text}</div>"

def display_number(number, element_id=None):
    """Отображает число."""
    element_id = element_id or f"number-{datetime.now().timestamp()}"
    return f"<span id='{element_id}'>{number}</span>"

def display_image(image_data, element_id=None):
    """Отображает изображение."""
    element_id = element_id or f"image-{datetime.now().timestamp()}"
    # Предполагаем, что image_data - это base64 строка
    return f"<img id='{element_id}' src='data:image/png;base64,{image_data}'/>"

def create_button(text, onclick_action, element_id=None):
    """Создает кнопку."""
    element_id = element_id or f"button-{datetime.now().timestamp()}"
    return f"<button id='{element_id}' onclick='{onclick_action}'>{text}</button>"

def create_text_field(default_text="", element_id=None):
    """Создает текстовое поле."""
    element_id = element_id or f"text-field-{datetime.now().timestamp()}"
    return f"<input type='text' id='{element_id}' value='{default_text}'/>"

def create_slider(min_value, max_value, default_value, element_id=None):
    """Создает слайдер."""
    element_id = element_id or f"slider-{datetime.now().timestamp()}"
    return f"<input type='range' id='{element_id}' min='{min_value}' max='{max_value}' value='{default_value}'/>"

def create_grid_layout(elements, cols=3, element_id=None):
    """Создает сеточный макет."""
    element_id = element_id or f"grid-{datetime.now().timestamp()}"
    grid_html = f"<div id='{element_id}' style='display: grid; grid-template-columns: repeat({cols}, 1fr);'>"
    for element in elements:
        grid_html += f"<div>{element}</div>"
    grid_html += "</div>"
    return grid_html

def create_list_layout(elements, element_id=None):
    """Создает список элементов."""
    element_id = element_id or f"list-{datetime.now().timestamp()}"
    list_html = f"<ul id='{element_id}'>"
    for element in elements:
        list_html += f"<li>{element}</li>"
    list_html += "</ul>"
    return list_html

def create_tabbed_layout(tabs, element_id=None):
    """Создает макет с вкладками."""
    element_id = element_id or f"tabs-{datetime.now().timestamp()}"
    tab_headers = ""
    tab_contents = ""
    for i, (tab_name, tab_content) in enumerate(tabs.items()):
        tab_id = f"{element_id}-tab-{i}"
        tab_headers += f"<button onclick=\"showTab('{element_id}', '{tab_id}')\">{tab_name}</button>"
        tab_contents += f"<div id='{tab_id}' class='tab-content'> {tab_content}</div>"

    tabbed_html = f"""
    <div id='{element_id}' class='tab'>
        {tab_headers}
        {tab_contents}
    </div>
    <script>
    function showTab(elementId, tabId) {{
        var tabs = document.querySelectorAll('#' + elementId + ' .tab-content');
        tabs.forEach(function(tab) {{
            tab.style.display = 'none';
        }});
        document.getElementById(tabId).style.display = 'block';
    }}
    // Show the first tab by default
    document.addEventListener('DOMContentLoaded', function() {{
        var firstTab = document.querySelector('#' + elementId + ' .tab-content');
        if (firstTab) {{
            firstTab.style.display = 'block';
        }}
    }});
    </script>
    """
    return tabbed_html
# Example
def human_readable(tensor):
    if not isinstance(tensor, list) or len(tensor) < 4:
        return str(tensor)
    layer, coords, data, length = tensor[0]
    op = tensor[1][2]
    next_coords = tensor[4]
    return f"Layer: {layer}, Coords: {coords}, Data: {data}, Op: {op}, Next: {next_coords}"


===== Код из файла: /workspaces/Veector/src/main.py =====

from core import Veector
from model_manager import ModelManager
from virtual_space import VirtualSpace
from observervatoria import Observer, TokenAgent
import numpy as np
import os
import json
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
import gc
import psutil
from transformers import AutoTokenizer
import urllib.request
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def print_memory_usage():
    process = psutil.Process(os.getpid())
    ram_usage = process.memory_info().rss / 1024**2
    logger.info(f"RAM использование: {ram_usage:.2f} MB")

veector = Veector(use_memory=False, ipfs_enabled=False)
model_manager = ModelManager(veector, ipfs_enabled=False, model_dir="/workspaces/Veector/data")
veector.model_manager = model_manager

model_name = "DeepSeek-R1-Distill-Qwen-1.5B"
tensor_dir = f"/workspaces/Veector/data/blocks/{model_name}"
model_config_dir = f"/workspaces/Veector/data/models/{model_name}"

if not os.path.exists(tensor_dir):
    print(f"Ошибка: Директория {tensor_dir} не существует.")
    exit(1)

config_path = os.path.join(tensor_dir, "config.json")
if not os.path.exists(config_path):
    print(f"Ошибка: Файл config.json не найден в {tensor_dir}.")
    exit(1)

with open(config_path, "r") as f:
    config = json.load(f)
    vocab_size = config["vocab_size"]
    hidden_size = config["hidden_size"]
    num_layers = config["num_hidden_layers"]
    num_attention_heads = config["num_attention_heads"]
    intermediate_size = config["intermediate_size"]
    key_dim = config.get("key_dim", 256)
    num_key_value_heads = config["num_key_value_heads"]

virtual_space = VirtualSpace(veector, use_ipfs=False)
virtual_space.switch_model(model_name, vocab_size, hidden_size, num_layers, num_attention_heads, intermediate_size, key_dim, num_key_value_heads)

print(f"Параметры из config.json: vocab_size={vocab_size}, hidden_size={hidden_size}, num_layers={num_layers}")

block_files = list(Path(tensor_dir).glob(f"{model_name}_*_block*.pt"))
if not block_files:
    print(f"Ошибка: Файлы блоков модели {model_name} не найдены в {tensor_dir}.")
    exit(1)
else:
    print(f"Найдено {len(block_files)} файлов блоков модели {model_name} в {tensor_dir}:")
    for block_file in block_files[:10]:
        print(f" - {block_file.name}")
    if len(block_files) > 10:
        print(f" ... и еще {len(block_files) - 10} файлов")

tokenizer_path = os.path.join(model_config_dir, "tokenizer.json")
if not os.path.exists(tokenizer_path):
    print("Скачиваю tokenizer.json с Hugging Face...")
    os.makedirs(model_config_dir, exist_ok=True)
    urllib.request.urlretrieve(
        "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/tokenizer.json",
        tokenizer_path
    )
tokenizer = AutoTokenizer.from_pretrained(model_config_dir)

observer = virtual_space.observer

def generate_response(observer, input_text, tokenizer, max_steps=5):
    input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(observer.device)
    logger.info(f"Input IDs shape: {input_ids.shape}")
    generated_ids = input_ids.clone()

    for step in range(max_steps):
        with torch.no_grad():
            next_token_id = observer.forward(generated_ids, tokenizer, return_top_k=True)
            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)
            current_text = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)
            logger.info(f"Step {step}: Current text: {current_text}")
            if next_token_id.item() in [tokenizer.eos_token_id, tokenizer.pad_token_id]:
                break
    
    response = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)
    logger.info(f"Generated response: {response}")
    return response

def main():
    print("Очистка памяти перед инференсом...")
    gc.collect()
    print_memory_usage()

    print("Тестируем токенизатор с помощью generate_response...")
    test_prompt = "Hello, how are you?"
    test_response = generate_response(observer, test_prompt, tokenizer)
    print(f"Тестовый ответ (generate_response): {test_response}")

    print("\nНачинаем чат с generate_response (для выхода нажмите Ctrl+C)")
    try:
        while True:
            prompt = input("Вы: ")
            if not prompt.strip():
                print("Пожалуйста, введите текст.")
                continue
            
            response = generate_response(observer, prompt, tokenizer)
            print(f"Veector: {response}")
            print_memory_usage()
            gc.collect()

    except KeyboardInterrupt:
        print("\nЧат завершен.")
    finally:
        print_memory_usage()
        gc.collect()
        print("Очистка памяти завершена.")

if __name__ == "__main__":
    main()

===== Код из файла: /workspaces/Veector/src/memory.py =====

# /workspaces/Veector/device/src/memory.py
import hashlib
import pickle
import numpy as np
import time
from collections import OrderedDict  # Для реализации LRU вручную

class Memory:
    def __init__(self, capacity=1000, use_lru_cache=True, use_hashing=True):
        self.use_lru_cache = use_lru_cache
        self.use_hashing = use_hashing
        self.capacity = capacity
        
        if use_lru_cache:
            self.storage = OrderedDict()  # Используем OrderedDict для LRU
        else:
            self.storage = {}

    def _hash_key(self, key):
        if isinstance(key, (tuple, list)):
            key = tuple(key)
        if isinstance(key, np.ndarray):
            key = key.tobytes()
        return hashlib.sha256(pickle.dumps(key)).hexdigest()

    def store(self, key, value):
        if self.use_hashing:
            key = self._hash_key(key)
        if self.use_lru_cache and len(self.storage) >= self.capacity:
            self.storage.popitem(last=False)  # Удаляем самый старый элемент
        elif not self.use_lru_cache and len(self.storage) >= self.capacity:
            self._evict_oldest()
        self.storage[key] = value
        if self.use_lru_cache:
            self.storage.move_to_end(key)  # Перемещаем в конец для LRU

    def retrieve(self, key):
        if self.use_hashing:
            key = self._hash_key(key)
        if key in self.storage:
            if self.use_lru_cache:
                self.storage.move_to_end(key)  # Обновляем порядок для LRU
            return self.storage[key]
        return None
    
    def _evict_oldest(self):
        if self.storage:
            oldest_key = next(iter(self.storage))
            del self.storage[oldest_key]

    def clear(self):
        self.storage.clear()

    def __len__(self):
        return len(self.storage)

    def __contains__(self, key):
        if self.use_hashing:
            key = self._hash_key(key)
        return key in self.storage
    
class MemoryManager:
    def __init__(self, max_size=512):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
    
    def add_block(self, block_name, block):
        if self._get_size() + block_size(block) > self.max_size:
            self._evict_lru()
        self.cache[block_name] = block
        self.access_times[block_name] = time.time()
    
    def _evict_lru(self):
        oldest_block = min(self.access_times.items(), key=lambda x: x[1])[0]
        del self.cache[oldest_block]
        del self.access_times[oldest_block]

    def _get_size(self):
        return sum([block.nbytes / (1024 * 1024) if isinstance(block, np.ndarray) else 0 for block in self.cache.values()])

def block_size(block):
    return block.nbytes / (1024 * 1024) if isinstance(block, np.ndarray) else 0

===== Код из файла: /workspaces/Veector/src/model_manager.py =====

# /workspaces/Veector/src/model_manager.py
import os
import torch
import torch.nn.functional as F
import numpy as np
from ipfshttpclient import connect
from pathlib import Path
from virtual_space import VirtualSpace
from qiskit import QuantumCircuit

class ModelManager:
    def __init__(self, veector, block_size=(1024, 1024), ipfs_enabled=True, model_dir="../data/models"):
        """
        Менеджер моделей для работы с блочно-матричной архитектурой и квантовыми цепями.
        :param veector: Экземпляр ядра Veector.
        :param block_size: Размер блока матрицы (высота, ширина) — не используется, размеры из метаданных.
        :param ipfs_enabled: Включить IPFS-хранилище.
        :param model_dir: Директория для локальных данных.
        """
        self.veector = veector
        self.ipfs_enabled = ipfs_enabled
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.virtual_space = VirtualSpace(veector, use_ipfs=ipfs_enabled, model_manager=self)
        self.quantum_circuits = {}
        self.p2p_node = veector.p2p_node if ipfs_enabled and veector.p2p_node else None

    def load_pre_split_model(self, model_name, tensor_dir, vocab_size=None, hidden_size=None, num_layers=None, 
                             num_attention_heads=None, intermediate_size=None, key_dim=None, num_key_value_heads=None):
        """
        Загружает модель, предварительно разделённую на блоки, из директории.
        :param model_name: Название модели.
        :param tensor_dir: Путь к директории с блоками модели.
        :param vocab_size: Размер словаря (из config.json).
        :param hidden_size: Размер скрытого слоя (из config.json).
        :param num_layers: Количество слоёв (из config.json).
        :param num_attention_heads: Количество голов внимания (из config.json).
        :param intermediate_size: Размер промежуточного слоя в MLP (из config.json).
        :param key_dim: Размер ключей и значений для GQA (из config.json или вычисляется).
        :param num_key_value_heads: Количество голов для ключей и значений в GQA (из config.json).
        """
        if not os.path.exists(tensor_dir):
            raise ValueError(f"Директория {tensor_dir} не существует")

        block_files = list(Path(tensor_dir).glob(f"{model_name}_*_block*.pt"))
        if not block_files:
            raise ValueError(f"Файлы блоков модели {model_name} не найдены в {tensor_dir}")
        
        print(f"Проверка загрузки модели {model_name} из {tensor_dir}")
        print(f"Найдено {len(block_files)} файлов блоков в {tensor_dir}:")
        for block_file in block_files[:10]:
            print(f" - {block_file.name}")
        if len(block_files) > 10:
            print(f" ... и еще {len(block_files) - 10} файлов")

        # Проверяем наличие метаданных
        metadata_path = os.path.join(tensor_dir, f"{model_name}_metadata.json")
        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"Метаданные для {model_name} отсутствуют по пути {metadata_path}")

        # Проверяем обязательные параметры
        required_params = [vocab_size, hidden_size, num_layers, num_attention_heads, intermediate_size]
        param_names = ["vocab_size", "hidden_size", "num_layers", "num_attention_heads", "intermediate_size"]
        for param, name in zip(required_params, param_names):
            if param is None:
                raise ValueError(f"Параметр {name} должен быть передан из config.json")

        # Устанавливаем значения по умолчанию, если не указаны
        if key_dim is None:
            key_dim = (hidden_size // num_attention_heads) * (num_key_value_heads or 2)  # Например, 1536 / 12 * 2 = 256
            print(f"key_dim не указан, вычислен как {key_dim}")
        if num_key_value_heads is None:
            num_key_value_heads = 2  # Умолчание из config.json
            print(f"num_key_value_heads не указан, используется значение по умолчанию: {num_key_value_heads}")

        # Переключаем VirtualSpace на модель с полным набором параметров
        self.virtual_space.switch_model(
            model_name, vocab_size, hidden_size, num_layers, num_attention_heads, intermediate_size, key_dim, num_key_value_heads
        )
        print(f"Модель {model_name} загружена из {tensor_dir} с {len(block_files)} блоками")

    def perform_inference(self, model_name, input_data):
        """
        Выполняет инференс для указанной модели.
        :param model_name: Название модели.
        :param input_data: Входные данные (numpy массив, список или PyTorch тензор).
        :return: Результат инференса (PyTorch тензор).
        """
        if not hasattr(self.virtual_space, 'current_model') or self.virtual_space.current_model != model_name:
            raise ValueError(f"Модель {model_name} не загружена или не активна")
        
        # Преобразуем входные данные в PyTorch тензор
        if isinstance(input_data, np.ndarray):
            input_tensor = torch.from_numpy(input_data).to(self.virtual_space.matrix_models[model_name].device)
        elif isinstance(input_data, list):
            input_tensor = torch.tensor(input_data, device=self.virtual_space.matrix_models[model_name].device)
        else:
            input_tensor = input_data
        
        output = self.virtual_space.perform_inference(input_tensor)
        return output

    def add_quantum_circuit(self, model_name, circuit):
        """Добавляет квантовую цепь для модели."""
        if not isinstance(circuit, QuantumCircuit):
            raise ValueError("circuit должен быть объектом QuantumCircuit")
        self.quantum_circuits[model_name] = circuit
        print(f"Квантовая цепь добавлена для модели {model_name}")

    def execute_quantum_circuit(self, model_name, input_state=None):
        """Выполняет квантовую цепь для модели."""
        if model_name not in self.quantum_circuits:
            raise ValueError(f"Квантовая цепь для {model_name} не найдена")
        
        from qiskit import execute
        from qiskit.providers.aer import Aer
        circuit = self.quantum_circuits[model_name]
        num_qubits = circuit.num_qubits

        if input_state is not None:
            input_state = np.array(input_state, dtype=np.complex128)
            if input_state.size != 2 ** num_qubits:
                raise ValueError(f"Размер входного состояния {input_state.size} не соответствует {2 ** num_qubits}")
            circuit.initialize(input_state / np.linalg.norm(input_state), range(num_qubits))

        simulator = Aer.get_backend('statevector_simulator')
        job = execute(circuit, simulator)
        result = job.result().get_statevector()
        return np.array(result, dtype=np.complex128)

if __name__ == "__main__":
    from core import Veector
    veector = Veector(use_memory=False, ipfs_enabled=False)
    manager = ModelManager(veector, ipfs_enabled=False)

    # Загрузка модели
    manager.load_pre_split_model(
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "/workspaces/Veector/data/blocks/DeepSeek-R1-Distill-Qwen-1.5B",
        vocab_size=151936,
        hidden_size=1536,
        num_layers=28,
        num_attention_heads=12,
        intermediate_size=8960,
        key_dim=256,
        num_key_value_heads=2
    )

    # Тест инференса
    vocab_size = 151936
    max_sequence_length = 6
    batch_size = 1
    input_data = np.random.randint(0, vocab_size, (batch_size, max_sequence_length), dtype=np.int32)
    output = manager.perform_inference("DeepSeek-R1-Distill-Qwen-1.5B", input_data)
    print(f"Результат инференса: {output.shape}")

    predicted_tokens = torch.argmax(output, dim=-1)
    print(f"Предсказанные токены: {predicted_tokens}")

    # Тест квантовой цепи
    qc = QuantumCircuit(2)
    qc.h(0)
    qc.cx(0, 1)
    manager.add_quantum_circuit("quantum_test", qc)
    result = manager.execute_quantum_circuit("quantum_test", input_state=[1, 0, 0, 0])
    print(f"Результат квантовой цепи: {result}")

